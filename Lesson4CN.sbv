0:00:01.660,0:00:04.440
欢迎来到第四课

0:00:04.520,0:00:08.520
我们将要完成这个旅程

0:00:08.520,0:00:10.740
通过这些关键应用

0:00:10.840,0:00:14.720
我们已经学过的一系列高效的应用，包括

0:00:14.940,0:00:18.660
图片分类，图片定位，图片回归

0:00:18.820,0:00:21.400
我们涉及到了NLP

0:00:21.460,0:00:26.000
我们将深入学习NLP迁移学习（transfer learning）

0:00:26.300,0:00:32.480
我们还会学到表格数据（tabular data）, collaborative filtering

0:00:32.620,0:00:34.760
它们俩都超有用应用。

0:00:34.980,0:00:37.880
然后，我们要做一个U型掉头，

0:00:38.020,0:00:46.600
深入学习collaborative filtering, 剖析其背后的数学原理，详细计算过程

0:00:46.780,0:00:51.400
我们用这样的风格，从后往前，再将所有应用案例梳理一遍

0:00:51.580,0:00:56.800
从而理解所有这些案例应用背后的工作原理。

0:00:57.880,0:01:01.320
在我们开始前

0:01:02.000,0:01:04.000
有人在论坛里非常好心的指出

0:01:04.180,0:01:09.180
当我们对比自己的成绩与我们理解的之前的state of the art 最先进的水平时

0:01:09.340,0:01:11.340
也就是最近的camvid数据集的state of the art 最好的成绩

0:01:11.340,0:01:13.120
说这个对比并不公正

0:01:13.280,0:01:16.380
因为论文里其实用的是一个子数据集

0:01:16.480,0:01:20.380
论文里模型只是用了一部分类别，而我们用了全部的类别

0:01:20.500,0:01:24.620
Jason和他的小组非常用心的重跑了这个对比实验

0:01:24.700,0:01:28.840
采用了与论文相同的类别

0:01:28.940,0:01:31.580
我们的准确度上升到了94%

0:01:31.620,0:01:34.160
对比论文的成绩只有91.5%

0:01:34.280,0:01:37.340
这是一个非常棒的成绩和例子

0:01:37.480,0:01:43.600
说明哪怕只用默认设置

0:01:43.680,0:01:48.600
也能帮助我们获得接近甚至超越一两年前的最高水平的成绩

0:01:48.780,0:01:54.720
去年我们全力学习了这篇论文，所以知道（论文水平）在去年至少是最高水平了。

0:01:54.840,0:01:57.560
这是非常激动人心的！

0:01:58.660,0:02:02.720
我们一开始要讲的内容是

0:02:06.160,0:02:08.600
是回头看一下NLP

0:02:08.840,0:02:12.500
来深入理解NLP的工作原理

0:02:12.560,0:02:14.560
首先，快速复习一下

0:02:15.500,0:02:18.960
我们知道，NLP是自然语言处理

0:02:19.020,0:02:24.000
也就是对文本做处理

0:02:24.140,0:02:30.960
文本分类，是非常有用务实的一项应用

0:02:31.100,0:02:33.100
我们将从这里开始

0:02:33.260,0:02:40.020
因为文本分类可以被用来做垃圾邮件筛查

0:02:40.140,0:02:48.880
识别新闻造假，从医疗报告中生成诊断，

0:02:49.100,0:02:54.480
找到推特中对你的产品的谈及

0:02:54.480,0:02:56.280
等等

0:02:56.280,0:02:58.180
所以是特别有趣的

0:02:58.320,0:03:01.780
而且这里有一个非常棒的例子

0:03:01.880,0:03:09.900
有一个特别棒的例子，来自一位学员，从事法律

0:03:10.100,0:03:14.980
他在论坛里说

0:03:15.020,0:03:18.340
他在做法律文本分类时取得了不错的结果

0:03:18.500,0:03:21.640
借助的就是我们的NLP模型和方法

0:03:21.740,0:03:26.560
这就是他在一个学术会议上展示的海报

0:03:26.700,0:03:30.600
在海报里，他描述了我们的NLP的方法

0:03:30.820,0:03:33.460
其中有一系列步骤（3个步骤）

0:03:33.580,0:03:36.800
这里你会看到这个非常熟悉的分类矩阵

0:03:37.020,0:03:42.300
这三个步骤就是我们这节课的起点

0:03:42.460,0:03:46.280
我们从一个影评开始

0:03:46.300,0:03:53.240
就像这样的影评，然后判断它是正面或负面的电影评价

0:03:53.360,0:03:56.560
但这里有一个问题

0:03:56.660,0:04:02.000
在训练集中有2.5万个影评

0:04:03.420,0:04:08.620
我们有2.5万个影评

0:04:08.720,0:04:11.660
每个影评我们有一个信息

0:04:11.780,0:04:14.020
（评论人）喜欢或不喜欢（这个电影）

0:04:14.220,0:04:17.180
这就是我们要深入理解的内容

0:04:17.180,0:04:18.340
（时间是）今天和后续的课程中

0:04:18.440,0:04:23.560
我们的神经网络，记住，它们无非是一堆矩阵乘法(matrix multiplies)

0:04:23.660,0:04:28.160
和简单的非线性激活函数(non-linearities), 尤其是将负数用0取代的函数（ReLU)

0:04:28.240,0:04:31.300
那些参数矩阵（weight matricies）, 是从随机数开始的

0:04:31.480,0:04:35.560
如果你是从随机参数开始的

0:04:35.760,0:04:42.980
并试图训练这些参数去分辨正面与负面情绪的影评

0:04:42.980,0:04:49.620
你将面对25000个0和1(参数)试图告诉你，影评人喜欢这个电影还是不喜欢那个电影

0:04:49.620,0:04:51.400
这(信息)显然是不够的

0:04:51.500,0:04:55.360
也就是说没有足够的信息来将你（的模型）说英语

0:04:55.460,0:05:00.220
也就是教模型说英语足够好，好到能够告诉你影评人是否喜欢某个电影

0:05:00.380,0:05:03.220
有时这会非常含糊不清

0:05:03.300,0:05:06.800
英语，作为一种语言，在做影评时

0:05:06.960,0:05:13.760
这些（IMDB）都是网络影评，人们会用讽刺语言，这会非常有挑战

0:05:13.900,0:05:19.060
所以，很久以来，直到最近

0:05:19.200,0:05:24.620
甚至今年，神经网络(在这方面)表现不好

0:05:24.720,0:05:27.880
针对这种类型的分类问题

0:05:27.980,0:05:31.920
这是原因，没有足够的信息

0:05:32.000,0:05:35.940
所以，解题的关键，也许你们都能猜到

0:05:36.020,0:05:38.720
就是使用迁移学习(transfer learning), 这是远恒不变的技巧

0:05:38.880,0:05:42.140
所以，去年的这个课程里

0:05:42.300,0:05:44.700
我尝试了一些“疯狂”的事情

0:05:44.840,0:05:47.300
因为我想，如果我用迁移学习（transfer learning）会怎样

0:05:47.460,0:05:50.280
来证明它也能适用于NLP问题

0:05:50.400,0:05:55.360
我试了，结果是表现非常棒！

0:05:55.360,0:05:56.940
所以，现在，那之后的一年

0:05:57.120,0:06:01.420
NLP中的迁移学习已经成为绝对的热点

0:06:01.500,0:06:03.500
让我跟你们分享一下到底是（我）怎么做到的

0:06:03.660,0:06:13.260
关键点是，我们要从机器视觉的预训练模型开始

0:06:13.400,0:06:16.660
我们要用模型做的事，与它预训练来做的事，不一样

0:06:16.860,0:06:19.700
所以，对Imagenet 数据集而言

0:06:19.800,0:06:24.100
这个预训练模型，初衷是被训练来分类1000个类别

0:06:24.280,0:06:26.280
根据每一个图片来做分类

0:06:26.380,0:06:28.840
然后对这样的模型微调来分类别的内容

0:06:29.020,0:06:32.440
我们也要从一个预训练模型开始

0:06:32.600,0:06:36.000
来做别的任务，不是影评情绪分类

0:06:36.020,0:06:39.480
我们要从一个预训练模型开始，这模型叫语言模型

0:06:39.640,0:06:43.680
语言模型(language model)有着具体的含义

0:06:43.820,0:06:45.820
在NLP领域里，它的意思是

0:06:45.900,0:06:49.780
模型是在学习预测一个句子里的下一个词

0:06:49.900,0:06:53.320
为了预测句子中的下一个词

0:06:53.480,0:06:56.860
模型需要理解很多语言知识

0:06:56.940,0:06:59.360
假设我们用的是英文

0:06:59.360,0:07:01.340
和很多关于世界的常识

0:07:01.420,0:07:03.680
什么是常识(world knowledge), 这里是一个例子

0:07:03.680,0:07:07.960
这里有一个语言模型：给它一句话 “I'd like to eat a hot ___" 什么呢？

0:07:08.100,0:07:11.140
这个里应该是，很显然，“dog"

0:07:11.300,0:07:16.400
"it was a hot ___" 什么呢？很可能是“天气”

0:07:16.520,0:07:19.440
之前NLP的解决方案是

0:07:19.560,0:07:27.220
使用的是N-grams， 其实就是记录这两个或三个一组的词在一起出现的频率

0:07:27.260,0:07:29.960
N-gram在做这样的任务表现是很差的

0:07:29.960,0:07:32.280
如你所见，因为没有足量的信息

0:07:32.380,0:07:35.220
来判断下一个词是什么

0:07:35.260,0:07:38.600
但是使用神经网络的话，你绝对可以做到

0:07:38.640,0:07:43.040
这里有一个非常棒的地方，就是如果你训练一个神经网络

0:07:43.140,0:07:45.960
来预测一句话的下一个词

0:07:46.160,0:07:49.640
那么你其实有很多信息

0:07:49.760,0:07:53.640
不再是一个信息对应一个含有2000个词的影评

0:07:53.640,0:07:55.200
喜欢或不喜欢（这一个单一信息）

0:07:55.240,0:07:59.060
每一个词都可以用来帮助判断下一个词

0:07:59.340,0:08:01.340
所以，在一个2000词的影评里

0:08:01.480,0:08:06.020
现在有1999个机会来预测下一个词

0:08:06.180,0:08:09.040
更好的是，

0:08:09.100,0:08:11.220
你不必只看影评

0:08:11.280,0:08:13.460
因为真正难的地方

0:08:13.540,0:08:17.220
并不是这个人是否喜欢这个电影

0:08:17.280,0:08:20.180
而是你/模型如何说英语

0:08:20.200,0:08:24.080
所以，你/模型可以学习如何说英语

0:08:24.260,0:08:27.200
从一个更大的数据集中（来学英语）

0:08:27.240,0:08:30.540
我们所做的是，选择了wikipedia维基百科

0:08:30.700,0:08:36.540
Steven Meriitt和他的同事创建了wiki text 103数据集

0:08:36.620,0:08:42.660
可以说是一个含有大型维基百科文件的子类别

0:08:42.780,0:08:46.500
数据需要做一些预处理，可以下载

0:08:46.600,0:08:49.540
基本上，从维基百科获取数据

0:08:49.620,0:08:52.600
在维基百科数据集上构建语言模型

0:08:52.720,0:08:56.480
于是有了一个神经网络能预测下一个词

0:08:56.560,0:09:00.420
在每一个大型的维基百科文章里(做这样的预测)

0:09:00.560,0:09:03.220
这样就有了非常多的数据

0:09:03.260,0:09:05.580
如果没错的话，大概有上十亿的分词(Tokens)

0:09:05.700,0:09:08.640
这样，我们就有上十亿次独立的预测可以做

0:09:08.640,0:09:11.000
每次预测出错时

0:09:11.100,0:09:15.120
我们能计算损失值（loss），导数（gradients），

0:09:15.140,0:09:17.420
能更新参数，让模型不断改进

0:09:17.500,0:09:21.060
直到能做出很好的下一个词的预测

0:09:21.100,0:09:23.100
为什么这样的模型有用

0:09:23.160,0:09:27.220
因为这时的模型可能已经学会如何填词完句了

0:09:27.300,0:09:30.780
因此，模型已经学会了不少的英语

0:09:30.840,0:09:32.960
学会了不少世界如何工作的知识（常识）

0:09:33.040,0:09:37.880
例如，怎样的东西，在不同场景下，会是烫的

0:09:37.960,0:09:40.100
完美情况下，还能学习诸如此类的知识：

0:09:40.700,0:09:44.240
“在1996年，在联合国的演讲中

0:09:44.340,0:09:47.860
联合国主席___说“

0:09:47.940,0:09:50.780
这会是一个非常好的语言模型

0:09:50.900,0:09:55.040
如果这个模型能回答谁说那年的联合国主席的话。

0:09:55.040,0:09:58.200
因此，如果能将语言模型训练的非常好

0:09:58.320,0:10:04.200
那么这将非常好的方法，能让神经网络学到非常多的

0:10:04.200,0:10:09.020
知识，如我们的世界长得什么样子，有什么，以及如何工作

0:10:09.020,0:10:11.420
这是非常激动人心的项目领域

0:10:11.520,0:10:15.300
而且这是哲学研究了数百年的领域

0:10:15.300,0:10:19.000
其中有很多哲学理论是关于

0:10:19.120,0:10:23.160
什么是可以仅仅依靠学习语言就能获得的知识

0:10:23.220,0:10:27.140
结果是，我们可以看到，（能学到）很多很多

0:10:27.240,0:10:31.040
有趣的是，你可以一上来就用全部的维基百科数据来训练语言模型（language model）

0:10:31.120,0:10:33.920
然后我们将这个（训练好的模型）分享给你们所有人

0:10:33.980,0:10:36.680
就像机器视觉中的预训练模型那样（可以让所有人使用）

0:10:36.760,0:10:40.560
现在我们分享基于维基百科的预训练模型，用于自然语言NLP领域

0:10:40.660,0:10:43.220
并不是因为这个模型本身特别有用

0:10:43.340,0:10:46.820
能预测一句话中的下一个词，有一定的用处

0:10:46.920,0:10:49.180
但不是我们通常会用到的功能

0:10:49.200,0:10:53.620
但它（语言模型）告诉我们它是一个相当懂得语言的模型

0:10:53.680,0:10:55.680
它能理解语言描述的内容

0:10:55.760,0:11:00.420
然后，我们可以借用这个（语言）模型language model 来做迁移学习transfer learning

0:11:00.560,0:11:03.920
来创造一个新的语言模型language model

0:11:04.060,0:11:08.400
尤其擅长预测影评中的下一个词

0:11:08.480,0:11:16.080
如果我们能创建一个语言模型擅长预测影评中的下一个词，

0:11:16.080,0:11:19.540
（这个模型）是经过维基百科数据(wiki text)预训练模型发展而来

0:11:19.700,0:11:22.500
那么，（这个新模型）将能理解

0:11:22.560,0:11:25.100
”我最喜爱的男演员是Tom ___?“ （能猜出Cruise)

0:11:25.240,0:11:30.000
或者（能理解）“我认为拍摄是相当棒，

0:11:30.140,0:11:33.900
但我对____不是特别满意" (能猜出是应该填写导演）

0:11:33.980,0:11:37.640
所以，这个模型学会很多如何写影评的知识

0:11:37.640,0:11:41.940
它甚至还能学到比较火的电影名称

0:11:42.040,0:11:49.400
这就意味着，我们能使用大量的影评数据，

0:11:49.400,0:11:52.420
即便我们不知道它们是正面还是负面情绪

0:11:52.460,0:11:54.720
（只是）为了（让模型）学习影评该如何写

0:11:54.740,0:11:57.740
所有这些预训练和微调训练

0:11:57.740,0:11:59.560
我们不需要标注（labels）也能进行

0:11:59.700,0:12:04.760
这就是研究学者Yann LeCun 所说自监督学习(Self-Supervised Learning)

0:12:04.900,0:12:09.080
换言之，经典的监督学习模型(classic supervised models) 是有标注的

0:12:09.120,0:12:13.780
但这些标注不是其他人单独创造植入的，而是数据集内置好的

0:12:13.860,0:12:16.040
所以，这（自监督学习）就非常厉害了

0:12:16.080,0:12:19.820
因为，现在我们以及有模型擅长理解影评

0:12:19.820,0:12:21.740
我们可以微调这个模型

0:12:21.800,0:12:25.020
（借助迁移学习）来让模型做我们想要做的事

0:12:25.060,0:12:28.860
在这里我们需要做的事是判断影评的情绪，是正面还是负面

0:12:28.900,0:12:31.760
我（去年）的期望是（我去年也试过）

0:12:31.820,0:12:38.760
这25000个0和1（正面/负面）能够给予我们足够信息反馈来微调模型

0:12:38.800,0:12:41.820
事实证明，的确如此！

0:12:41.920,0:12:44.800
好的，Rachel，到提问时间啦

0:12:47.420,0:12:51.000
（提问）语言模型是否适用于论坛中的文本？

0:12:51.060,0:12:57.860
（这些文本通常包含）都是非正式英语，错别字，俗语俚语，快捷缩写用语，例如 S6 （而不用Sumsang S6)

0:13:00.660,0:13:02.660
是的，绝对适用。

0:13:02.720,0:13:06.980
尤其是如果你先有了基于维基百科的语言模型

0:13:07.060,0:13:10.540
然后再用目标文本数据集(target corpus)来对模型微调

0:13:10.600,0:13:13.240
corpus无非就是一堆文本

0:13:13.280,0:13:18.320
可以是邮件，推特，或医疗报告等等

0:13:18.420,0:13:21.280
你可以对其微调

0:13:21.280,0:13:25.500
使其能学习一些具体俗语俚语的用法

0:13:25.520,0:13:29.740
或者是缩写用法等等那些在完整文本数据集里没有类似的文本

0:13:29.820,0:13:34.880
有趣的是，人们对我们去年研究成果非常震惊的一点是

0:13:35.020,0:13:40.780
大家通常认为从维基百科里学到的语言模型不会有多少能力

0:13:40.800,0:13:43.960
理由是（维基百科）不是人们常规的写作风格

0:13:43.980,0:13:46.280
但结果证明却非常有帮助（训练出来的模型很有用）

0:13:46.340,0:13:55.080
因为维基百科与随机文字之间的差异要远大于维基百科与Redit之间的差异

0:13:55.140,0:13:57.720
因此，（基于维基百科训练的）模型99%的情况下能够给于我们想要的表现。

0:14:01.880,0:14:06.120
因此，这些语言模型本身其实就非常强大。

0:14:06.140,0:14:14.880
比如说，这里是关于swift key的一个例子

0:14:14.920,0:14:18.940
（swiftKey) 他们能对手机键盘（下一个字的）输入做预测

0:14:19.060,0:14:24.980
他们在这里描述了他们如何重写底层模型(underlying models)

0:14:24.980,0:14:26.780
来使用神经网络

0:14:26.800,0:14:30.500
这是1-2年前的内容，如今几乎所有手机键盘输入似乎都能做到这些

0:14:30.560,0:14:35.480
（具体形式是）你一边打字，键盘预测会展示它猜测你的下一个字（或词组）是什么

0:14:35.500,0:14:38.260
这就是运行在你手机中的语言模型

0:14:38.260,0:14:42.100
另一个案例是研究学者Andrej Karpathy

0:14:42.100,0:14:46.400
他现在在特斯拉里做类似这样的事情

0:14:46.480,0:14:49.900
在他还是博士生期间，他做了一个语言模型

0:14:49.920,0:14:57.700
采用了latex文本数据来训练，进而自动生成如图的latex文本内容

0:14:57.720,0:15:00.580
成为了这样的自动生成的论文内容

0:15:00.580,0:15:01.720
非常可爱！

0:15:01.780,0:15:06.520
我们其实对语言模型输出结果并不感兴趣

0:15:06.520,0:15:10.420
我们对语言模型感兴趣仅因为它是整个工作流程中所起的重要作用

0:15:12.500,0:15:18.160
上周，我们简要的过了一遍实验流程

0:15:18.200,0:15:20.440
现在我们回顾一下

0:15:20.620,0:15:27.500
基本步骤是，我们从数据开始，数据格式是

0:15:27.500,0:15:31.920
例如，我们创造了IMDB_SAMPLE数据集，格式是CSV

0:15:31.920,0:15:33.740
然后用pandas来读取CSV

0:15:33.740,0:15:42.260
（lable栏）正面或负面positive or negative （text 栏）是影评内容(is_valid 栏) 是布尔逻辑值(boolean, true or false) 告知是训练集还是验证集

0:15:42.360,0:15:44.360
所以，这里看到的是影评例子。

0:15:44.460,0:15:51.640
我们可以用TextDataBunch.from_csv来做一个专门用于语言模型的DataBunch

0:15:51.660,0:15:54.800
然后基于此，再做一个学习器(Learner)，然后就可以训练了

0:15:54.840,0:15:58.640
我们可以保存DataBunch（不是模型哦）

0:15:58.660,0:16:02.040
意味着，（当我们再次使用数据时）前面的数据预处理，我们不用重做一遍。

0:16:02.040,0:16:04.180
只需加载(.load)就可以了

0:16:05.740,0:16:10.000
背后的工作流程是什么呢？

0:16:10.000,0:16:13.220
如果我们以文本分类模型的DataBunch的格式加载之前保存的数据(TextClasDataBunch.load)

0:16:13.300,0:16:15.300
(data.show_batch) 将允许我们（看到文本的同时）还能看到标注(labels)

0:16:15.320,0:16:22.220
如之前（上节课）所说，(TextClasDataBunch.load) 将文本分解成了独立的单元，我们称之为分词(tokens)

0:16:22.220,0:16:25.200
分词(token)可以是一个词分割成的任意一份

0:16:25.240,0:16:27.240
多数情况下，每个分词就是一个（正常）的词

0:16:27.260,0:16:31.820
有时像 it's这样的词，会被拆分成 it 和 's 这样两个分词

0:16:31.880,0:16:39.100
任何一个标点符号，如逗号，句号，都是一个分词，等等

0:16:41.960,0:16:45.540
下一步，是数字化(Numericalization)

0:16:45.620,0:16:51.820
在这一步里，我们将要从每个影评分词里提取独特的分词

0:16:51.820,0:16:53.520
然后将（所有独特分词）汇集到一个很大的列表(list)里

0:16:53.580,0:16:56.060
这里是（列表里）前10个频率最高且独特的分词

0:16:56.080,0:16:59.820
这个巨大的分词列表，就叫做单词表(Vocabulary)

0:16:59.880,0:17:10.560
我们接下来要做的，就是用分词在列表中的位置(ID, 数字）来代替影评中的分词本身

0:17:10.660,0:17:13.040
这就是数字化(Numericalization)

0:17:13.180,0:17:17.200
随着你深入学习下去，

0:17:17.260,0:17:24.700
单词表(Vocabulary)中的每一个词都需要占用模型参数矩阵(weight matrix)中的一行数值

0:17:24.760,0:17:28.580
为了避免这个参数矩阵(weight matrix)变得过大

0:17:28.680,0:17:35.020
我们将单词表(Vocab)的大小限制在60000个单词以内

0:17:35.020,0:17:38.520
如果一个单词出现频率低于2次

0:17:38.520,0:17:40.320
我们不会将其放入单词表(Vocab)

0:17:40.360,0:17:45.060
所以我们通过这种方法将单词表大小控制在合理空间

0:17:45.120,0:17:50.160
当你看到"xxunk", 这是一个未知分词

0:17:50.220,0:17:55.540
当你看到这些未知分词(unknown tokens)，意思是

0:17:55.580,0:18:02.400
这些词不够常见，所以不纳入单词表

0:18:02.420,0:18:06.800
这里就是我们的数字化（后的影评例子）版本

0:18:06.820,0:18:09.380
我们还有一些特殊的分词

0:18:09.380,0:18:11.940
比如， “xxfld”

0:18:12.040,0:18:14.240
这是特殊的分词

0:18:14.300,0:18:19.300
可以代表的东西有：标题，总结，简介，主体之类文档的各个部位

0:18:19.320,0:18:22.660
各个部位都会有一个独立的fld并带有数字标记

0:18:22.760,0:18:29.860
还有如果有单词是全大写，那么会转化成"xxcap"这样的分词

0:18:29.940,0:18:35.900
我个人习惯经常用data_block API

0:18:35.940,0:18:41.480
这样就不需要记住所有DataBunch的类别

0:18:41.480,0:18:43.460
已经相关的参数(parameters or arguments)等等

0:18:43.460,0:18:44.900
这样也更灵活

0:18:44.960,0:18:50.680
另一个（生成DataBunch)的策略是，首先确定你要生成怎样的列表

0:18:50.680,0:18:52.100
你的自变量(independent variable)是什么

0:18:52.200,0:18:54.440
这里我们的自变量是文本

0:18:54.580,0:18:56.580
来自哪里？（来自）CSV文本

0:18:56.620,0:19:01.340
你想要如何（将数据）拆分成训练和验证集？

0:19:01.420,0:19:04.980
这里col=2(也就是第三列）数据就是“is_valid”帮助判断是训练集还是验证集

0:19:05.480,0:19:10.920
你要如何对数据做标注，也就是“正面/负面”情绪标注

0:19:10.920,0:19:12.800
col=0 (也就是第1列）包含这些信息

0:19:12.800,0:19:18.520
然后就可以生成DataBunch了。

0:19:18.540,0:19:26.340
现在我们要用全部数据集，里面有25000影评是用来训练的

0:19:26.400,0:19:29.340
25000是用来做验证集

0:19:29.460,0:19:33.840
还有50000是无监督影评

0:19:33.880,0:19:38.580
也就是说这50000没有被打过分（没有标注）

0:19:38.620,0:19:42.560
这里看到“正面的”， 这里是“负面的”， 这里是“无监督的”影评

0:19:45.040,0:19:52.000
我们要从语言模型开始着手

0:19:52.080,0:19:56.420
好消息是，我们不需要再训练基于维基百科wiki103数据集的语言模型

0:19:56.420,0:19:57.560
（你完全可以重新训练）并不难

0:19:57.680,0:20:01.560
（训练）步骤都是一样的，下载wiki103数据

0:20:01.560,0:20:04.760
运行相同的代码

0:20:04.760,0:20:08.080
但即便在不错的GPU上，也需要跑2-3天，

0:20:08.120,0:20:11.520
所以，不值得重复这样的工作

0:20:11.560,0:20:15.200
你完全可以用我们训练好的语言模型。哪怕你有一个巨大的医疗文本数据集

0:20:15.200,0:20:16.840
或者是法律文本数据集

0:20:16.860,0:20:20.760
你有应该从wiki103的语言模型开始，因为完全没有任何理由从随机参数的模型开始训练起

0:20:20.860,0:20:23.860
如果你能用上，迁移学习肯定是更好的选择

0:20:25.760,0:20:33.600
我们现在要开始着手的部分是微调(fine-tuning)基于IMDB数据集的语言模型(language model)

0:20:33.600,0:20:36.100
(在构建DataBunch时) 我们采用TextFileList

0:20:36.180,0:20:43.880
IMDB完整数据集不在一个单一的CSV里，而是每一个文本都是一个独立的txt文件

0:20:43.960,0:20:49.160
这就是为什么我们改用了TextFileList，然后用from_folder从文件夹中调取数据

0:20:49.200,0:20:53.120
同时确保我们在这里filter_by_folder调取训练集和测试集的文件夹（不用管其他文件夹）

0:20:53.240,0:20:58.260
然后用random_split_by_pct(0.1)来随机将训练集拆分 (成训练集0.9和验证集0.1)

0:20:58.300,0:21:05.740
有趣的是，这里用了随机拆分0.1的操作，为什么不直接用前面给我们的训练集文件夹和测试集文件夹来代替呢（predefined train and test）

0:21:05.840,0:21:08.560
这里是迁移学习（transfer learning）很酷的地方

0:21:08.620,0:21:12.980
尽管测试集和验证集需要与训练集分离（不参与训练）

0:21:12.980,0:21:17.280
但实际我们只需将标注(labels)分离（不参与训练）

0:21:17.360,0:21:19.600
所以，我们不能所以测试集中的标注

0:21:19.600,0:21:24.520
比如，你不能使用Kaggle竞赛中的标注，当然（Kaggle）也不会给你

0:21:24.540,0:21:27.760
但你完全可以使用自变量（independent variables）

0:21:27.780,0:21:34.720
在这里，你完全可以使用测试集中的文本数据（也就是自变量）来训练模型

0:21:34.720,0:21:35.560
这样我们就有了一个很棒的（改进模型的）技巧

0:21:35.640,0:21:38.160
当我们训练模型时

0:21:38.220,0:21:40.600
将训练集和测试集合并在一起(concat)

0:21:40.660,0:21:44.580
然后，（从中）拆分（split out) 一小部分数据做验证集

0:21:44.580,0:21:47.800
这样你就有更多数据训练语言模型（但实际的验证集也没有太少）

0:21:47.800,0:21:49.320
这就是我们的改进模型的技巧。

0:21:49.400,0:21:51.960
当你在Kaggle上做NLP（自然语言处理）竞赛时

0:21:52.060,0:21:57.320
或者，当你只有一个较小的标注过的数据集时

0:21:57.320,0:22:02.020
那么记住要用所有的文本数据（自变量，不论是否对应有应变量，即标注）来训练你的模型

0:22:02.020,0:22:03.240
因为没理由不这么做

0:22:03.820,0:22:06.340
我们如何对数据做标注？

0:22:06.340,0:22:08.660
注意，我们的语言模型，其实有自己的标注

0:22:08.740,0:22:13.220
文本数据本身就是标注，所以label_for_lm()帮助我们完成标注

0:22:13.260,0:22:17.780
然后用databunch()完成构建DataBunch, 然后保存模型，这些会耗费几分钟时间

0:22:17.860,0:22:21.620
来完成分词化(tokenization)和数值化(numericalization)

0:22:21.680,0:22:25.580
因为它会耗费数分钟时间，因此我们对其保存，以后再用时，直接TextLMDataBunch.load就可以了

0:22:25.640,0:22:28.900
我们不要再重做一次数据处理。这里是我们的数据的样子

0:22:28.980,0:22:33.200
接下来的操作，你会感到很熟悉

0:22:33.240,0:22:39.480
我们要构建一个学习器Learner, 但不是构建一个CNN学习器，

0:22:39.520,0:22:42.500
我们要构建的是一个语言模型（使用language_model_learner)

0:22:42.580,0:22:51.400
所以，背后的工作机制，不是构建CNN，而是构建了一个循环神经网络RNN（Recurrent Neural Network)

0:22:51.420,0:22:54.860
我们会在后续课时学习它们是如何一步步构建起来的

0:22:54.920,0:22:59.060
但简言之，它们的结构很相似

0:22:59.100,0:23:06.140
输入值被给到参数矩阵，然后矩阵相乘，再将负数用0取代

0:23:06.220,0:23:09.560
然后在进入下一个矩阵乘法，如此循环

0:23:09.620,0:23:13.300
所以结构没什么不同

0:23:13.380,0:23:18.800
和平时一样，当我们构建学习器Learner时，需要输入两个参数arguments

0:23:18.840,0:23:21.580
数据，也就是我们语言模型的数据

0:23:21.640,0:23:26.920
然后是这里的预训练模型(pretrained_model)，由我们自行挑选使用(URLs.WT103)

0:23:26.980,0:23:34.440
我们选择的预训练模型是URLs.WT103, 构建这个学习器Learner时，fastai会帮助下载这个模型

0:23:34.440,0:23:40.320
就像图片分类应用中的预训练模型一样被自动下载

0:23:40.380,0:23:46.280
这里drop_mult=0.3设置了dropout的比例，我们还没讲到dropout

0:23:46.280,0:23:49.460
我们有简单谈及什么是正则化（regularization）

0:23:49.460,0:23:52.720
我们可以削减正则化（regularization）来降低欠拟合的发生

0:23:52.720,0:23:57.520
当下我们只需要知道，可以用drop_mult < 1

0:23:57.620,0:24:04.540
因为当时我实验代码时，模型处于欠拟合状态，所以削减drop_mult的值（使其小于1），就能规避欠拟合

0:24:04.560,0:24:09.140
我们有了学习器Learner, 可以做学习率查找lr_find,

0:24:09.240,0:24:11.480
看起来挺标准常见的情况

0:24:11.480,0:24:20.120
然后用fit_one_cycle来训练，这其实只是对最后几层参数在做微调更新fine-tune

0:24:20.180,0:24:24.860
通常我们在微调最后几层之后

0:24:24.920,0:24:28.880
下一步，就是解冻模型unfreeze

0:24:28.880,0:24:31.420
然后训练全部的参数（所有层）

0:24:31.480,0:24:35.900
这里就是我们的代码，解冻，训练全部

0:24:35.940,0:24:41.140
如你所见，即便是在GPU上，也用了2-3小时

0:24:41.160,0:24:44.720
但事实上，我们依旧处于欠拟合状态

0:24:44.740,0:24:48.480
今晚我可能会再训练实验一下，看看能不能训练得更好

0:24:48.600,0:24:53.700
你看，嗯，其实这并不算欠拟合

0:24:53.760,0:24:58.900
我想我们还可以继续训练下去，因为精度还没有下行过（变差过）

0:24:58.980,0:25:04.200
所以，继续训练没有坏处。有趣的是

0:25:04.240,0:25:10.320
30%的精度，意味着我们能正确预测下一个词的概率是1/3

0:25:10.380,0:25:13.120
这听起来是个很高的值

0:25:13.140,0:25:17.680
如此高概率的正确猜测下一个词是什么

0:25:17.680,0:25:21.360
这预示我们的语言模型表现不错

0:25:21.440,0:25:26.780
对于更小众的领域的文本数据

0:25:26.840,0:25:29.620
例如医疗和法律文本记录

0:25:29.620,0:25:33.100
你会发现（以它们为数据的语言模型的）精度会更高

0:25:33.100,0:25:37.240
所以，有时这个值可能到达50%或更多

0:25:37.260,0:25:40.920
但是30%或更多已经很好了

0:25:41.060,0:25:49.700
现在我们可以运行learn.predict(),（并给函数）加入一段文字的开头部分

0:25:49.780,0:25:53.480
模型将为你完成这段话

0:25:53.500,0:25:59.860
要提醒大家的是，这里的目的不是来设计一个文本生成模型

0:25:59.920,0:26:05.180
这里的目的，是为了检验语言模型能够生成一些看似合理(vaguely sensible)的语句

0:26:05.260,0:26:10.220
当然，有很多技巧可以用来生成更高质量的文本句子

0:26:10.260,0:26:13.200
但我们这里一个也没有用。

0:26:13.300,0:26:18.400
但你可以看出来

0:26:18.460,0:26:24.840
我们模型生成的绝对不是随机字词，而是看似英语的句子，虽然没有什么实际的意思

0:26:24.900,0:26:31.760
那么，截至现在，我们已经有了一个基于影评的语言模型

0:26:35.540,0:26:43.520
现在我们要将模型保存下来，进而能够被文本分类器使用，作为它的预训练模型

0:26:43.540,0:26:46.260
但我不想保存全部的模型

0:26:46.260,0:26:53.840
之后我们会学到，训练好的语言模型的第二部分，都是关于如何预测下一个词

0:26:53.900,0:26:57.260
而不是理解所读到句/词内容

0:26:57.280,0:27:01.120
所以，模型中关于理解所见词句的那一部分

0:27:01.160,0:27:03.380
叫做编码器（encoder）

0:27:03.380,0:27:08.460
我们将它保存起来learn.save_encoder. 后面课时中我们会学到相关细节

0:27:08.520,0:27:16.360
这里保存的就是能帮助我们理解词句的模型部分，而非预测下一个词的模型部分

0:27:18.080,0:27:21.260
现在，我们准备好了去构建文本分类器

0:27:21.360,0:27:24.460
第一步，依旧是构建DataBunch

0:27:24.520,0:27:27.820
做法几乎都是一样的

0:27:27.900,0:27:36.000
但是这里我们要确认这里的单词表(vocab)是之前语言模型所用单词表

0:27:36.040,0:27:44.020
如果语言模型单词表的第10个词是"the", 我们也需要分类器模型的单词表中的第10个词是"the"

0:27:44.060,0:27:49.080
否则我们调用的预训练模型将毫无意义

0:27:49.080,0:27:58.040
这就是为什么我们有（这行代码）vocab=data_lm.vocab, 来确这里的DataBunch拥有完全相同的保单词表。这是非常重要的一步

0:27:58.040,0:28:00.980
用“test"文件夹来分割训练集和验证集

0:28:01.020,0:28:05.800
上次，我们用的是从训练集与测试集的合并集中随机分割，按照一定的百分比

0:28:05.860,0:28:10.500
这次我们要确保验证集中的标注原封不动

0:28:10.500,0:28:12.680
所以，我们这里该用了split_by_folder方式分割

0:28:12.680,0:28:18.780
这次我们做的标注不是为了训练语言模型，而是为了做分类（正面/负面）

0:28:18.780,0:28:22.060
最后生成DataBunch

0:28:22.080,0:28:27.080
注意：有时候你会发现你的GPU内存不够

0:28:27.160,0:28:36.340
你会经常遇到这个问题，如果(你的内存偏小), 我的GPU内存是11GB，如果你的内存不够，需要将bs=50调得更小一些

0:28:36.500,0:28:42.500
你可能还需要重启你的Notebook kernel核， 直接从这里开始运行（来生成DataBunch)

0:28:42.520,0:28:45.340
bs=50批量规模=50是我的11GBGPU内存所能承受的范围

0:28:45.380,0:28:48.540
如果你使用的亚马逊AWS的p2或p3

0:28:48.680,0:28:53.300
或者是谷歌的K80

0:28:53.300,0:28:57.920
我猜估计是16GB的GPU内存，因此你的批量规模可以更大一点到64

0:28:57.940,0:29:01.940
因此你需要根据你的GPU大小来确定批量大小

0:29:01.940,0:29:06.880
这里是我们的DataBunch, 这里看到的是标注

0:29:06.920,0:29:13.420
这次，不是构建一个语言模型学习器，而是构建一个文本分类模型学习器

0:29:13.420,0:29:18.780
同样的操作，选择数据，选择dropout正则化处理的需求量（drop_mult）

0:29:18.820,0:29:24.780
如果你的学习器是过拟合状态，可以调高drop_mult值；如果是欠拟合状态，需要调低drop_mult值

0:29:24.860,0:29:29.020
最重要的一步是加载我们的预训练模型learn.load_encoder

0:29:29.020,0:29:34.760
记住，我们加载的是预训练模型中的编码器（encoder）这一部分

0:29:34.840,0:29:41.060
然后，封冻模型（只训练到最后一个层组layer_group)，再用lr_find寻找最优学习率

0:29:41.100,0:29:50.700
然后训练一会3分钟，我们的精度就高达0.9189

0:29:50.760,0:29:59.660
这个非常有帮助，不论你的行业是法律，医疗，媒体，或政府

0:29:59.660,0:30:04.780
你可能只需用你所在行业的数据训练一次

0:30:04.840,0:30:09.680
而这可能只需要一晚的时间即可训练完成

0:30:09.760,0:30:16.820
一旦你完成了这个语言模型的训练，你可以衍生出各种分类器和模型

0:30:16.920,0:30:20.740
比如这里的非常棒的分类器，我们只用了三分钟训练完成

0:30:20.780,0:30:27.160
当你第一次尝试时，可能觉得很麻烦

0:30:27.240,0:30:31.960
因为你的第一个模型耗费4小时甚至更久来训练

0:30:31.980,0:30:38.000
但是你要记住的关键是，面对后续你可以创造的这个领域里的所有模型，这（件麻烦事）你只需做一次就够了

0:30:38.040,0:30:42.920
因为有了这个语言模型，所有这些分类器和模型的构建和训练只需几分钟

0:30:42.920,0:30:49.480
我们保持这个模型，所以下次就不要在重新训练了

0:30:49.540,0:30:53.860
这里很有趣的是，我会花几分钟来解释一下

0:30:53.900,0:30:58.100
我在这里不用unfreeze,而是采用freeze_to

0:30:58.100,0:31:03.360
意思是只对最后两个层组（layer_groups）做训练，其他层全部封冻不训练

0:31:03.400,0:31:12.400
我们发现针对这些文本分类问题，不解冻全模型，一层一层解冻，帮助更大

0:31:12.480,0:31:15.980
这里先解冻最后2层，训练一下

0:31:16.000,0:31:20.300
再多解冻一层，继续训练

0:31:20.400,0:31:24.060
最后全部解冻，再继续训练

0:31:24.080,0:31:29.780
你还会看到，这里有动量（momentum），设置为（0.8， 0.7）

0:31:29.780,0:31:34.360
我们会在下面1-2周里学到动量momentum的使用

0:31:34.440,0:31:42.980
我们正在尝试自动化这一流程设置，等到下周时，也许这一步已经内置了，因此无需讲解这一点了。

0:31:43.040,0:31:53.180
基本上，我们发现在训练循环神经网络RNN时，降低动量momentum对训练有帮助

0:31:53.380,0:32:01.820
那么，就过半小时左右的训练，精度可以提上到0.944

0:32:01.900,0:32:05.840
事实上，训练这个分类器所用时间更短（10分钟）

0:32:05.880,0:32:11.720
我们可以进一步提升模型表现，通过使用一系列小技巧

0:32:11.720,0:32:15.420
现在还不清楚是否会学所有的技巧，part1和 part2课程里会各学一部分

0:32:15.480,0:32:19.540
但即使只用目前我们看到的这个简单的训练流程

0:32:19.600,0:32:25.400
但效果已经很棒了，如果我们和去年的顶级水平SOTA（state of the art)对比

0:32:25.460,0:32:45.660
根据Salesforce Research研究人员在IMDB数据上的精度是91.8，另一片2017年论文针对具体领域的文本情绪分析取得了最高精度94.1

0:32:45.720,0:32:49.520
我们在这里取得了94.4

0:32:49.580,0:32:55.160
截至目前，我训练的最优模型精度在95-95.1左右

0:32:55.400,0:33:06.380
如果你要做文本分类任务，那么这个标准化的迁移学习是非常棒的

0:33:06.480,0:33:08.700
有问题吗？Rachel

0:33:10.820,0:33:14.440
这就是NLP的相关内容

0:33:14.520,0:33:17.360
后续课时中还会有更多介绍

0:33:17.420,0:33:21.840
现在我们要转换到表格数据任务(tabular)

0:33:21.920,0:33:25.520
表格数据是非常有趣的，因为

0:33:25.600,0:33:30.440
表格数据，是人们日常使用最多的数据

0:33:30.500,0:33:35.940
比如excel这样的Spreadsheet 电子表格和数据库（relational database）

0:33:36.020,0:33:44.060
(提问) 学习率中那个神奇的数字2.6**4是怎么来的

0:33:47.460,0:33:59.220
这是很好的问题！这里的学习率都是不同的值除以2.6**4

0:34:00.160,0:34:06.900
**4的原因会在今天课程结束时讲到

0:34:07.000,0:34:09.960
现在我们先来看看2.6， 为什么会有2.6？

0:34:13.980,0:34:18.360
我们稍后会有更多细节，先简单说一下

0:34:18.420,0:34:30.960
slice(___ , ___) 中的两个数值的差异是他们分别代表的是模型最底层（lowest layer)的学习速度和模型最高层(highest layer) 的速度

0:34:31.040,0:34:34.220
这就是判别学习率（discriminative learning rate）

0:34:34.440,0:34:41.240
它要解决的问题是，（从高到底）每一层学习率递减的幅度

0:34:41.300,0:34:46.900
我们发现对于NLP RNN而言，答案是（每一次递减的幅度是）2.6

0:34:46.940,0:34:49.480
我们是如何发现2.6的呢？

0:34:49.580,0:34:55.480
一年前，我跑了很多模型

0:34:55.520,0:34:58.620
采用了很多不同的超参数设置

0:34:58.660,0:35:02.900
例如dropout，学习率，判别学习率（discriminative learning rate）等等

0:35:02.920,0:35:05.560
然后我构建了随机森林(Random Forest)

0:35:05.720,0:35:11.660
它是一个模型，可以用于预测NLP分类模型的精度

0:35:11.720,0:35:14.100
基于超参数的设置

0:35:14.160,0:35:22.940
然后采用随机森林的解读函数（interpretation methods）来找出最优参数设置是什么

0:35:22.980,0:35:27.000
我发现针对这个超参数的最优参数值应该设置为2.6

0:35:27.040,0:35:32.160
我不认为将这个方法发表过，甚至可能从未提及过这个方法

0:35:32.180,0:35:35.460
所以，这是一个从未公开过的技巧信息

0:35:35.460,0:35:39.680
事实上，在我完成这个（随机森林）实验的几个月后，

0:35:39.880,0:35:45.900
David Marady和其他人一起发表了一个类似的方法

0:35:45.900,0:36:03.000
所以，这个方法的基本思路大家可以去网上了解了。其中的一些思路来自Frank Hatter和他的协作者的研究，他们发现可以用随机森林来寻找最优超参数

0:36:03.000,0:36:04.460
这是非常有用的技巧

0:36:04.520,0:36:08.700
很多人对AutoML感兴趣

0:36:08.700,0:36:12.940
也就是通过构建一个模型来寻找训练你其他模型的方法

0:36:13.040,0:36:15.960
我们不是这种方法的痴迷者

0:36:15.980,0:36:24.820
但我们发现构建模型来帮助理解超参数工作原理，然后找到这些超参数的简单使用规则

0:36:24.860,0:36:29.580
比如，这个超参数采用2.6总是很好用

0:36:29.620,0:36:36.900
这就是我们所说的之前在实验的方法。

0:36:37.900,0:36:43.180
好的，现在来说说表格数据(tabular data)

0:36:43.220,0:36:48.560
也就是你所见的Excel之类和数据库之类的数据

0:36:48.700,0:36:57.820
或者是财务表格之类，可以是包含各种数据信息。

0:36:58.020,0:37:04.620
我尝试将表格数据应用的领域做了一个列表（在这里）

0:37:04.620,0:37:17.680
使用神经网络来做表格数据研究，至少在去年我们展示这方面研究成果时，（我们这个方向的研究应该是2年前开始的）

0:37:17.780,0:37:22.620
当我们初次发表相关研究时，人们高度质疑其前景，认为这是个糟糕的研究方向

0:37:22.620,0:37:25.760
也就是用神经网络来做表格数据分析

0:37:25.940,0:37:31.680
因为所有人都知道应该用线性回归，随机森林，或者是gradient boosting machines来做才对

0:37:31.840,0:37:37.500
这些方法的确有它们自己擅长的领域。但自那以后（第一次发表）

0:37:37.680,0:37:45.120
这些所谓众所周知的常识，其实是错的

0:37:45.140,0:37:49.440
所谓的神经网络对表格数据不起作用是错误的，事实上，神经网络对分析表格数据非常有用。

0:37:49.500,0:37:52.580
我们已经在之前的不同课程里有证明这一点

0:37:52.600,0:37:56.840
还有一点也很有帮助的是，

0:37:57.040,0:38:07.880
一些非常知名的研究机构也开始发表论文和博文，介绍他们是如何使用神经网络来分析表格数据的

0:38:07.960,0:38:12.380
有一个反复出现的关键要素是

0:38:12.560,0:38:18.780
尽管特征工程(feature engineering)无法被免去，但的确已经变得非常简单

0:38:18.840,0:38:25.780
比如，Pinterest已经将原本用来做主页的内容筛选推荐的梯度提升器(gradient boosting machines)移除

0:38:25.880,0:38:28.520
而换上神经网络

0:38:28.560,0:38:34.000
他/她们还在学术会议上展示了这一改变使得特征提取变得非常简单

0:38:34.040,0:38:39.540
因为原本那些手动设计的特征工程特征提取已不再是必需品

0:38:39.580,0:38:43.640
你还是需要一些，但已经变得非常简单

0:38:43.640,0:38:49.220
结果是她/他们不经提升了精度而且需要的维护也更少了

0:38:49.420,0:38:58.280
我不会告诉你说，神经网络是你唯一所需的处理表格数据的工具

0:38:58.500,0:39:06.420
之前当我在用机器学习做表格数据分析时，99%的情况下都会用随机森林

0:39:06.460,0:39:10.880
而现在90%的情况下，我都是在用神经网络做表格数据分析

0:39:10.880,0:39:15.080
神经网络，已经成为我的标准常规的首选个实验工具

0:39:15.100,0:39:19.280
现在，神经网络这个工具已经非常可靠和高效了。

0:39:19.340,0:39:22.920
导致使用神经网络做表格数据很困难的原因是

0:39:23.080,0:39:29.900
因为截至目前，仍旧没有一个非常简单的构建和训练处理表格数据的神经网络

0:39:29.900,0:39:33.060
因为没人将它通过库/library分享出来

0:39:33.100,0:39:38.820
所以，我们刚刚完成了fastai.tabular

0:39:38.940,0:39:44.680
我认为这是有史以来第一次让

0:39:44.700,0:39:48.260
表格数据分析能够从一个库/library中调用上神经网络

0:39:48.260,0:39:50.140
让我来展示一下现在这是多么容易的一件事

0:39:50.220,0:39:56.560
这里的代码来自fastai repo的example案例文件夹

0:39:56.560,0:39:59.680
我没有做任何改动

0:39:59.700,0:40:04.660
常规操作，先加载库，fastai库，然后是fastai.tabular 库

0:40:04.920,0:40:12.460
我们默认你的数据格式是pandas.dataframe

0:40:12.500,0:40:18.180
pandas.dataframe 是在python里处理表格数据的标准格式

0:40:18.340,0:40:24.280
读取数据的方法很多，可能最高效的方法是使用pd.read_csv

0:40:24.460,0:40:29.440
不论你的数据是什么，用这种方法能便捷转化成pandas.dataframe

0:40:34.900,0:40:37.580
好，把问题“砸”向我

0:40:40.980,0:40:45.500
（提问）那些你不用神经网络的10%的情况是怎样的情况

0:40:48.940,0:40:50.940
好问题

0:40:53.200,0:40:55.660
我想我还是会用神经网络来试试（这10%的情况）

0:40:57.900,0:41:05.660
但是，我不知道，你看当你对某些事驾轻就熟时

0:41:05.740,0:41:10.840
你就会产生直觉知道哪些领域方面哪些技巧可能不管用

0:41:10.940,0:41:14.080
我这周会想想这个问题，当下我没有一个明确判断方法给到大家

0:41:14.140,0:41:23.680
但我想说，为什么不两个都试试呢？它们两个都很简单，高效的训练方法，跑一下，看看结果

0:41:23.680,0:41:29.420
如果结果相似，可以都分别深入研究，看是否能进一步改进效果

0:41:29.420,0:41:34.320
如果随机森林效果明显更好，那就使用随机森林

0:41:34.340,0:41:37.120
反正什么有效果就用什么

0:41:40.740,0:41:48.940
现在看到的notebook有误，只会会更新正确的notebook到课程repo（github repository）中

0:41:48.940,0:41:50.720
抱歉

0:41:51.940,0:41:56.400
我们从数据格式dataframe开始

0:41:56.620,0:42:05.820
我们采用了ADULT_SAMPLE, 这是一个经典的老数据

0:42:05.820,0:42:08.860
我想要回头找找数据出处，然后记录到notebook中

0:42:09.100,0:42:15.720
这是一个很小的老旧数据集，但适合用来做（表格数据）实验

0:42:15.940,0:42:21.800
数据在CSV文件中，所以你可以用pd.read_csv来读取数据

0:42:22.020,0:42:28.220
如果你的数据在数据库中，pandas也能读取；

0:42:28.280,0:42:31.140
如果是在spark或Hadoop里，pandas也能读取

0:42:31.200,0:42:34.340
pandas能从绝大多数数据源/格式中调取数据

0:42:34.340,0:42:38.220
这就是为什么我们现在pandas作为默认起步操作

0:42:38.300,0:42:47.500
继续常规操作，我们可以有data_block API

0:42:47.560,0:42:51.960
这里我们创建的是TabularList

0:42:52.000,0:42:54.680
用from_df从dataframe中提炼

0:42:54.840,0:43:01.120
接下来需要选择你的dataframe, 提供模型保存地址，以及其他中间步骤

0:43:01.160,0:43:07.200
然后需要告诉模型，类别变量（categorical variables）和连续变量(continuous variables)是什么

0:43:07.320,0:43:13.680
下周我们会学到这些（类别和连续变量）对神经网络意味着什么

0:43:13.700,0:43:17.040
现在，快速总结一下

0:43:17.060,0:43:21.500
你的自变量（independent variable）是你用来训练模型做预测

0:43:21.660,0:43:27.380
例如，教育经历，婚姻状况，年龄等

0:43:27.420,0:43:35.720
有些变量，如年龄，基本上是数字

0:43:35.760,0:43:40.060
可以是任意数字，比如13.6岁

0:43:40.180,0:43:42.180
或者19.4岁，等等

0:43:42.240,0:43:50.480
但像婚姻状况这样的变量，则可以提供一系列离散选项

0:43:50.520,0:43:53.200
已婚，离异，单身等等

0:43:53.300,0:43:58.740
有些变量的可选项很多，如职业

0:43:58.740,0:44:04.900
有些变量，是二元的，真或假

0:44:05.080,0:44:14.900
但所有可以从一组可选项中选择的变量，都叫类别变量

0:44:15.140,0:44:23.100
在使用神经网络学习对类别变量和连续变量建模时，我们将采用不一样的方法。

0:44:23.140,0:44:28.460
对类别变量，我们采用embeddings方法，今天稍后将学到

0:44:28.460,0:44:34.420
对于连续变量，我们可以对它们像对像素一样，输入给神经网络

0:44:34.460,0:44:40.060
因为图片像素和连续变量的值都是数值

0:44:40.080,0:44:43.040
所以这个处理比较简单

0:44:43.080,0:44:52.640
这也是为什么我们需要告诉TabularList谁是类别谁是连续变量的原因

0:44:52.640,0:44:56.380
也有其他的方法，比如在pandas中对数据做预处理

0:44:56.400,0:44:59.300
将一些变量转化为类别变量

0:44:59.340,0:45:03.040
但是用一个API完成所有步骤，更简单方便，不要想太多

0:45:03.240,0:45:13.540
然后，就是一个像机器视觉中的叫变形设置transforms的东西

0:45:13.600,0:45:18.520
机器视觉中的变形设置，能让图片按坐标轴翻转

0:45:18.520,0:45:20.860
或者做旋转，提高亮度，以及归一化处理

0:45:21.000,0:45:28.560
但是对于表格数据，我们不要变形设置，而是用预处理processes

0:45:28.600,0:45:33.060
它与变形设置很相似，但关键不同在于

0:45:33.120,0:45:36.700
预处理是提前发生的

0:45:36.860,0:45:43.380
预处理是预先处理好dataframe, 而不是一边训练一边处理数据

0:45:43.400,0:45:47.880
变形设置，主要是为了数据增强(data augmentation)

0:45:47.920,0:45:50.580
（在训练时）对数据样本做随机处理，每次变形都不会一样

0:45:50.600,0:45:54.460
而预处理，你只会做一次，而且是再训练前

0:45:54.500,0:45:59.880
fastai库提供了一系列预处理功能

0:46:00.020,0:46:09.880
这里我们会用到FillMissing, 功能是对缺失数据做处理（填补）

0:46:09.900,0:46:15.960
然后会要出类别变量，将它们变成pandas.categories (pandas 的类别数据格式）

0:46:16.000,0:46:19.900
然后还会做归一化的预处理

0:46:19.920,0:46:29.700
也就是对连续变量做归一化（减去它们的均值，除以它们的标准差）得到一个均值为0标差为1的数据分布

0:46:29.760,0:46:36.700
我们具体处理确实数据的方法，会在下周讲到

0:46:36.720,0:46:39.460
简单的说，会用中位数来取代缺失值

0:46:39.560,0:46:44.560
然后再加上一列数，都是二元数值，表明是否是缺失填补的数值

0:46:44.760,0:46:53.780
以及归一化，实际上是对所有预处理都一样，

0:46:53.780,0:46:55.300
不论对训练集做什么

0:46:55.340,0:46:58.220
都需要对验证集做同样的处理

0:46:58.220,0:46:59.320
和测试集

0:46:59.400,0:47:02.320
不论你如何填补缺失数据

0:47:02.360,0:47:05.800
你需要对验证集做一样的处理

0:47:05.800,0:47:08.560
fastai代替你处理所有这些细节

0:47:08.620,0:47:15.300
（代替你做的这些细节）如果自己手动去做，如果你像我一样，完全做对之前肯定会多次出错的

0:47:15.300,0:47:19.740
这就是我们所说的预处理

0:47:19.840,0:47:27.220
然后，将数据分割成训练集和验证集

0:47:27.220,0:47:32.600
这一次我们用序号列表（a list of indices）来帮助分割

0:47:32.600,0:47:35.120
这里的序号是从800到1000

0:47:35.120,0:47:37.760
这个很常见，虽然我不理解这个数据的具体细节

0:47:37.760,0:47:41.040
但非常常见的是，希望验证集

0:47:41.040,0:47:44.440
是连续比邻的一组数据

0:47:44.440,0:47:48.020
如果数据是地图片区，那么就应该相邻的地图片区

0:47:48.060,0:47:52.800
如果数据是时间序列，那么就应该是相邻的日期/days

0:47:52.860,0:47:55.900
如果数据是视频帧，那么就应该是相邻的视频帧

0:47:55.900,0:47:57.860
否则，你就是在作弊

0:47:57.940,0:48:06.880
所以，好的做法是用split_by_idx，然后截取一段连续的序号，如果你的数据有这样特点结构的话

0:48:07.020,0:48:09.180
或者用其他的方法来达到相同目的

0:48:09.220,0:48:14.260
那么这样我们就有了训练集和验证集

0:48:14.320,0:48:19.400
然后，设置标注，标注可以直接来自一开始生成的dataframe

0:48:19.400,0:48:22.100
我们只需告知是那一列数据（用来做标注）

0:48:22.160,0:48:26.360
我们的应变量(dependent variable), 这里是“是否超过50，000美元”这一列数据

0:48:26.380,0:48:32.500
也就是收入，这是我们要预测的

0:48:32.500,0:48:36.080
我们之后会讲到测试集，这里我们可以添加测试集

0:48:36.080,0:48:39.100
这样，我们就有了DataBunch

0:48:39.160,0:48:43.780
用.show_batch打开databunch，我们可以看到这样的数据展示

0:48:43.880,0:48:49.180
这就是我们的数据

0:48:49.260,0:48:54.440
要使用databunch的操作，我们也非常熟悉

0:48:54.580,0:48:58.160
先构建一个学习器，这里要用的是tabular learner表格数据学习器

0:48:58.260,0:49:03.740
植入数据，模型结构，和评估工具

0:49:03.840,0:49:07.380
然后fit来训练

0:49:08.240,0:49:11.100
我们来看看提问

0:49:15.220,0:49:21.840
（提问）如何用fastai将NLP的分词与元数据（meta data，描述数据的数据） 比如表格数据相结合

0:49:21.980,0:49:29.020
比如，IMDB分类问题，如何利用演员表，制作年份，题材风格等信息

0:49:31.740,0:49:33.740
这方面，我们还没准备好

0:49:33.840,0:49:38.040
因此我们还需要对此进一步学习神经网络模型结构工作原理

0:49:38.060,0:49:45.020
理论上，方法应该和之前我们的植入类别变量和连续变量的方法一致

0:49:45.040,0:49:51.740
基本上，在神经网络中，你可以有两组不同的输入值

0:49:51.740,0:49:55.540
合并在一起，在一同进入某层中

0:49:55.540,0:49:59.440
既可以选择进入前面某层(early layer)也可以选择进入末尾某层(later layer)，因情况而定

0:49:59.440,0:50:03.240
如果这里有文本数据，图片数据，以及元数据（meta data）

0:50:03.460,0:50:10.360
你可以要让文本进入RNN，图片进入CNN，元数据进入像这样的表格数据学习器

0:50:10.480,0:50:15.820
然后你可以将它们做合并（并列摆放），输送给某些全联接层

0:50:15.820,0:50:17.040
对它们做端到端训练(end-to-end)

0:50:17.100,0:50:23.540
我们很可能要在Part2才会学到部分或全部的这些操作处理

0:50:23.540,0:50:26.960
现在还不确定是否part1里有足够时间来覆盖这些技巧

0:50:27.280,0:50:33.440
但是从概念上看，这些方法就是我们接下来三周时间里要学的技巧的简单延伸

0:50:36.060,0:50:46.380
（提问）你认为scikit learn 和XGBoost会日益过时吗？因为未来多数人会用深度学习，只有在面对小数据集时才会去用scikit learn 和XGBoost？

0:50:50.960,0:50:52.960
我不知道，我不擅长做预测。

0:50:54.240,0:50:57.880
我不是一个机器学习模型

0:51:01.820,0:51:04.560
我认为XGBoost是一个非常棒的软件

0:51:04.600,0:51:08.180
其实梯度提升领域有一些非常棒的软件

0:51:08.200,0:51:14.880
它们都有不错的功能，尤其是随机森林有非常棒的解读功能

0:51:15.080,0:51:20.740
我们肯定会有类似的功能给到神经网络，但现在没准备好

0:51:20.940,0:51:27.280
我不知道，现在它们两都是有用的工具

0:51:27.520,0:51:36.660
scikit-learn 是一个用于数据处理和建模的库

0:51:36.720,0:51:42.880
很难预测未来会怎样

0:51:42.940,0:51:47.040
某种意义上，它更聚焦于早期的老方法来建模

0:51:47.060,0:51:52.360
但是它也在增加新内容

0:51:52.380,0:52:02.220
我一直尝试将scikit-learn 融入到fastai中，但又总是发现更好的办法，所以又从fastai中移除了scikit-learn

0:52:02.240,0:52:06.700
这就是为什么目前为止fastai中依旧找不到scikit-learn的身影

0:52:06.700,0:52:09.680
因为总能找到其他更好的方法

0:52:12.440,0:52:22.780
我们要么在今天结束前要么下周开始时，学习layers=[200, 100]的含义

0:52:22.820,0:52:28.760
但基本上，这是我们如何设计模型结构，就像Resnet34的用意一样

0:52:28.800,0:52:33.820
稍后我们会讲到评估工具(metrics)

0:52:33.860,0:52:39.280
快速回顾一下，评估工具就是这些被打印出来的数字，它不会改变模型效果

0:52:39.280,0:52:43.060
这里我们在打印精度，看看模型的表现

0:52:45.260,0:52:49.960
好的，这就是如何做表格数据处理和建模

0:52:50.020,0:52:53.060
目前进展很顺利，因为很快我们将达到“分水岭”

0:52:53.080,0:53:00.100
意思是，在3节半个课时之后，我们将完成所有应用的初步探索，然后再开始对它们做更深入的学习

0:53:00.160,0:53:04.180
我们将刚刚好踩点完成分水岭前半部分的内容

0:53:04.280,0:53:08.260
接下来的内容是collaborative filtering 协同过滤

0:53:08.300,0:53:18.000
collaborative filtering 协同过滤 是你有的信息是关于

0:53:18.800,0:53:22.180
誰买了什么，或者是誰喜欢什么

0:53:22.260,0:53:29.260
基本上是关于用户，观众等等之类的数据信息

0:53:29.320,0:53:36.080
比如，他/她们买了什么，写了什么和观看和评论了什么

0:53:36.120,0:53:39.220
所以，最简单的协同过滤

0:53:39.300,0:53:45.100
可以只有2列数据，一个是用户ID，一列是电影ID

0:53:45.140,0:53:49.560
数据内容无非是，这个用户买了这个电影，那个用户买了那个影片

0:53:49.680,0:53:56.420
所以，亚马逊拥有非常巨大的用户ID和产品ID（你买的东西）列表

0:53:56.460,0:53:58.800
然后，你可以在这个表格数据基础上增加额外信息

0:53:58.840,0:54:04.220
比如，用户留下了评论，评论内容是一类信息

0:54:04.220,0:54:08.580
这样就有（一个新表格，里面包含）用户ID，电影ID，评分星级

0:54:08.700,0:54:17.060
你还可以增加时间信息，这个用户在这个时间点购买了产品并给出了这个评级

0:54:17.140,0:54:21.560
但它们基本上都是一样的结构

0:54:21.600,0:54:27.400
大体有两种构造协同过滤（collaborative filtering）的数据结构的方法

0:54:27.580,0:54:36.940
一种方法是2列的表格数据，一列用户，一列电影

0:54:37.040,0:54:44.280
每一对数据（一个用户ID，一个电影ID）表明这个用户观看了这部电影

0:54:44.340,0:54:49.200
可能还可以加上星级评分（第三列）

0:54:49.200,0:54:53.020
比如3，4，1 星等等

0:54:53.160,0:55:03.440
另一种构建数据的结构是这样，用户都在这（行rows）

0:55:03.600,0:55:12.420
所有的电影都在这里（列/columns）

0:55:12.620,0:55:25.060
然后你可以找到其中任意一格/Cell

0:55:25.140,0:55:31.700
里面值可以是用户给予电影的评级；或者是用户看过此电影，如果数值是1的话；或其他

0:55:31.780,0:55:35.640
这是2种不同方法但展示的是相同的数据信息

0:55:35.840,0:55:43.940
从概念理解上看，第二种方法似乎更容易一点

0:55:44.140,0:55:51.240
但多数情况下，你不会将数据以这种方式存储，因为一个叫sparse matrix（稀疏矩阵）的存在

0:55:51.240,0:55:55.780
也就是说大多数用户还没有看过大多数电影

0:55:55.860,0:56:00.480
或者说大多数客户还没有购买过大多数商品

0:56:00.540,0:56:09.000
如果你要将（用户看过电影，客户买过商品的）两两组合的信息植入矩阵的一个独立格子（Cell）中，那么这个矩阵会异常巨大

0:56:09.040,0:56:13.220
所以，你通常这么存储，或者

0:56:13.280,0:56:17.740
你可以这么存储，但要用某种稀疏矩阵格式来存

0:56:17.780,0:56:24.400
如何这个听起来有意思的话，你应该去试试Rachel的Computational Linear Algebra 课程

0:56:24.400,0:56:30.420
在那里我们有特别多的关于稀疏矩阵的知识

0:56:30.480,0:56:36.040
现在在这里我们主要采用左边的数据结构

0:56:36.100,0:56:45.520
对于协同过滤，有一个很棒的数据集叫MovieLense

0:56:45.620,0:56:54.120
由grouplens团队创建，你可以下载不同版本的数据集

0:56:54.200,0:57:05.700
有2000万影评版本，10万影评版本，我们还创建了一个更小版本，这是我们今天要用的版本

0:57:05.820,0:57:12.100
可能下周我们会用到更大版本的数据集。

0:57:12.160,0:57:17.140
我们可以用URLs.ML_SAMPLE来获取小版本数据

0:57:17.180,0:57:23.020
数据是CSV格式，可以用pd.read_csv读取

0:57:23.080,0:57:29.760
这就是我们的数据，这里是用户ID，我们并不知道具体用户是谁

0:57:29.840,0:57:35.660
这里是电影ID，数据集中含有电影信息，但我们只会在下周里用到

0:57:35.680,0:57:40.500
还有就是评级信息rating和时间戳timestamp

0:57:40.540,0:57:42.780
现在我可以忽略时间戳(timestamp)

0:57:42.800,0:57:52.260
这里我们用head()来查看前几行数据

0:57:52.280,0:57:58.700
现在我们有了一个dataframe. 协同过滤任务很方便的地方是

0:57:58.760,0:58:03.840
我们所需的整个数据集就是这样一个简单的结构

0:58:03.940,0:58:10.800
然后，我们可以构建学习器，用get_collab_learner, 然后直接植入我们dataframe

0:58:10.800,0:58:16.740
关于模型结构，我们只需要告诉需要用n_factors多少个factors

0:58:16.860,0:58:18.860
中场休息后，我们会学到什么是n_factors

0:58:18.900,0:58:24.080
如果告诉score的范围，会比较有帮助

0:58:24.160,0:58:26.380
这些也是在中场休息之后会讲到

0:58:26.380,0:58:30.440
这里的score范围是，最小score是0，最大score是5

0:58:30.460,0:58:36.120
有了学习器后，就可以训练了，用fit_one_cycle

0:58:36.180,0:58:41.600
训练几轮之后，这就是我们的结果

0:58:41.640,0:58:52.660
最后，我们有一个（学习器），再给予用户ID和电影ID后，就能预测这个用户是否会喜欢这个电影

0:58:52.880,0:58:59.580
这显然是一个非常有用的应用

0:58:59.580,0:59:02.720
所以很多人在之前的课程中都尝试跑过这个实验

0:59:02.720,0:59:07.560
也有人将这个方法用到自己工作中

0:59:07.620,0:59:13.260
但会发现在实际工作中使用这种方法，会比我们看到的要复杂的多

0:59:13.360,0:59:17.420
因为在实际环境中，存在一个叫cold start problem 冷启动问题

0:59:17.480,0:59:25.180
所谓cold start problem 冷启动问题 是因为当你特别在意推荐电影的时候，往往是当你有一个新用户时

0:59:25.320,0:59:31.460
当你特别关心推荐电影时候，往往是面对一个新电影时

0:59:31.520,0:59:36.580
然而这种情况下，你没有相关数据在你的协同过滤系统中，所以很难

0:59:36.720,0:59:43.460
然而目前fastai中暂时还没有任何可以解决cold start problem 冷启动问题的工具

0:59:43.520,0:59:50.700
因为cold start problem 的我所知道的唯一解决方案，或者说唯一合乎逻辑的解决方案

0:59:50.780,0:59:54.880
是采用另一个模型，不是协同过滤模型，而是基于元数据的模型（meta data driven model）

0:59:54.960,0:59:59.660
来应对新用户，新电影

0:59:59.740,1:00:06.280
我不知道网飞(Netflix)是否仍旧这么做，至少当初我开始使用网飞时

1:00:06.340,1:00:08.600
它一上来会给我展示很多电影

1:00:08.640,1:00:12.040
问我是否看过这个，是否喜欢它

1:00:12.180,1:00:16.540
所以他们通过用户体验（UX）来解决冷启动问题

1:00:16.600,1:00:24.080
所以他们不存在冷启动问题，因为他们找来20个特别常见的电影，问我是否喜欢

1:00:24.120,1:00:27.780
他们用我20个回答找来另外20个我有可能看过的电影，问我是否看过

1:00:27.820,1:00:33.260
当我们完成60个电影问答后，已经不存在冷启动问题了

1:00:33.320,1:00:36.900
对于新电影，这通常不是问题

1:00:36.960,1:00:48.320
因为通过对前100个没看过这个电影的用户做询问是否喜欢，那么后续的10万，100万个用户就不会存在冷启动问题

1:00:48.420,1:00:56.320
另一件可做的工作是， 如果因为某些原因你无法完成用户体验来询问用户是否喜欢某个东西，

1:00:56.400,1:01:05.780
比如，你在销售商品，你不会希望给用户一大堆产品，问客户是否喜欢，因为你希望他们直接购买（商品数量太多，不现实）

1:01:05.960,1:01:16.980
你不如采用元数据表格模型，使用数据包括用户地理位置，年龄和性别，来做最初的推荐猜测

1:01:16.980,1:01:29.980
所以，协同过滤适用的情况是，当你已经有了些许关于你的用户和电影，或用户和产品的信息，等等

1:01:37.060,1:01:46.760
（提问） 对于含有或是用部分英语单词的印度语(Hindi written in English words)以及含有表情符号的文本，我们这里的语言模型应该如何训练？

1:01:47.960,1:01:49.960
要带上第二个问题吗？

1:01:50.640,1:01:54.860
是的，这是一个好问题

1:01:57.220,1:02:02.360
含有表情的（emojis）文本，没问题

1:02:02.440,1:02:06.200
维基百科中的表情符号不多

1:02:06.320,1:02:14.520
当它们出现时，也是维基百科解释表情符号，而不是这些表情符号以日常使用方式出现

1:02:14.720,1:02:24.120
但是你可以也应该做这样的语言模型微调

1:02:24.160,1:02:28.460
你可以找来文本里面包含表情符号的日常使用的例子

1:02:28.660,1:02:35.360
因此，你可以用你的推特文本，或redit文本来微调训练基于wiki103训练好的模型

1:02:35.400,1:02:44.540
其实没那么多表情符号，我们有几十万个不同单词，但只有极少的表情符号

1:02:44.580,1:02:49.020
所以，模型能很快学会如何使用表情符号

1:02:49.020,1:02:51.860
（对模型而言）这个太简单了

1:02:51.940,1:02:59.260
我不熟悉印度语，但我非常熟悉的是中文（中国大陆mandarin）

1:02:59.300,1:03:04.560
你可以用中文文字来训练模型

1:03:04.600,1:03:09.260
常用中文字词有5-6000个

1:03:09.260,1:03:13.140
同时也有中文的罗马化（romanization) 表达，也就是拼音(pinyin)

1:03:13.300,1:03:21.140
这里有些难点挑战，虽然中文字到拼音，几乎可以做到一一对应

1:03:21.160,1:03:25.160
但是文字与发音还无法一一对应

1:03:25.220,1:03:29.380
但是不能从拼音到文字的一一对应

1:03:29.380,1:03:33.160
因为一个拼音可以对应多个文字

1:03:33.200,1:03:44.100
首先要记住的是，如果要用这套方法来训练基于中文的文本分类器

1:03:44.100,1:03:46.960
你需要从一个基于中文的语言模型开始

1:03:46.960,1:03:51.920
所以，fastai有一个叫model zoo模型动物园的东西

1:03:51.980,1:03:56.560
在这里，我们不断添加越来越多的不同语言的语言模型

1:03:56.580,1:04:00.720
同时也在增加不同领域的模型，比如

1:04:00.820,1:04:07.280
英文的医疗文本，以及非文本类的针对基因序列的语言模型

1:04:07.280,1:04:11.840
还有针对细胞分子的，针对音乐音符的，等等

1:04:11.900,1:04:15.420
这就是你可以尝试的路径起点

1:04:15.480,1:04:28.360
如果你想将简体/繁体中文转换成拼音，一种方法是你可以

1:04:28.360,1:04:32.240
做单词表（voca）的一一对应

1:04:32.280,1:04:43.740
或者随着学习的深入，你会了解到，其实只有第一层神经网络是在将分词（tokens）转化为数组（vectors)

1:04:43.740,1:04:45.660
你也可以把它们抛弃掉

1:04:45.720,1:04:49.160
只是微调模型的第一层

1:04:49.160,1:04:58.040
所以，刚刚说的这第二步操作，需要未来几周的学习，才能帮助你更好的理解

1:04:58.040,1:05:01.840
如果这是你现在就想做的，我们可以在论坛里继续讨论

1:05:01.900,1:05:05.660
这个任务也算是一个很好验证我们的理解

1:05:09.120,1:05:15.860
（提问）时间序列（time series）在表格数据中的应用时，是否会用到RNN

1:05:15.900,1:05:23.920
我们会在下周用到时间序列的表格数据

1:05:24.020,1:05:33.460
来回答你的问题，通常情况下，你不用RNN来处理时间序列的表格数据

1:05:33.560,1:05:43.340
但你会提炼出几列新数据，例如星期几，是否是周末，是否是节假日，是否正常营业，等等

1:05:43.420,1:05:49.720
我们发现，增加这些新数据，当然这些数据生成是可以自动化的

1:05:49.800,1:05:54.060
能给我们state of the art 最先进的模型表现

1:05:54.220,1:06:05.740
的确有不错的RNN在时间序列中的应用，但不是那种表格数据中的时间序列

1:06:05.740,1:06:10.900
例如，像零售店物流的数据库数据，类似这样的。

1:06:14.560,1:06:18.340
（提问） 是否有相关深入学习冷启动问题的资源

1:06:23.140,1:06:25.560
我需要去找找

1:06:26.100,1:06:29.820
如果谁有不错的资源，请在论坛里告知大家

1:06:35.180,1:06:40.520
好的，到这里为止，我们已经到了中场休息时间

1:06:40.520,1:06:44.320
这里也是整个part1课程的前半部分的截止点

1:06:44.500,1:06:51.400
截至现在我们也了解到了所有的应用

1:06:51.400,1:06:57.440
所以，接下来的下半部分就是进一步深入挖掘和学习这些应用背后的工作机制

1:06:57.660,1:07:03.520
我们会学到背后的理论，源代码是怎么写的，等等

1:07:03.720,1:07:12.820
所以这里是一个不错的休息时机，另外，今天是我的生日（幸亏是全屏录制，终于被我看到时间了，11.13号的生日！）

1:07:12.880,1:07:16.040
所以，这是一个特殊的时刻（yeah，happy birthday Jeremy!)

1:07:16.200,1:07:23.100
让我们休息一下，8:05PM回来

1:07:29.640,1:07:35.780
微软的Excel是我最喜欢的探索数据的工具

1:07:35.780,1:07:38.620
和理解模型的工具

1:07:38.700,1:07:43.980
我会确保（这些excel）大家能在repo中找到

1:07:44.020,1:07:48.780
这个内容，可以大部分可以用谷歌sheet来做

1:07:48.840,1:07:55.620
我不断尝试将Excel内容搬到谷歌Sheet，但发现谷歌Sheet实在太难用了

1:07:55.680,1:08:03.480
所以，请找一个微软的Excel版本，因为

1:08:03.520,1:08:07.040
真的，无可匹敌，我试过所有可能的代替品

1:08:07.040,1:08:16.620
Spreadsheet 电子表格的口碑不好，主要是来自不太会使用的人群

1:08:16.640,1:08:22.540
就像一直靠python生活的人，突然间使用Excel会说：这也太难用了吧

1:08:22.540,1:08:30.280
要特别精通Spreadsheet 电子表格你需要上百上千小时的使用，但能擅长它，数个小时就行

1:08:30.320,1:08:36.820
一旦你能胜任Spreadsheet 电子表格的使用，你会发现所有数据被你一览无余，感觉超棒！

1:08:37.020,1:08:46.100
今天我给你们一个Excel技巧，如果你按住ctrl/cmd + 箭头键，

1:08:46.120,1:08:53.740
例如，箭头上下，帮助你快速在一块数据矩阵的头尾来回移动，箭头左右，帮助你快速在其左右两端来回移动

1:08:53.840,1:08:57.120
这是目前最高效的快速移动方法

1:08:57.160,1:09:03.920
现在我想快速在数据间来回跳跃

1:09:03.920,1:09:06.940
按下ctrl/cmd + down + right 能快速跳到数据矩阵的最下方再到最右方

1:09:06.980,1:09:12.200
ctrl/cmd + up + left 到左上方，这样一来就能快速了解数据情况

1:09:12.240,1:09:18.200
这里是一些数据

1:09:19.600,1:09:23.560
一种了解协同过滤数据集的方法是这样

1:09:23.600,1:09:40.440
从Movielens数据中筛选看电影最多的15名用户，和被看最多的15部电影，它们自然合成的矩阵

1:09:40.660,1:09:48.480
你会看到，对你这么做时，矩阵也不再是稀疏状态，只有少数空白单元格

1:09:51.900,1:09:57.180
我们可以用这样的数据来建模

1:09:57.420,1:10:04.380
那么我们如何建模呢？

1:10:04.400,1:10:15.120
我们要尝试的是，做一个预测，比如用户293号是否会喜欢电影49号

1:10:15.320,1:10:24.280
我们需要创建一个函数，来表达这个决定/判断/预测

1:10:26.480,1:10:31.800
这里有一个简单的解决方案

1:10:31.840,1:10:35.680
我们要采用这个矩阵乘法的思路

1:10:35.680,1:10:41.820
所以，在这里我创建了一个随机的矩阵

1:10:41.880,1:10:45.060
这里是随机值矩阵

1:10:45.200,1:10:50.320
这里是另一组随机值矩阵

1:10:50.380,1:10:56.880
具体而言，我们为每个电影创造了5个随机值（一个数组）

1:10:56.880,1:11:03.300
也为每个用户创造了5个随机值(一个数组）

1:11:03.360,1:11:08.860
所以我们可以说

1:11:08.880,1:11:15.440
用户14号，电影27号

1:11:15.520,1:11:18.360
用户是否喜欢这个电影？

1:11:18.520,1:11:26.820
关于电影评级（rating）我们可以做的是，让这两个数组相乘（点积，dot product)

1:11:26.860,1:11:32.160
这里就是我们的点积

1:11:32.180,1:11:39.520
然后，对于每一个单元格，我们做相同的操作

1:11:39.520,1:11:46.800
感谢Spreadsheet 电子表格，我们只需一次操作，后面复制粘贴即可

1:11:46.800,1:11:50.000
为什么我们要这么操作（点积）？

1:11:50.020,1:11:55.140
因为这是神经网络的基本步骤，也算是神经网络的起点

1:11:55.320,1:12:01.860
神经网络的起点，就是对两个矩阵做矩阵乘法

1:12:01.860,1:12:05.720
这是你神经网络的第一层的不变本质

1:12:05.860,1:12:11.760
所以，我们只需要思考要用怎样的矩阵来做矩阵乘法

1:12:12.060,1:12:21.460
很明显，我们需要一个矩阵给到用户

1:12:21.460,1:12:23.340
或者（更准确）一个用户对应一个数组（vector）

1:12:23.340,1:12:26.180
一个矩阵对应所有的用户

1:12:26.260,1:12:31.760
一个数组对应一个电影，或者一个矩阵对应所有电影

1:12:31.760,1:12:38.440
然后，对它们做矩阵乘法（不再是两个数组的点积），得到一些数值（不是一个数值）

1:12:38.500,1:12:43.640
这些数值还没有意义，它们都是随机数，但现在我们可以

1:12:43.640,1:12:58.960
用梯度下降，来让这些数值（电影矩阵）和这些数值（用户矩阵）给我们生成新数值，来尽可能的接近这些数值（真实目标值）

1:12:59.020,1:13:01.520
那要怎么做呢？

1:13:01.580,1:13:06.920
我们目前的设置是一个线性模型

1:13:07.000,1:13:10.420
下一步是需要一个损失函数

1:13:10.480,1:13:14.700
我们可以这么来计算我们的损失函数

1:13:14.740,1:13:29.020
这里我们看到对于电影27号，用户14号应该给予它了3分评级

1:13:29.100,1:13:33.160
但是我们随机矩阵得到的值是0.91

1:13:33.340,1:13:39.900
所以，残差平方和 sum squared error 的计算内容会是（3 - 0.91）** 2

1:13:44.840,1:13:47.800
然后，我们把它们都加起来

1:13:47.840,1:13:59.320
实际上在Excel中已经准备好了残差平方和函数，叫做SUMXMY2(sum x minus y squared)函数

1:13:59.320,1:14:02.900
植入这两个ranges（电影和用户矩阵）

1:14:02.920,1:14:05.720
除去它们（矩阵乘法后矩阵内数值）的数量，获得均值

1:14:05.940,1:14:13.900
这里的值，其实就是残差平方和的平方根

1:14:14.000,1:14:25.080
有时你会听到所谓的均方误差 MSE（mean squared error)， 有时则是均方根误差 RMSE（root mean squared error)

1:14:25.120,1:14:28.800
因为我们用了平方根，所以是均方根误差

1:14:29.960,1:14:33.700
因此，我们有了一个损失值

1:14:33.780,1:14:40.620
现在我们所需要做的就是用梯度下降来更新我们的参数矩阵

1:14:40.680,1:14:43.780
来让这个损失值更小

1:14:43.800,1:14:48.480
这个工作，Excel也会为我完成

1:14:48.560,1:15:01.360
我们需要通过Add-in来下载安装

1:15:01.360,1:15:03.940
Solver Add-in

1:15:04.180,1:15:12.300
Excel中的梯度下降的规划求解(solver)就是solver add-in

1:15:12.300,1:15:14.240
就是帮助完成常规的梯度下降

1:15:14.300,1:15:21.580
前往data, solver, 确保设置中（setting）的规划求解(solver extension)处于启动状态

1:15:21.600,1:15:26.240
然后，你所需要做的就是告知，哪个单元格代表损失函数

1:15:26.240,1:15:30.420
V41这个单元格就是你的损失函数所在

1:15:30.460,1:15:35.800
哪些单元格代表你的变量

1:15:35.860,1:15:41.160
H19到V23

1:15:41.200,1:15:45.100
然后是B25到F39

1:15:45.320,1:15:53.840
然后，就是让损失函数值选择最小值

1:15:53.840,1:15:57.320
来更新这些变量矩阵中的值

1:15:57.320,1:15:58.920
然后点击solve键

1:15:58.940,1:16:04.300
你会看到损失值从2.81开始，这里数值在不断下降

1:16:04.400,1:16:12.840
这里所做的一切，就是用梯度下降（就像我们之前的Notebook里手动做梯度下降一样）

1:16:13.000,1:16:19.560
只不过，不是在用均方误差来更新a和b这两个参数

1:16:19.800,1:16:32.640
这里是在求解这里的损失值，采用的均方根误差，所有预测值都是来源于这两个矩阵的矩阵乘法

1:16:32.860,1:16:41.580
这样就行了，我们让程序跑一会，再看看什么结果

1:16:41.620,1:16:50.860
我们可以设计一个macro宏来自动化创建神经网络

1:16:50.920,1:16:55.980
这里其实是构建了一个简单的线性函数

1:16:56.040,1:17:01.160
结合了梯度下降来求解协同过滤问题

1:17:01.360,1:17:08.580
现在，让我们回头看看，在Notebook中是怎么做的

1:17:08.760,1:17:14.600
这里我们用了get_collab_learner

1:17:14.940,1:17:20.460
Notebook中的函数是get_collab_learner

1:17:20.500,1:17:28.980
如果你想深入了解深度学习，一种方法是

1:17:28.980,1:17:32.720
深入挖掘fastai源代码，看看里面发生了什么

1:17:32.720,1:17:39.680
如果你想知道怎么做到这一点，你需要非常了解你的代码编辑器来深挖源代码

1:17:39.700,1:17:43.180
基本上，你需要知道做两件事

1:17:43.220,1:17:49.620
一个是根据名称就能跳入到某个类或函数中

1:17:49.620,1:17:54.340
另一个是，当你看到一个类或函数名称，你能够跳入到它的源代码中

1:17:54.340,1:17:59.700
这里，我想要找到get_collab_learner

1:17:59.700,1:18:05.740
多数编辑器，包括我用的vim，都可以设置好

1:18:05.740,1:18:18.260
这样你可以按下tab自动提供多个完整类或函数名称，然后按下enter 就能跳入到某个你选择的函数源代码中

1:18:18.260,1:18:22.360
这样我们就找到了get_collab_learner的源代码

1:18:22.520,1:18:29.840
如你所见，（这个函数）非常的小，它们通常都是这样（在fastai中）

1:18:29.920,1:18:36.300
这里所做的是将datafram包装起来，在做成一个DataBunch, 非常简单

1:18:36.460,1:18:41.760
但关键一步是构建一个模型，一个特定的模型

1:18:41.800,1:18:45.240
是叫EmbeddingDotBias类构建模型

1:18:45.240,1:18:47.020
然后给予这个函数所需的argument参数

1:18:47.100,1:18:50.940
所以，你需要了解自己的编辑器，看如何从某个函数名称跳入这个函数源代码

1:18:51.000,1:18:56.260
在vim中，你所需要做的就是，（鼠标在某个函数上时）按下ctrl +  ]

1:18:56.360,1:19:01.780
这样你就进入了EmbeddingDotBias的源代码

1:19:01.860,1:19:08.300
这样一来，所有我们需要的都在一个页面中了

1:19:08.340,1:19:11.120
如你所见，其实没有多少代码

1:19:11.140,1:19:21.460
这里fastai为你创建的模型，其实是pytorch 模型

1:19:21.460,1:19:27.060
这个pytorch的模型，叫做nn.Module

1:19:27.120,1:19:31.000
这是pytorch中的模型的名称

1:19:31.000,1:19:34.320
知道这些是一个不错的起步

1:19:34.520,1:19:45.340
当pytorch的nn.Module在运行时, 比如它在计算某一层的输出值时

1:19:45.340,1:19:50.660
这个类专门会执行一个叫forward函数

1:19:50.720,1:19:54.940
我们在这里了解forward 函数是如何定义的

1:19:55.000,1:20:02.800
当这个模型在创建时，会执行这个__init__函数

1:20:03.020,1:20:10.080
之前我们有提及，这个__init__，python人士把它叫做dunder init

1:20:10.240,1:20:18.340
dunder init 是我们如何创建模型，dunder forward是我们如何运行模型

1:20:18.500,1:20:26.080
如果你仔细看会发现，（dunder forward）里面没有告诉你要如何计算梯度

1:20:26.180,1:20:30.120
这是因为pytorch能自动计算梯度

1:20:30.160,1:20:38.820
我们只需要告诉pytorch如何计算模型的输出值，pytorch会自动为我们计算梯度

1:20:38.960,1:20:52.740
这里的模型，含有一组用户的参数和一组电影的参数

1:20:52.780,1:20:56.780
一组用户的偏差（bias), 一组电影的偏差

1:20:56.840,1:21:02.700
而它们都来自get_embedding的输出结果

1:21:02.760,1:21:14.780
这里是get_embedding的源代码/定义

1:21:14.820,1:21:22.720
它所做的仅仅是要求执行nn.Embedding

1:21:22.720,1:21:29.640
因为在pytorch中有一系列标准的神经网络层已经为我们准备好了

1:21:29.660,1:21:33.080
这里的目的是创建一个embedding嵌入层

1:21:33.140,1:21:36.000
这里的工作是随机化处理

1:21:36.040,1:21:41.620
是针对embedding参数来做常规的随机数处理

1:21:41.620,1:21:42.840
那么什么是嵌入层embedding

1:21:42.900,1:21:52.040
嵌入层，毫无疑问，就是一个参数矩阵

1:21:59.180,1:22:03.760
尤其是，嵌入层，作为一个参数矩阵，长得像这样

1:22:03.780,1:22:12.620
嵌入层，是这样一个参数矩阵，你可以查看并提取任意一个数组

1:22:12.660,1:22:20.860
任意一种参数矩阵，之后课时里我们会更深入学习

1:22:20.860,1:22:27.420
嵌入层，作为参数矩阵，是由一系列数组构成，数组都有自己的序号

1:22:27.420,1:22:31.640
因此，你可以用序号从参数矩阵中提取数组

1:22:31.760,1:22:35.040
这就是嵌入层矩阵的实质

1:22:35.080,1:22:39.640
这里我们有一个嵌入层矩阵是针对用户的

1:22:39.780,1:22:42.820
另一个嵌入层矩阵是给电影的

1:22:42.860,1:22:48.640
这里是两个矩阵的矩阵乘法

1:22:48.700,1:22:52.020
但仔细想想，这是不够的

1:22:52.220,1:22:57.440
因为我们遗漏的情况是，比如，有些电影就是比其他电影，更被所有人喜欢

1:22:57.480,1:23:03.780
也许有些用户就是比其他用户更喜欢电影

1:23:03.780,1:23:07.900
我不仅仅要对这两个数组做点积

1:23:08.040,1:23:18.080
我还想添加一个值来代表这个电影面对所有人的受欢迎程度，一个数值代表这个用户对所有电影的喜欢程度

1:23:18.080,1:23:20.060
这些数值被称为bias 偏差

1:23:20.060,1:23:28.900
还记得，在我们手动写梯度下降代码的Notebook，关于如何处理偏差时，我们最参数矩阵增加了一列数值并将它设置为1

1:23:28.900,1:23:35.460
而在现实操作中，我们常用做法是

1:23:35.460,1:23:39.880
就明确说出来我们要增加一个偏差值

1:23:39.920,1:23:47.540
所以，我们要的不是预测值=（用户）数组和（电影）数组的点积

1:23:47.580,1:23:55.320
我们要的是这两个数组点积再加上这个电影的偏差和这个用户的偏差

1:23:55.380,1:23:59.900
刚才所说的就是源代码所描述的事情

1:24:00.000,1:24:13.000
当我们构建模型时，我们构建了用户的嵌入矩阵(embedding matrix)和电影的嵌入矩阵，以及用户的偏差数组(bias vector) 和电影的偏差数组

1:24:13.060,1:24:21.600
当我们计算模型时，我们就是让这两个（embedding matrix 嵌入矩阵）做矩阵乘法

1:24:21.640,1:24:27.400
就像我们刚才所做的一样，对吧。我们先做点积，叫做这个“dot"

1:24:27.440,1:24:32.660
然后再加上偏差

1:24:32.660,1:24:35.700
暂时先不管min_score

1:24:35.700,1:24:37.500
这就是我们的返回值

1:24:37.540,1:24:44.860
所以，我们模型（源代码）就是在做我们之前在这里（Excel）中所做的内容

1:24:44.940,1:24:49.120
并且增加了偏差的计算

1:24:49.180,1:24:58.320
这是一份非常非常简单的线性模型

1:24:58.360,1:25:04.080
对于这样的协同过滤问题

1:25:04.140,1:25:09.360
这样简单的线性模型能有很好的表现

1:25:09.560,1:25:17.420
我们还有一个技巧是我们最后要讲的

1:25:17.420,1:25:26.280
我们之前有说过的min_score = 0, max_score=5

1:25:26.320,1:25:28.800
这里要注意的是

1:25:30.520,1:25:37.040
要注意的是，如果我们

1:25:37.100,1:25:47.020
当我们做完了点积再加上偏差后，能得到一个任意数值（在这个坐标轴上）

1:25:47.020,1:25:50.100
从非常负到非常正的数

1:25:50.100,1:25:55.900
但我们知道，我们只想得到从0到5的数

1:25:55.900,1:26:01.360
这里是5， 这里是0

1:26:01.460,1:26:12.300
如果我们用这样的函数来将计算得到的任意值映射到0-5的区间

1:26:12.360,1:26:21.140
这个函数的形态叫做sigmoid

1:26:21.260,1:26:24.940
这里是向着5无限趋近

1:26:25.040,1:26:27.040
这里是向着0无限趋近

1:26:27.060,1:26:39.880
这样一来，不论点积加偏差给我们怎样的值（在X轴上），通过这个函数转化出来的值永远不会超过5或小于0

1:26:39.920,1:26:45.480
严格的说，这么做并不是必要的

1:26:45.520,1:26:53.200
因为我们的参数（矩阵）是可以学习的，会找到真实值的区间范围

1:26:53.200,1:26:57.440
为什么我们要做这件多余的事，如果它不是必须的话

1:26:57.440,1:27:01.980
原因在于，我们希望模型训练不要太费劲（让它训练能尽可能更轻松一些）

1:27:02.080,1:27:12.540
如果我们做一下设置，让预测值不可能过大或者过小

1:27:12.580,1:27:18.920
这样一来模型就能拿出更多资源（参数值）来预测我们真正关心的内容：哪个用户会喜欢哪个电影

1:27:19.000,1:27:26.420
我们会反复讲到这个概念，如何让神经网络更好的工作的关键是

1:27:26.540,1:27:32.320
所有我们做的这些小的决策（技巧设置）来让模型能更轻松的学习重要的事情

1:27:32.360,1:27:36.120
这就是我们在这里的最后一个小技巧

1:27:36.360,1:27:46.080
也就是我们将结果（点积加偏差的结果）输送给S 函数（sigmoid ）

1:27:46.180,1:27:55.560
S函数其实就是1/（1+e**x),  函数定义不重要，重要的是我们之前提到的它的形态

1:27:55.680,1:27:58.700
将任意值映射成0到1之间

1:27:58.780,1:28:05.860
如果在乘上（max_score - min_score) 然后加上 min_score, 那么所有值将在min_score 和  max_score之间

1:28:05.880,1:28:15.980
那么，这就意味着，这个非常小且简单的神经网络，尽管小它依旧是神经网络

1:28:16.020,1:28:20.020
只有一个参数矩阵（但实际上是2个参数矩阵），没有非线性激活函数

1:28:20.100,1:28:22.460
所以，这可以算是世界上最无聊的神经网络了

1:28:22.480,1:28:26.100
当然还是加了一个S函数的

1:28:26.120,1:28:30.160
对了，这个模型还是有非线性激活函数的，S函数就是

1:28:30.180,1:28:33.380
但只有一个层的参数结构

1:28:33.480,1:28:41.760
但实际上，这个模型可以给我们非常接近最先进的SOTA结果

1:28:41.760,1:28:48.740
我上网查找了这个movielens 100K 数据集的模型最优表现

1:28:48.960,1:28:57.540
我发现我们的这个模型的表现超过了所有我能下载到底标准商业化针对这类任务的软件的表现

1:28:57.620,1:29:02.400
其中决定性的技巧就是，我们加上了这个S函数

1:29:04.200,1:29:06.200
有问题吗？

1:29:09.660,1:29:12.300
（提问）有一个问题是，你如何设置你的vim

1:29:12.340,1:29:16.180
我已经有了你的.vimrc的链接，我想知道你是怎么看（vim）的

1:29:16.180,1:29:20.180
他们特别喜欢你的vim设置

1:29:20.220,1:29:23.580
你们喜欢我的vim设置，但是我的vim几乎没有任何设置啊

1:29:23.720,1:29:26.760
真的没什么技巧设置

1:29:26.980,1:29:34.540
不论你对你的编辑器做怎样的设置，你需要它长得这个样子

1:29:34.600,1:29:42.760
如果你有一个类，但没有专注，你需要它处于折叠收起状态，这样就不用看到它

1:29:42.840,1:29:48.620
所以你需要一个能简易折叠代码的功能

1:29:48.620,1:29:50.620
vim能自动帮助你做到

1:29:50.700,1:29:55.880
还有就是如我所说，你需要能够跳入到某个类或函数的定义中的功能

1:29:55.920,1:29:59.600
在vim里，我们用tags来实现

1:29:59.620,1:30:04.940
这里的操作能跳入到learner里，基本上vim都提供这些功能

1:30:05.060,1:30:11.860
你只需要读指南就行了，我的.vimrc是最简单的，几乎没有使用任何拓展工具

1:30:11.880,1:30:15.460
另一个非常棒的编辑器就是VSCode

1:30:15.540,1:30:19.520
免费且超棒

1:30:19.700,1:30:25.740
所有我们刚刚看到vim能做到的VSCode都能做到

1:30:25.980,1:30:35.500
我喜欢用vim,因为可以在远程设备上也能使用

1:30:35.500,1:30:44.180
当然你也可以用git clone将fastai repo下载到本地，再用VSCode打开来实验代码

1:30:44.260,1:30:48.960
但不要用GitHub repo来查询源代码

1:30:48.960,1:30:53.480
这么做会让你发疯的，你需要能自由灵活的打开，关闭，跳跃代码

1:30:53.660,1:31:03.080
也许大家能在论坛里贴出自己的vim 或vscode， sublimed等等的技巧操作

1:31:03.160,1:31:11.340
如果有人要选择一个本地的编辑器，我会选择vscode, 我认为这是最好的选择

1:31:11.400,1:31:17.640
如果你要在terminal中使用编辑器，我会推荐vim或者Emacs

1:31:17.660,1:31:21.840
对我而言，它们显然更好

1:31:21.920,1:31:36.740
在结束今天的课程之前，我想用这个协同过滤的例子，来解释我们如何在接下来的三个课时中

1:31:36.760,1:31:39.700
来构建更复杂的神经网络

1:31:39.720,1:31:46.740
基本上，这些就是我们需要学习的核心概念

1:31:46.800,1:32:07.520
让我们来想想，当使用CNN时

1:32:07.520,1:32:11.160
或者任何神经网络来自图片识别

1:32:11.320,1:32:18.060
我们有很多像素，让我们拿一个像素看看

1:32:18.200,1:32:25.660
我们有一个红，绿，蓝像素

1:32:25.660,1:32:31.040
每个像素的值都是在0到255之间

1:32:31.220,1:32:38.940
我们对这些像素做归一化处理，所以它们变成浮点数，分布式均值为0标差为1的分布

1:32:39.040,1:32:45.720
让我们先按照255的规格来举例子，红色像素值是10

1:32:45.720,1:32:48.200
（绿色像素值） 20， （蓝色像素值）30

1:32:52.900,1:32:55.500
那么我们接下来能做什么呢？

1:32:55.560,1:33:00.160
基本上，我们会将它们当作一个数组

1:33:00.180,1:33:05.120
然后和一个矩阵做矩阵乘法运算

1:33:05.180,1:33:11.060
这个矩阵，取决于你要的行数和列数，

1:33:11.120,1:33:17.320
我们就把它当作是3行，多少列的矩阵呢？

1:33:17.360,1:33:19.600
这是你的选择

1:33:19.600,1:33:29.640
就像协同过滤的任务，我选择一个尺寸为5作为嵌入数组的大小

1:33:29.640,1:33:35.180
所以，（列是5）也就我们所说的尺寸为5的嵌入参数数组

1:33:35.220,1:33:39.740
这就是我们自主选择的参数矩阵的大小（3，5）

1:33:39.860,1:33:44.820
3 x 5 的矩阵

1:33:44.820,1:33:50.960
一开始，这个参数矩阵的值都是随机的

1:33:50.960,1:33:55.300
还记得我们在创建嵌入矩阵的时候，有2行代码，

1:33:55.320,1:33:58.560
第一行是创建一个矩阵，第二行是填入随机数

1:33:58.680,1:34:04.940
这就是背后的工作原理，虽然被fastai和pytorch的代码覆盖而隐藏在下面

1:34:05.040,1:34:10.700
当我们构建参数矩阵时，就是创建一个随机的参数数矩阵

1:34:10.920,1:34:17.100
矩阵的行必须是3，来对应输入值的大小，列可以自由设定

1:34:17.220,1:34:23.380
然后让输入数组与参数矩阵做矩阵乘法，得到一个数组

1:34:23.440,1:34:26.240
数组的大小是5

1:34:29.080,1:34:35.220
大家经常问，我需要掌握多少线性代数才能来做深度学习

1:34:35.460,1:34:37.460
有这些有够了

1:34:37.460,1:34:41.140
如果这些你也不熟悉，没问题

1:34:41.300,1:34:51.640
你需要知道矩阵乘法，你不需要深入了解它们，只需要知道它们长什么样子以及如何计算

1:34:51.740,1:34:59.700
你需要能掌握某个大小的矩阵与另一个大小的矩阵做矩阵乘法，会得到怎样大小的矩阵

1:34:59.700,1:35:02.140
也就是矩阵维度的对应匹配

1:35:02.360,1:35:11.440
如果你有一个数组大小3， 在numpy和pytorch里我们用@来与另一个3x5大小的矩阵做矩阵乘法，得到的一个大小为5的数组

1:35:11.460,1:35:15.740
下一步是什么呢

1:35:15.840,1:35:26.320
结果要通过一个激活函数，比如ReLU 线性整流函数，其实就是max(0, x)

1:35:26.320,1:35:32.960
然后会返回一个新数组，当然，大小不变

1:35:33.000,1:35:37.200
因为没有激活函数会改变输出值的尺寸

1:35:37.200,1:35:41.140
只会改变内容（数值大小），所以数组大小不变，依旧是5

1:35:43.680,1:35:49.160
下一步呢？我们要与另一个矩阵做矩阵乘法

1:35:49.160,1:35:55.160
同样的，列可以是任意数值，但行必须要对应一致

1:35:55.200,1:35:58.280
所以矩阵大小是（5， 任意选择）

1:35:58.440,1:36:07.780
这个矩阵可以是5行，可以是10列

1:36:07.780,1:36:16.860
继续生成输出结果，会是一个大小为10的数组

1:36:16.920,1:36:21.580
然后，给到ReLU, 得到一个大小不变的数组

1:36:21.660,1:36:40.280
然后，再给到另一个，这里改成8列而不是10列，（方便后续讲解）

1:36:40.440,1:36:46.600
如果我们实在做数字识别，那么就有10个不同的数字

1:36:46.680,1:36:55.700
因此我们最后一个参数矩阵必须是10列

1:36:55.840,1:37:01.120
这就意味最后的输出值会是一个数组，大小是10

1:37:01.200,1:37:08.040
你还记得我们是如何做数字识别的吗？我们拿来真实目标值

1:37:13.180,1:37:15.180
真实目标值也是一个大小为10的数组

1:37:15.200,1:37:18.300
如果当前图片的数字对应的是3

1:37:21.560,1:37:24.240
这个就是我们有预测的

1:37:24.480,1:37:31.520
这就意味着其他都是0，只有（第四个值）序号3的值是1

1:37:31.720,1:37:50.640
整体而言，神经网络，从输入值开始，逐一通过参数矩阵，激活函数（ReLU)， 参数矩阵，ReLU，参数矩阵，直到最后的输出值

1:37:50.680,1:37:53.600
然后比较这两个（输出值与真实目标值）

1:37:55.700,1:38:00.220
来看它们之间的差异大小，用的是损失函数来计算差异

1:38:00.340,1:38:02.820
下周我们会学习所有要用的损失函数

1:38:02.820,1:38:05.080
目前我们只学了MSE均方误差

1:38:05.200,1:38:12.880
我们可以将这最终输出值看出概率值

1:38:12.880,1:38:16.660
我们对比输出值数组和真实值数组里面的每一对值来计算损失值

1:38:16.680,1:38:21.520
然后在计算每一个参数值相对于损失值的梯度

1:38:21.720,1:38:23.720
然后就可以做一次参数更新

1:38:23.760,1:38:28.540
接下来我要讲的是专业词汇，因为它们非常重要

1:38:30.840,1:38:34.960
这些东西里面装的都是数字

1:38:35.060,1:38:39.920
具体而言，一开始它们是随机数的参数矩阵，

1:38:40.080,1:38:47.940
这些黄色的矩阵，在pytorch里，我们将它们parameters参数

1:38:51.260,1:38:54.300
有时我们叫它们weights 权重

1:38:54.380,1:39:00.180
但是weights 权重不太准确，因为parameters还可以是偏差biases （但weights不能是biases)

1:39:00.240,1:39:06.020
我们基本上认为它们是通用的（parameters = weights），但严格的说，这些黄色矩阵应该叫parameters 参数

1:39:06.080,1:39:22.800
在这些参数矩阵做完矩阵乘法之后，计算出来的数组，这里有些数值是有这些参数矩阵相乘而来

1:39:25.460,1:39:31.880
接下来还有一个数值是通过ReLU（激活函数）计算而来

1:39:40.620,1:39:45.780
不论是蓝色的参数矩阵相乘结果，还是紫色的激活函数计算结果

1:39:47.620,1:39:51.320
都叫做activations 激活值/层

1:39:51.340,1:39:58.180
不论是参数还是激活值，它们都是数值

1:39:58.240,1:40:04.680
但是参数值是被存储下来，专门为了运算而存在

1:40:04.940,1:40:12.060
激活值，是（参数）计算的结果

1:40:12.060,1:40:14.940
所以，参数与激活值是最关键的需要记住的

1:40:15.320,1:40:24.580
使用这些专有名词，并且要正确准确使用

1:40:24.580,1:40:28.980
如果你看到这些术语，那么它们都有自己具体对应的意思

1:40:29.020,1:40:31.460
不要混淆它们

1:40:31.540,1:40:36.200
记住，它们没什么神秘的

1:40:36.300,1:40:42.680
激活值，无非是矩阵相乘结果，或者是激活函数计算结果

1:40:42.780,1:40:48.520
参数，就是我们用来做相乘的矩阵数值

1:40:48.620,1:40:54.460
就这么简单。然后，还有一些特殊的层结构

1:40:54.500,1:41:02.160
每一层都是在做计算

1:41:02.280,1:41:06.180
这些在运算的（如图所示，箭头指向）都是layers 层

1:41:06.300,1:41:16.540
它们就是神经网络的层，每一层都会产生激活值，因为层里面有计算，计算结果就是激活值

1:41:19.300,1:41:21.580
一开始有一个特殊层

1:41:21.580,1:41:23.520
叫做输入层

1:41:23.700,1:41:28.060
在最后我们有一组激活值

1:41:28.060,1:41:34.960
我们叫这些激活值为输出值（数学层面没什么特别，语言层面，作为最后的值，给不它们不一样的名称）

1:41:35.080,1:41:45.260
这里要注意的是，神经网络的输出值，没有什么数学层面的特殊性，它们也无非就是一个层的激活值

1:41:45.560,1:41:50.760
我们在协同过滤案例中所做的有趣的事情是

1:41:50.980,1:41:59.380
我们在结束时增加了一个激活函数

1:42:00.840,1:42:03.320
增加了一个额外的激活函数

1:42:03.800,1:42:06.020
也就是S函数

1:42:06.260,1:42:10.040
具体而言，是一个被放缩的S函数，输出值在0到5之间

1:42:10.080,1:42:12.080
这很常见

1:42:12.140,1:42:16.140
使用激活函数作为最后一层，是很常见的

1:42:16.200,1:42:19.100
几乎完全不会是ReLU这样的损失函数

1:42:19.100,1:42:24.300
因为极少情况下，你要的输出值需要砍去负值

1:42:24.300,1:42:27.360
更多情况下，你会想要损失函数是S函数或者是类似的函数

1:42:27.480,1:42:31.260
因为很可能你想要的输出值是在某个区域之间的值

1:42:31.320,1:42:33.640
或者在某个区间放缩

1:42:33.960,1:42:44.380
这几乎是全部内容了。我们已经覆盖了输入值，权重/参数，激活值，激活函数，有时也叫做非线性激活函数

1:42:44.500,1:42:52.380
输出值，然后是对比这两组数值的函数，叫做损失函数

1:42:52.520,1:42:55.080
目前我们用到的是MSE均方误差

1:42:57.540,1:43:02.460
这就是今天的内容

1:43:02.740,1:43:10.120
下周我们会加入一些额外信息

1:43:10.160,1:43:13.400
我们会学到用于分类问题的损失函数

1:43:13.400,1:43:15.180
叫做cross-entropy 交叉熵

1:43:15.300,1:43:21.160
我们会学到一个激活函数，用于单标注分类，叫做softmax

1:43:21.300,1:43:32.260
我们还会学到，在做微调fine-tuning时，背后的工作原理，也就是freeze解冻，unfreeze封冻模型，也就是在做迁移学习时，到底发生了什么

1:43:32.320,1:43:36.100
谢谢大家，下周见！
