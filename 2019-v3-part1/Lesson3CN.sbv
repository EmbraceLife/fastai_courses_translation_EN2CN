0:00:01.900,0:00:05.009
欢迎来到第三课，我们

0:00:05.009,0:00:06.670
首先要做一个简短的更正

0:00:06.680,0:00:09.440
如你们所知

0:00:09.450,0:00:11.150
我们上次课说这张图是来自

0:00:11.160,0:00:11.660
quora

0:00:11.670,0:00:13.400
上周我们说的没错，图的确取自

0:00:13.410,0:00:15.530
Quora但后来我们发现

0:00:15.540,0:00:17.000
这张图最初是源于Andrew Ng

0:00:17.010,0:00:19.070
的非常棒的机器学习课程

0:00:19.080,0:00:21.380
源自Coursera，我们对不准确的引用表示抱歉

0:00:21.390,0:00:24.349
不过因此我们反而可以聊一聊

0:00:24.359,0:00:25.670
Andrew的非常棒的coursera机器学习课程

0:00:25.680,0:00:28.939
Andrew的非常棒的coursera机器学习课程

0:00:28.949,0:00:31.130
我们看到很多人给予

0:00:31.140,0:00:34.100
这门课4/5星评级

0:00:34.110,0:00:36.830
从某些角度看这门课有点过时但是大量

0:00:36.840,0:00:40.030
课程内容更古不变的

0:00:40.040,0:00:42.830
并且是以

0:00:42.840,0:00:45.350
自下而上的风格进行教授的，因此

0:00:45.360,0:00:47.510
如果将andrew自下而上的课程与

0:00:47.520,0:00:49.369
我们的自上而下的课程相结合

0:00:49.379,0:00:51.889
尤其是当你想

0:00:51.899,0:00:54.319
学习更多机器学习基础知识时，你

0:00:54.319,0:00:54.329
学习更多机器学习基础知识时，你

0:00:54.329,0:00:55.580
也应该尝试我们fastai的机器学习课程

0:00:55.580,0:00:55.590
也应该尝试我们fastai的机器学习课程

0:00:55.590,0:00:57.529
如果来到我们的fastai页面

0:00:57.529,0:00:57.539
如果来到我们的fastai页面

0:00:57.539,0:00:59.330
并点击机器学习

0:00:59.330,0:00:59.340
并点击机器学习

0:00:59.340,0:01:00.889
然后就能前往

0:01:00.889,0:01:00.899
然后就能前往

0:01:00.899,0:01:03.770
我们的机器学习课程页面，其课程长度是本课程的2倍

0:01:03.770,0:01:03.780
我们的机器学习课程页面，其课程长度是本课程的2倍

0:01:03.780,0:01:06.289
与此同时，我们机器学习课程

0:01:06.289,0:01:06.299
与此同时，我们机器学习课程

0:01:06.299,0:01:07.969
将更渐进的带你了解

0:01:07.969,0:01:07.979
将更渐进的带你了解

0:01:07.979,0:01:09.880
一些基础概念如

0:01:09.880,0:01:09.890
一些基础概念如

0:01:09.890,0:01:12.320
验证集，模型解读

0:01:12.320,0:01:12.330
验证集，模型解读

0:01:12.330,0:01:14.990
，Pytorch tensor 如何工作以及

0:01:14.990,0:01:15.000
，Pytorch tensor 如何工作以及

0:01:15.000,0:01:18.499
类似的细节，因此我认为这些

0:01:18.499,0:01:18.509
类似的细节，因此我认为这些

0:01:18.509,0:01:20.210
课程如果组合在一起，将帮助你

0:01:20.210,0:01:20.220
课程如果组合在一起，将帮助你

0:01:20.220,0:01:23.810
深入挖掘机器学习内容。

0:01:23.810,0:01:23.820
深入挖掘机器学习内容。

0:01:23.820,0:01:25.429
我了解到很多人两门课都学的人

0:01:25.429,0:01:25.439
我了解到很多人两门课都学的人

0:01:25.439,0:01:27.020
都会提到他们从每一门课中都学到很多新知识

0:01:27.020,0:01:27.030
都会提到他们从每一门课中都学到很多新知识

0:01:27.030,0:01:29.510
或者你也可以

0:01:29.510,0:01:29.520
或者你也可以

0:01:29.520,0:01:30.800
在课程中来回切换

0:01:30.800,0:01:30.810
在课程中来回切换

0:01:30.810,0:01:37.609
看看哪一个更适合你。 上周我们讲到

0:01:37.609,0:01:37.619
看看哪一个更适合你。 上周我们讲到

0:01:37.619,0:01:41.260
将模型加载到网上云端

0:01:41.260,0:01:41.270
将模型加载到网上云端

0:01:41.270,0:01:43.429
下面要说的可以让这项工作非常简单

0:01:43.429,0:01:43.439
下面要说的可以让这项工作非常简单

0:01:43.439,0:01:45.710
在课程v3网页里

0:01:45.710,0:01:45.720
在课程v3网页里

0:01:45.720,0:01:49.490
有一个章节叫“产品化”production

0:01:49.490,0:01:49.500
有一个章节叫“产品化”production

0:01:49.500,0:01:53.030
虽然此刻我们只给出一个平台，但是

0:01:53.030,0:01:53.040
虽然此刻我们只给出一个平台，但是

0:01:53.040,0:01:54.469
当这个视频被放出来的时候，

0:01:54.469,0:01:54.479
当这个视频被放出来的时候，

0:01:54.479,0:01:56.330
更多平台会被添加上来；我们的平台指南会

0:01:56.330,0:01:56.340
更多平台会被添加上来；我们的平台指南会

0:01:56.340,0:01:59.600
指导我们非常轻松的将模型加载到网络上

0:01:59.600,0:01:59.610
指导我们非常轻松的将模型加载到网络上

0:01:59.610,0:02:03.859
我们举个例子看看有多么简单

0:02:03.859,0:02:03.869
我们举个例子看看有多么简单

0:02:03.869,0:02:06.709
这里是如何使用Zeit的指南，

0:02:06.709,0:02:06.719
这里是如何使用Zeit的指南，

0:02:06.719,0:02:09.350
由三藩市学习小组的Navjot Matharu撰写

0:02:09.350,0:02:09.360
由三藩市学习小组的Navjot Matharu撰写

0:02:09.360,0:02:11.809
如你所见，只有简短一页纸的说明

0:02:11.809,0:02:11.819
如你所见，只有简短一页纸的说明

0:02:11.819,0:02:13.800
简单到几乎没什么可做的

0:02:13.800,0:02:13.810
简单到几乎没什么可做的

0:02:13.810,0:02:17.720
同时还免费，

0:02:17.720,0:02:17.730
同时还免费，

0:02:17.730,0:02:22.290
当然它不会同时为你响应1万个请求

0:02:22.290,0:02:22.300
当然它不会同时为你响应1万个请求

0:02:22.300,0:02:24.360
但是它可以让你顺利起步上手

0:02:24.360,0:02:24.370
但是它可以让你顺利起步上手

0:02:24.370,0:02:27.690
并且我发现它工作顺畅且反应很快

0:02:27.690,0:02:27.700
并且我发现它工作顺畅且反应很快

0:02:27.700,0:02:29.340
因此加载模型到网络上

0:02:29.340,0:02:29.350
因此加载模型到网络上

0:02:29.350,0:02:31.979
已经不再是很慢或很复杂的事情了

0:02:31.979,0:02:31.989
已经不再是很慢或很复杂的事情了

0:02:31.989,0:02:32.790
已经不再是很慢或很复杂的事情了

0:02:32.790,0:02:32.800
已经不再是很慢或很复杂的事情了

0:02:32.800,0:02:34.140
更厉害的是，

0:02:34.140,0:02:34.150
更厉害的是，

0:02:34.150,0:02:36.420
你可以用它来做MVP（简易可行产品）

0:02:36.420,0:02:36.430
你可以用它来做MVP（简易可行产品）

0:02:36.430,0:02:37.650
当你的网络产品需要回应上千请求时

0:02:37.650,0:02:37.660
当你的网络产品需要回应上千请求时

0:02:37.660,0:02:40.020
你已经知道

0:02:40.020,0:02:40.030
你已经知道

0:02:40.030,0:02:41.490
哪些任务可顺利执行，因而你可以开始

0:02:41.490,0:02:41.500
哪些任务可顺利执行，因而你可以开始

0:02:41.500,0:02:43.890
升级你的服务器类型

0:02:43.890,0:02:43.900
升级你的服务器类型

0:02:43.900,0:02:45.979
或者采用更传统的

0:02:45.979,0:02:45.989
或者采用更传统的

0:02:45.989,0:02:49.860
更大型的工程化方式来解决问题。

0:02:49.860,0:02:49.870
更大型的工程化方式来解决问题。

0:02:49.870,0:02:53.280
如果你直接采用这个简易的代码包

0:02:53.280,0:02:53.290
如果你直接采用这个简易的代码包

0:02:53.290,0:02:55.410
你将生成我的泰迪熊识别器

0:02:55.410,0:02:55.420
你将生成我的泰迪熊识别器

0:02:55.420,0:02:58.199
因为里面的模型是我的泰迪熊识别模型

0:02:58.199,0:02:58.209
因为里面的模型是我的泰迪熊识别模型

0:02:58.209,0:02:59.970
目的是告诉大家

0:02:59.970,0:02:59.980
目的是告诉大家

0:02:59.980,0:03:01.710
整个工作流程非常简单

0:03:01.710,0:03:01.720
整个工作流程非常简单

0:03:01.720,0:03:05.340
你还可以提供自己页面风格

0:03:05.340,0:03:05.350
你还可以提供自己页面风格

0:03:05.350,0:03:07.410
以及你定制的响应逻辑

0:03:07.410,0:03:07.420
以及你定制的响应逻辑

0:03:07.420,0:03:09.150
和其他类似内容。所有的设计都是

0:03:09.150,0:03:09.160
和其他类似内容。所有的设计都是

0:03:09.160,0:03:11.160
极简模式。你会看到

0:03:11.160,0:03:11.170
极简模式。你会看到

0:03:11.170,0:03:13.710
后端的所有内容

0:03:13.710,0:03:13.720
后端的所有内容

0:03:13.720,0:03:17.539
就是一个简单的RESt构架风格

0:03:17.539,0:03:17.549
就是一个简单的RESt构架风格

0:03:17.549,0:03:20.729
会返回给我们一个JSON

0:03:20.729,0:03:20.739
会返回给我们一个JSON

0:03:20.739,0:03:22.380
而前段就是一个超级简单Javascript代码

0:03:22.380,0:03:22.390
而前段就是一个超级简单Javascript代码

0:03:22.390,0:03:27.150
因此，通过这个简易的应用

0:03:27.150,0:03:27.160
因此，通过这个简易的应用

0:03:27.160,0:03:29.069
我们可以感受到如何构建一个

0:03:29.069,0:03:29.079
我们可以感受到如何构建一个

0:03:29.080,0:03:33.030
可以会Pytorch模型对话的网页APP

0:03:33.030,0:03:33.040
可以会Pytorch模型对话的网页APP

0:03:33.520,0:03:40.080
我们来看看本周大家做的网页APP

0:03:40.620,0:03:44.560
Edward Ross制作了“这是什么牌的车”的网页APP

0:03:44.900,0:03:48.260
更准确的说，是“这是什么牌的车”（都是澳大利亚的车照片）

0:03:48.980,0:03:53.700
很有意思的是，Edward在谈及创建过程时

0:03:54.200,0:04:01.140
这个创建过程对于理解模型非常有帮助。

0:04:03.940,0:04:09.140
另外一个有意思的地方是，他谈及在手机上运行这个模型。

0:04:09.780,0:04:18.300
很多人的第一反应是在手机上用模型，需要调用手机版的Tensorflow 或者是ONX之类的复杂APP。

0:04:18.600,0:04:23.720
但事实上你可以让模型在云端跑，将结果输出到网页即可。

0:04:24.020,0:04:28.260
或者非常简单的网页GUI前端对接REST后段之类来实现。

0:04:28.580,0:04:31.960
多数情况下，你不需要直接在手机端运行模型。

0:04:32.000,0:04:34.000
这是一个很好的例子。

0:04:34.560,0:04:40.100
cwarner 创建了“吉他识别器”。

0:04:41.860,0:04:43.860
这个APP可以帮助你判断食物是否足够健康

0:04:46.600,0:04:48.600
很明显，这道菜被判定是“健康”的，这显然不对，我认为汉堡更健康（玩笑）

0:04:49.240,0:04:54.660
特立尼达和多巴哥，这个地方（国家）是蜂鸟的故乡。

0:04:55.020,0:04:58.000
所以，如果你去玩的话，可以用这个模型识别不同类型的蜂鸟。

0:04:58.980,0:05:00.980
还有的模型帮助你判断图片中的蘑菇是否可食用。

0:05:01.720,0:05:06.340
如果你恰好是Charlie Harrington的表亲，

0:05:08.520,0:05:10.520
你可以判断在众多的表亲当中，谁是谁。我记得这个模型是为他的未婚妻设计制作的。

0:05:19.320,0:05:21.320
这个模型甚至可以告诉你所有表亲的兴趣爱好。虽然是一个小众模型，但至少有36个人感觉很受用。

0:05:24.060,0:05:26.060
我没有表亲，36个，真不少！

0:05:26.580,0:05:31.680
这是一个APP，“吃”进一段视频

0:05:32.360,0:05:34.360
输出一个含有表情识别的视频。

0:05:35.080,0:05:38.840
真的很酷！

0:05:39.020,0:05:42.160
我喜欢！

0:05:43.620,0:05:46.420
Team26 好样的！

0:05:47.220,0:05:52.220
这是一个类似的APP，为美国手语协会制作的。

0:05:53.480,0:06:03.480
这并不难（没有什么巨大差异），无非是从“吃”进图片，变成了“吃”进视频，视频只需要选择某些图片frame即可。

0:06:04.160,0:06:11.500
模型吃进视频的某些图片，输出结果，再将结果反馈到界面上即可。

0:06:12.280,0:06:18.960
这个真的很酷，如今你可以自由选择在本地电脑或是云端实现。

0:06:19.880,0:06:27.680
Henri Palacci 创建了“从外空间识别你的城市”APP

0:06:28.060,0:06:32.500
然而由于模型准确度很高，而导致他觉得不舒服很“诡异”。

0:06:32.960,0:06:37.960
这是我所在的城市图片，模型正确识别了所在国家。

0:06:38.320,0:06:41.740
他介绍了在训练模型时，他格外研究了验证集。

0:06:42.380,0:06:46.960
以确保卫星图分区没有重叠或者靠的太近。

0:06:47.380,0:06:50.620
所以他意识到需要下载更多数据。

0:06:50.900,0:06:56.580
当他用新获取的更多数据训练模型后，模型表现很好，能识别任意卫星图所对应的国家。

0:06:57.000,0:06:59.880
接下来这个模型也非常有趣。

0:07:00.360,0:07:05.660
它是将一个单变量时间序列转化成一张图片。

0:07:06.240,0:07:11.700
这个转化是通过Gramina Angular Field实现的（我从未听说过的技术）

0:07:12.100,0:07:17.840
作者说已经在这项任务（单一时间序列转化图片）上做到接近state of art专业水平。

0:07:19.040,0:07:24.620
我喜欢这类创新，将非图片数据转化为图片

0:07:25.320,0:07:28.920
这个项目有趣之处在于

0:07:30.280,0:07:32.280
这是一个可以识别面部表情的模型。

0:07:32.560,0:07:38.860
他着重实验的是在fastai的默认设置下，模型能训练到怎样的水平

0:07:38.940,0:07:40.940
我认为这是非常有趣的实验，因为

0:07:41.060,0:07:43.060
我们总是被告知这类的任务很难训练

0:07:46.460,0:07:48.460
需要大量具体行业知识

0:07:48.460,0:07:50.020
但我们发现通常事实并非如此。

0:07:50.500,0:07:54.000
作者面对这个表情识别数据集

0:07:54.580,0:08:04.380
的2013年学术论文的state of art水平成绩做对比，发现自己模型表现接近甚至略优。

0:08:08.280,0:08:10.280
所有的成绩都是在fastai默认值，没有加入个性化调参下，实现的。这个真的很酷！

0:08:11.020,0:08:16.880
Alena Harley，我们上周有展示过她的作品。

0:08:17.300,0:08:19.600
在genomics基因工程领域里做了另一件非常有趣的项目。

0:08:21.260,0:08:29.060
着重研究variant analysis 和false positives

0:08:29.420,0:08:34.100
这两个方法的应用对象是这些（鼠标所指）图片

0:08:34.300,0:08:40.940
她发现自己模型能在false postive上比行业最优水平更高

0:08:41.140,0:08:46.160
事实上比行业标准水平的准确度高500%

0:08:47.460,0:08:49.460
这是一个非常有代表性的案例：

0:08:49.960,0:08:57.140
如果你每天需要花费大量时间处理某项任务（这个案例里是降低false postive)

0:08:57.680,0:09:01.500
也许你可以使用深度学习帮助你大幅提高工作效率

0:09:02.000,0:09:09.380
同时，这又是一个将非图片数据转化为图片数据来做分类的案例。

0:09:09.920,0:09:14.080
这些项目都非常的酷！

0:09:16.460,0:09:22.200
能看到大家做了这么多有趣的网页APP或者是模型，真的很开心。

0:09:22.900,0:09:27.360
今天，我们要学习更多你们可以上手构建的新模型。

0:09:29.420,0:09:35.120
我们将快速把所有新模型过一遍，然后回头审视它们之间的共性特点。

0:09:35.520,0:09:40.380
你还可以将这些新模型也转变成网页APP

0:09:41.360,0:09:46.360
但你需要稍微修改网页APP模版来是个这些不同应用的新模型。

0:09:46.600,0:09:50.580
将它们都做成网页APP是非常好的学习消化的方法。

0:09:51.640,0:09:57.260
第一个数据集，是卫星图像。

0:09:57.740,0:10:04.160
卫星图像，是深度学习的一个可以深耕的领域。

0:10:04.740,0:10:08.980
当然，已经有很多人尝试将深度学习应用到该领域，

0:10:08.980,0:10:10.480
但都是浅尝辄止。

0:10:10.840,0:10:14.640
我们的数据基本长得是这个样子。

0:10:16.140,0:10:22.300
我们可以看到卫星图片区，每个片区都有标注

0:10:22.660,0:10:27.880
（每个片区的标注不止一个）其中一个标注肯定是关于天气

0:10:28.420,0:10:33.000
比如这张图是多云或者局部多云

0:10:33.200,0:10:38.060
其他标注会告诉你标注者看到的有趣的特征景象

0:10:38.160,0:10:40.160
primary 这的是原始雨林

0:10:40.300,0:10:42.740
agriculture 指的是农业耕地

0:10:42.940,0:10:44.940
road就是道路，等等

0:10:45.340,0:10:50.980
所以，大家肯定已经看出，这个任务与我们之前的分类任务，是有区别的

0:10:51.300,0:10:55.700
因为这里不是只有一个标注，而是又多个标注（对应一张图片）

0:10:56.060,0:10:59.140
多标注分类问题的做法与之前方法很相似

0:10:59.460,0:11:02.480
但是我们先要做的是下载数据

0:11:02.720,0:11:05.300
这些数据源于Kaggle

0:11:05.480,0:11:09.560
Kaggle 主要因其竞赛而出名

0:11:09.940,0:11:15.640
当你在学习（深度学习机器学习时），Kaggle是非常好的选择，因为你可以知道你的模型表现。

0:11:15.880,0:11:19.080
你可以直观了解自己是否真的理解所学所用。

0:11:19.360,0:11:23.400
我通常将模型目标定在前10%的选手水平。

0:11:23.680,0:11:29.200
我的经验是前10%的选手真的知道自己在做什么

0:11:29.460,0:11:33.040
所以如果你能进入前10%，这是非常好的信号！

0:11:33.580,0:11:39.220
基本上所有Kaggle数据集是无法在非Kaggle平台上下载。

0:11:39.620,0:11:43.040
至少对于竞赛数据是如此，所以你必须通过Kaggle来下载

0:11:43.240,0:11:48.140
好消息是Kaggle提供了基于python的下载工具

0:11:48.440,0:11:54.040
我们提供了快捷的下载指南

0:11:56.840,0:12:02.900
要下载数据，你先要下载安装Kaggle下载工具

0:12:03.180,0:12:06.160
你只需要输入这行代码即可

0:12:06.160,0:12:12.140
如果只有一行代码，我们通常给你一个comment out 版本，然后你自己取消comment，直接运行即可

0:12:12.140,0:12:14.040
这里有一个很酷的技巧

0:12:14.420,0:12:22.980
选择多行代码，按下cmd + / 键，实现comment out， 重复上述操作，就是取消comment out

0:12:23.340,0:12:28.440
如果你跑这行代码，将为你下载Kaggle工具

0:12:29.540,0:12:36.940
取决于你的系统，你可能需要加上sudo 或加上 /

0:12:37.140,0:12:40.000
/pip 或则 source activate

0:12:41.240,0:12:46.740
所以，去看看Kaggle setup 指南，还可以看课程官网的[重返工作Return to work]的页面指南

0:12:48.500,0:12:53.960
其中conda install 与 pip install 下所需的操作是一样的

0:12:57.160,0:13:02.720
下载工具完成后，你可以开始下载数据

0:13:03.520,0:13:08.980
代码很简单，基本上就是说 kaggle 竞赛 下载 竞赛名称

0:13:09.560,0:13:11.560
然后在输入你要下载的数据

0:13:11.780,0:13:14.140
剩下的其他工作很少，

0:13:14.520,0:13:18.440
无非就是认证身份

0:13:19.340,0:13:27.860
然后在这几段内容中，你会看到去哪里获取包含你身份认证的文件

0:13:28.240,0:13:30.240
我就不再重复了。

0:13:30.240,0:13:31.840
跟着这里的指南走就行。

0:13:35.960,0:13:41.380
有时候Kaggle的数据集不是以zip或tar形式出现，而是用7zip形式

0:13:41.840,0:13:45.400
文件包含7z后缀

0:13:45.600,0:13:50.680
如果是这样的情况，你可以运行这行代码，pip install下载安装p7zip

0:13:51.040,0:13:55.540
或者因为有个好人为我们提供了conda install的方案

0:13:55.780,0:13:57.780
能在所有平台上下载安装

0:13:58.120,0:14:02.860
所以，你直接跑这行代码，不需要任何所谓sudo 之类

0:14:05.120,0:14:10.660
这个例子说明conda 非常方便，你可以下载安装binary library等等

0:14:12.500,0:14:17.980
所以，没有p7zip时，就用这个conda方法

0:14:18.220,0:14:22.600
这行代码为你unzip解压7zip数据集

0:14:23.020,0:14:27.820
这里的数据集用的是tar.7z形式，这行代码可以直接为你解压数据集

0:14:29.000,0:14:34.580
这里的7za 是你要用来跑的7zip archive的解压程序

0:14:36.180,0:14:41.580
这些都是基础技能，如果你不熟悉command line, 你需要稍微多花点时间实验一下这些代码

0:14:41.900,0:14:47.020
去论坛上提问寻求帮助，当然一定要先搜索，看看是否已被回答解决过了。

0:14:47.840,0:14:52.000
我们现在就开始吧。

0:14:52.380,0:14:55.740
一旦你的数据下载并解压，你看看打开来看看了

0:14:58.360,0:15:03.960
因为我们有多标注数据

0:15:04.360,0:15:07.580
针对每一张卫星图片，

0:15:07.840,0:15:14.420
很明显，我们不能用多个文件夹对应同一张拥有多个标注的图片。我们需要另辟蹊径。

0:15:14.820,0:15:18.560
Kaggle的方法是提供了一个csv文件

0:15:18.880,0:15:23.760
一列是文件名，一列是标注（多标注）

0:15:24.040,0:15:30.300
我们可以用pandas library来读取CSV文件

0:15:30.920,0:15:37.540
如果你没用过pandas，那么你只需要知道pandas是应对表格数据的“常规工具”

0:15:37.760,0:15:43.240
当然是指在python语言下的工具，pd是常用的pandas简称

0:15:43.500,0:15:48.340
我们没有对数据做任何处理，这两行代码只是展示数据内容

0:15:48.620,0:15:52.520
我们可以通过此查看前几行的内容

0:15:53.160,0:15:58.920
我们要将此转化为可以建模的数据

0:15:59.300,0:16:10.720
也就是说我们要将CSV转化为DataBunch

0:16:10.880,0:16:14.500
一旦DataBunch做好后，我们可以用show_batch来看数据和标注

0:16:14.760,0:16:19.040
然后我们可以构建CNN模型并开始训练

0:16:19.480,0:16:29.220
通常，深度学习里最复杂繁琐的步骤，就是将数据集整理成能“喂给”模型的形式。

0:16:29.260,0:16:35.700
截止到现在，我们已经教给大家如何用集成好的函数，直接生成DataBunch

0:16:36.220,0:16:42.300
所谓factory集成函数，就是告诉函数我们要用这样的数据源，这样的设置，来生成DataBunch.

0:16:42.600,0:16:48.560
这种方法，我们已经学过几种形式，用得都还不错。

0:16:48.720,0:16:53.180
但有时我们还需要更多的灵活性。

0:16:53.500,0:17:00.980
但是有很多选择我们需要自行决定，比如：文件夹在哪，数据是什么格式，标注以怎样形式存在

0:17:01.220,0:17:04.860
你如何分配验证集，以及你如何对数据做变形，等等

0:17:05.060,0:17:10.800
于是，我们创造了一个非常独特的API，叫data_block，我们以此为傲。

0:17:11.180,0:17:21.220
data_block API帮助我们将上述所有选择决策，一一拆分使其独立存在，分别设置各自参数

0:17:21.440,0:17:24.440
来构建你需要的特制数据

0:17:25.040,0:17:29.700
以planet 数据集为例

0:17:30.200,0:17:43.140
我们需要分别做以下决定：图片文件夹所在，标注在哪个CSV文件里，CSV列与列分割符号是什么

0:17:43.420,0:17:47.680
CSV 中对应的文件夹名称在这里填写，后缀名在这里填写

0:17:47.980,0:17:51.820
然后随机拆分出20%数据作为验证集

0:17:52.220,0:18:00.920
在此基础上，我们创建出dataset, 然后再加入变形transform, 最后就生成我们的DataBunch

0:18:01.100,0:18:04.080
最后我们对数据做normalization 正则化，采用Imagenet提供的stats（均值和方差）

0:18:04.320,0:18:07.260
这就是我们所需要一一设置的步骤

0:18:07.500,0:18:10.600
为了进一步讲解data_block的工作原理，

0:18:10.860,0:18:19.500
首先，我们要回头去讲解一下data_block里调用的所有pytorch中的classes

0:18:19.720,0:18:27.940
以后这些Pytorch中的classes会经常在fastai和pytorch文档中出现

0:18:28.520,0:18:35.020
第一个需要了解的是Dataset

0:18:35.360,0:18:38.540
它是pytorch的一部分

0:18:38.700,0:18:41.920
这是它的源代码

0:18:42.160,0:18:46.500
如你所见，这个代码其实什么也没做

0:18:47.040,0:18:56.680
这个Dataset Class 在Pytorch里事实上设定了两件事

0:18:56.820,0:18:58.940
分别是__getitem__ 和 __len__

0:18:59.640,0:19:09.340
在Python的世界里，人们将__{...}__， 称之dunder。因此我们叫它们 dunder getitem 和 dunder len

0:19:09.620,0:19:13.340
可以说它们是“魔法”函数

0:19:13.680,0:19:15.680
来实现一些特殊的职能

0:19:16.260,0:19:27.800
__getitem__的职能是，如果你有一个数据赋值给了变量o, 你可以用o[3]来提取o内部的数值

0:19:28.080,0:19:31.400
具体意思是，我们从o中提取第四个值

0:19:31.400,0:19:32.360
3是index，数据的序号

0:19:32.660,0:19:40.680
这个__len__能让我们使用len(o)函数

0:19:40.980,0:19:43.320
你们还会发现，此时这两个函数，没有实质内容。

0:19:43.740,0:19:51.840
也就是说，要Pytorch构建数据，首先需要构建Dataset，

0:19:52.000,0:19:56.880
但Pytorch不会告诉你Dataset中应该有什么

0:19:57.200,0:20:11.480
换言之，Pytorch Dataset这两个函数，能让你知道你的数据集中某个序号对应的内容是什么（用getitem)，以及数据集有多大(用len)

0:20:11.760,0:20:20.740
fastai 有很多Dataset 的子class来为不同类型的数据集做相似的这两件工作

0:20:21.000,0:20:25.160
目前，我们有针对图片数据集的Dataset子classes

0:20:25.720,0:20:33.820
在这里getitem将返回给我们一张图片和一个标注

0:20:34.060,0:20:37.860
这就是我们说所的Dataset

0:20:37.900,0:20:40.520
但是只是Dataset还不够模型来训练使用

0:20:40.940,0:20:53.020
首先，回顾SGD课程里，训练模型时，我们需要一次准备好一组数据

0:20:53.020,0:20:54.960
从而让GPU能并行计算

0:20:55.120,0:20:57.120
这就是我们的mini-batch

0:20:57.340,0:21:02.400
mini-batch就是我们将一组数据，一次性给到模型，从而使其做并行计算

0:21:02.820,0:21:12.300
为了制作mini-batch, 我们需要Pytorch的DataLoader class类

0:21:13.620,0:21:19.120
DataLoader需要吃进dataset

0:21:19.420,0:21:26.160
从而能够指定提取哪个位置上的数据/值，进而可以做随机提取不同位置的数据

0:21:26.380,0:21:30.620
进而创建一个任意大小的batch

0:21:30.920,0:21:35.640
然后将这个mini-batch送给GPU，让模型来使用

0:21:35.920,0:21:43.580
简言之，DataLoader的工作是将单个数据整合成小批量mini-batch, 在输送给模型来做训练

0:21:43.880,0:21:47.140
这就是DataLoader，基于Dataset而来。

0:21:47.480,0:21:56.000
因此，你已经不得不做一系列抉择：怎样的dataset, 要用数据做什么，数据来源形式是什么

0:21:56.320,0:21:58.740
当你构建DataLoader时，要选择批量的大小

0:21:58.880,0:22:01.700
但这仍旧不够训练模型

0:22:01.980,0:22:05.920
因为我们还没办法验证模型好坏。

0:22:06.160,0:22:15.220
之前我们构建的是训练数据，是无法帮助我们认识到模型真正的水平，我们需要一个独立于训练集之外的验证集来实现这一点

0:22:15.540,0:22:20.900
要做到这一点，我们需要一个fastai class类，叫做DataBunch

0:22:21.100,0:22:28.040
DataBunch的职能是将train dataloader 和 valid dataloader捆绑在一起

0:22:28.300,0:22:39.200
当你查看fastai 文档，看见这些monospace 字体时，说明它们是可以进一步查看的

0:22:39.600,0:22:54.440
以tran_dl为例，你可以在arguments中找到它，记住一定要继续往后看看它的类，我们看到它属于DataLoader 类

0:22:54.800,0:23:08.280
因此，当你构建DataBunch时，你需要有一个训练集的DataLoader, 一个验证集DataLoader, 然后生成的DataBunch可以给到模型训练了

0:23:08.560,0:23:12.660
这些都是我们所用的基础要素

0:23:12.940,0:23:18.160
让我们回到Notebook里

0:23:18.360,0:23:25.360
这几行的工作就是生成DataBunch

0:23:25.680,0:23:37.980
这里是关于图片文件夹选择，因为Dataset 能提取出图片和标注，所以我们需要首先给出图片地址，然后是标注的涞源

0:23:38.220,0:23:41.940
然后就是将数据分割成训练和验证两部分

0:23:42.120,0:23:45.860
这里是将它们变成Pytorch dataset

0:23:45.980,0:23:48.240
这里是对它们做变形

0:23:48.440,0:23:55.960
这里是将它们一次性变成DataLoader然后是DataBunch

0:23:56.220,0:24:00.700
现在我们来看看这些data_block API 的例子

0:24:01.080,0:24:09.720
一旦你理解了data_block API，关于如何将你的数据转化成模型需要的数据形式，将不再是问题

0:24:09.940,0:24:13.180
这里是使用data_block API的例子

0:24:13.440,0:24:21.860
比如MNIST数据集，也就是手写数字分类的图片集

0:24:22.380,0:24:34.140
我们可以这么来构建数据集：首先，告知数据是来源于一系列图片文件夹

0:24:34.620,0:24:42.400
然后，图片的标注是有文件夹名称提供

0:24:42.500,0:24:46.600
再然后，我们将数据分割成训练和验证集

0:24:46.700,0:24:50.100
基于它们所在文件夹（训练和验证文件夹）

0:24:50.380,0:24:53.980
我们可以增加测试集（可选项）

0:24:54.280,0:24:56.900
后续课程中会更多谈到测试集

0:24:57.000,0:25:01.400
然后我们就能将数据转化成pytorch dataset

0:25:01.580,0:25:07.320
在接下来，我们可以用这一组变形设置来对数据集变形

0:25:07.520,0:25:11.480
并且变形后图片尺寸改为224

0:25:11.680,0:25:14.220
然后再转化为DataBunch。

0:25:14.580,0:25:29.060
因此，每一步都有自己的参数设置，来将数据转化成你要的样子。但对于像MNIST这样的数据集，每一步设置的默认参数值基本就能满足我们的需要。

0:25:29.560,0:25:35.040
我们来验证一下：用这行代码提取一个数据样本

0:25:35.260,0:25:46.940
这里用到的是dataset, 不是dataloader. 因此我们可以用序号0来提取第一个样本的图片和标注

0:25:47.140,0:25:51.720
我们能用show_batch来看一组图片和标注，然后就可以训练了。

0:25:51.980,0:25:56.140
这里显示给我们数据集的类别

0:25:56.360,0:25:59.920
这个版本的数据集，被缩减为3和7两个类别版本。

0:26:00.240,0:26:10.840
这里是用到了缩小版的planet数据集，方便我们做实验。

0:26:11.040,0:26:15.240
同理，我们的图片数据集来源于某个文件夹

0:26:15.480,0:26:18.560
数据标注源于某个CSV文件

0:26:18.760,0:26:21.540
我们采用默认20%的随机训练验证集拆分

0:26:21.700,0:26:29.320
再生成dataset, 然后是进行预设的变形，然后变形后的图片尺寸变小了

0:26:29.400,0:26:31.400
然后就生成DataBunch.

0:26:33.520,0:26:38.560
DataBunch内设了画图功能。

0:26:38.700,0:26:42.800
这里还有今天课程后面会讲的数据集

0:26:43.040,0:26:48.480
我们来看看Camvid数据集，数据长得这个样子

0:26:48.640,0:26:53.460
数据样本包含图片而且每个像素都有颜色标注。

0:26:53.780,0:26:58.520
我们来看看它的Databunch生成：同样是数据源于文件夹

0:26:58.860,0:27:09.960
然后，用一个函数来提取每个图片像素的标注。这个函数告诉我们每个像素的颜色标注是什么

0:27:10.120,0:27:14.880
再是随机分割数据集

0:27:15.260,0:27:30.980
再生成datasets, 设置参数可以用classes, 帮助我们区分不同像素的颜色类别；上面这个行代码帮助我们解决这个类别问题。

0:27:31.200,0:27:36.800
然后是做变形，再生成DataBunch, 同时将批量设置为64个样本

0:27:37.160,0:27:43.140
然后就让DataBunch打印一组图片标注样本看看

0:27:43.400,0:27:51.620
再来看个例子，如果我们要生成这样的Databunch, 标注方框框出花瓶，椅子，遥控器，书等等

0:27:51.760,0:27:53.760
这是所谓的物体识别数据集。

0:27:54.020,0:28:00.880
我们这里有一个缩小版的COCO数据集，这是一个学术界知名的物体识别领域的数据集。

0:28:01.060,0:28:04.860
我们可以用相同的流程制作DataBunch: 数据源于coco文件夹，

0:28:05.100,0:28:09.180
用这个函数生成标注

0:28:09.240,0:28:12.480
然后分割数据，生成物体识别类的ObjectDetectDataset,

0:28:12.660,0:28:20.740
在生成DataBunch, 随后你会了解到为了保证足够的内存，我们需要降低批量大小（设置为16）

0:28:20.960,0:28:24.980
同时，我们还需要用到所谓的collation 函数。

0:28:25.180,0:28:29.340
这一步完成后，我们就能展示一下批量数据的样子了。

0:28:29.600,0:28:34.920
我们在这里用到超方便的Notebook在哪里找呢？

0:28:35.080,0:28:38.820
这个Notebook就是data_block的文档

0:28:39.000,0:28:42.400
还记得我说过，所有的文档都源于Notebook吗？

0:28:42.620,0:28:46.980
你可以在你的fastai repo/docs_src/中找到

0:28:47.180,0:28:53.020
这样你就可以很方便的尝试各种输入值，参数，验证输出值的实验了

0:28:53.200,0:28:56.680
这里显示的是文档的源代码也就是ipynb

0:28:56.880,0:29:01.100
这里是文档网页所在

0:29:01.300,0:29:07.220
记住，所有你需要在fastai中用到的东西，都可以在文档中查看详情

0:29:07.640,0:29:22.880
如果你要搜索data_block 文档详情，可以像这样搜索

0:29:23.120,0:29:33.920
如果你找到某个感兴趣的文档想进一步是实验代码，你可以前往它对应的docs_src/中的ipynb源码，进行试验

0:29:34.240,0:29:42.420
我们讲完了一个快捷版本的data_block API 介绍

0:29:42.720,0:29:50.220
但还有很多函数和使用方法，大家可以自己尝试

0:29:50.420,0:29:58.020
这就是我们对Planet数据集使用的data_block API

0:29:58.420,0:30:05.180
在文档中你会发现这两个步骤被合并成了一步

0:30:05.580,0:30:14.560
我们当然可以在这里也合并，但稍后你会学到为什么要将它们分步进行，也一样很好

0:30:14.760,0:30:19.020
有几点有趣的细节需要关注：

0:30:19.360,0:30:29.260
变形，变形的默认值，可以用shift + tab来查看

0:30:29.420,0:30:34.420
变形的默认值是flip随机反转

0:30:34.820,0:30:41.240
但是只限定于水平翻转

0:30:41.440,0:30:48.160
这个可以理解，如果是识别猫狗，你通常是做水平翻转，而不是垂直翻转

0:30:48.380,0:30:57.940
但卫星图像，不论图片中是河流还是道路都可以水平和垂直翻转，没有所谓正确翻转方向

0:30:58.500,0:31:10.220
flip_vert默认值是否，这里修改成True.事实上不仅是垂直翻转，这里还包含了多个90度的翻转

0:31:10.400,0:31:15.680
所以其实有8个对称的翻转变形。

0:31:15.880,0:31:26.960
我发现这些变形设置参数值对planet数据集很好用，其中格外有趣的是max_warp

0:31:27.100,0:31:33.260
perspective warping这项技术很少library提供，即便提供，运行都很慢。

0:31:33.460,0:31:38.880
fastai library 是第一个提供快速运行的perspective warping.

0:31:39.100,0:31:49.260
有趣之处在于，如果你从上向下观察，或者反过来，图片形状发生改变

0:31:49.860,0:32:00.280
当你对猫狗拍照时，有时从高往低处拍，有时相反的情况，这样导致的图片形态的变化，是你希望加入图片的变形

0:32:00.580,0:32:06.440
并将这种变形融入到你的批量数据中

0:32:06.740,0:32:12.760
但是对于卫星图则不行，因为拍照时总是垂直向下的拍摄

0:32:12.900,0:32:19.580
所以如果你加入perspective warping这种变形是不会存在于现实中

0:32:19.680,0:32:21.680
所以我将这个变形关闭

0:32:21.720,0:32:26.960
这就是数据增强data augmentation，在后续课程中我们还会大篇幅讲到

0:32:27.320,0:32:32.880
目前，你可以感受一下数据增强，到底是在做什么

0:32:33.380,0:32:51.700
一般而言，值得注意的是，如果你的数据是卫星图或医疗影像，无所谓上下之分，将flip_vert设置为True，模型通常能有更好的表现。

0:32:51.880,0:32:58.480
这里都是必须的工作来构建我们的DataBunch

0:32:58.900,0:33:13.420
接下来我们要对卫星图片做多标注分类，识别每张图片对应的天气，地表形态等

0:33:13.620,0:33:19.760
但事实上，我们没有新东西可学，之前学过的基本可以照搬使用

0:33:20.000,0:33:28.000
我们来看看构建模型，需要输入参数包括，数据，模型结构

0:33:28.440,0:33:42.920
之前通常采用Resnet34, 然后尝试Resnet50, 发现Resnet50 效果更好

0:33:43.820,0:33:48.940
我的确对metrics做了些修改

0:33:49.280,0:33:54.800
提醒一下，metrics与模型训练表现无关。

0:33:55.120,0:33:59.420
改变metrics，不会丝毫改变模型能力强弱

0:33:59.640,0:34:03.980
使用metrics的目的，是为了将它们在训练时打印出来

0:34:04.300,0:34:09.620
这里你看到打印出两列metrics：准确度和fscore

0:34:09.900,0:34:17.040
如果你的目的是提升模型效果，改变metrics，是不会有任何帮助的

0:34:17.340,0:34:29.980
关于metrics的使用，你可以不用，使用一个，使用多个metrics，在训练时打印出来

0:34:30.500,0:34:41.120
在这里我需要两个metrics，一个是准确度，另一个是Kaggle的评判标准

0:34:41.380,0:34:46.440
Kaggle说它会用F-score来评价我的模型

0:34:46.780,0:34:52.540
我不会花时间跟你解释什么是F-score，因为它没那么有趣，但你可以自己搜索一下

0:34:52.840,0:35:00.500
但它的基本概念如下：当你有一个分类器，计算结果是会有false postive (预测正，但错了） 和 false negative （预测负，但错了）

0:35:00.740,0:35:04.720
我们该如何权衡这两个值，得到一个唯一的评估值？

0:35:05.060,0:35:13.500
虽然有很多方法，但F-score提供一个非常棒的单一值

0:35:13.960,0:35:24.520
而且F-score有不同版本，F-1， F-2，kaggle要求使用F-2

0:35:24.920,0:35:37.000
我们这里有一个函数fbeta来生成F-1或者F-2或则其他F-score

0:35:37.180,0:35:41.720
我们可以看看它的signature（函数定义）

0:35:42.380,0:35:57.220
其中参数beta默认值是2，kaggle要求F-2所以无需修改， 我们这里要关注的是thresh，也就是阈值

0:35:57.420,0:36:01.080
它的用途是什么呢？

0:36:01.260,0:36:11.480
还记得我们是如何查看accuracy 准确度的定义的吗？使用两个问题来查看

0:36:11.600,0:36:17.880
我们采用argmax这个函数，我们使用它的目的是，如果还记得的话

0:36:18.320,0:36:32.440
我们有这些图片数据输入值，给到模型，得到这些输出值，共10个值

0:36:32.720,0:36:35.500
就好像是我们在做MNIST的10个数字的分类识别

0:36:35.680,0:36:42.560
这10个数值就好比是对应10个数字digit每个数值背后的概率值

0:36:42.720,0:36:47.280
然后我们查看所有概率值，找到最高的那个

0:36:47.480,0:36:58.860
而numpy, pytorch, math等library提供找出最大值对应的序号的函数，都叫做argmax

0:36:59.240,0:37:12.860
所以宠物分类器要找出概率最高的宠物，会用accuracy中的argmax锁定概率值最高的序号，进而找到对应的类别

0:37:13.200,0:37:20.860
然后对比真实值（差值）再求均值

0:37:21.100,0:37:23.100
这就是accuracy的定义

0:37:23.380,0:37:33.920
我们无法直接将accuracy套用到planet中因为有多个标注

0:37:34.360,0:37:55.260
取而代之的做法如下：首先，data.c告诉我们输出值的数量

0:37:55.820,0:38:08.460
每一个输出值对应一个类别的概率值。因此，data.classes的数量与data.c的值相同

0:38:08.500,0:38:13.960
这里我们可以查看所有类别的细节

0:38:14.140,0:38:24.080
在这个案例中，每个类别也都会对应一个概率值，但我们不是选择17个类别中的某一个，而是选择n个

0:38:24.240,0:38:27.860
我们的方法是将设置一个阈值，让17个概率值与它最比较

0:38:28.360,0:38:38.180
所有高于阈值的类别，我们认为模型判断该图片含有这类特征或景物。于是我们选择这个阈值。

0:38:38.600,0:38:50.120
我们发现针对这个数据集，通常将阈值定在0.2，模型表现不错，当然你可以尝试实验找出最优值

0:38:50.280,0:38:59.800
于是，我想让模型输出阈值=0.2的准确度accuracy,但常规accuracy在这里不起作用。

0:38:59.980,0:39:05.180
我们需要新的metrics，叫做accuracy_thresh

0:39:05.360,0:39:12.820
这个函数不是去一个最大值（argmax),而是让所有类别概率值与阈值比较，返回所有大于阈值的类别概率值。

0:39:13.000,0:39:18.100
因此，其中一个参数就是thresh

0:39:18.220,0:39:23.280
当然我们的metric会自动帮助执行accuracy_thresh

0:39:23.480,0:39:28.320
但是每次运行该函数时我们不希望反复告知我们要的阈值

0:39:28.520,0:39:36.060
所以我们需要定制一新版本的accuracy函数，每次都会使用thresh=0.2

0:39:36.460,0:39:57.280
一种解决方案是在accuracy_thresh加上制定thresh值的基础上，再做一个新函数叫acc_02(inp, targ)

0:39:57.800,0:40:32.000
但python3提供一种更简便的方案：采用partial, partial(accuracy_thresh, thresh=0.2) 效果是在执行同一个函数accuracy_thresh时，将参数thresh设定为0.2

0:40:32.100,0:40:45.200
这里的acc_02与之前我们写的acc_02的效果是一样的，但更加简洁。在fastai中我们经常使用这样的方法。

0:40:45.580,0:40:52.900
很多时候我们需要输入一个函数，而且往往是轻微变体版本

0:40:53.300,0:41:03.220
这里我们有两个变体版本acc_02, f_score, 然后将它们一同放入metrics中

0:41:03.440,0:41:10.260
然后，继续常规的一些任务，lr_find, recorder.plot

0:41:10.620,0:41:18.760
找到最陡峭的一段，大概在1e-2，这就是我们的学习率

0:41:18.880,0:41:23.680
然后训练一段时间，5epocs, slice(lr)，再看看训练情况

0:41:23.900,0:41:31.900
我们的accuracy准确度大概有96%， F-beta大概有92.6%

0:41:32.240,0:41:38.640
然后我们可以前往kaggle planet 排行榜

0:41:38.840,0:41:45.160
看看private 排行榜，第50名的位置是0.93

0:41:45.520,0:41:51.600
这样一来，我们能马上意识到模型表现还行，没有什么大问题

0:41:51.800,0:42:00.880
事实上，你会发现，一旦数据集准备完毕，训练模型，其实没什么额外可做的。

0:42:01.400,0:42:12.080
（提问）如果你的加载到云端APP的模型作出错误预测，是否有办法搜集这些错误，进而对模型做有针对性的改进训练？

0:42:12.380,0:42:21.000
当然，是的，这是好问题。首先，是否能搜集数据？当然可以，你自己做主自己搜集

0:42:21.260,0:42:23.980
你们自己本周就可以试试

0:42:24.460,0:42:32.360
你需要你的用户告诉你哪里出错了：某辆澳大利亚汽车你标记品牌是Honda,但实际上是Falcon

0:42:32.880,0:42:38.760
要搜集这些数据，唯一的办法就是让用户给你反馈，哪里出错了。

0:42:39.440,0:42:50.360
因此你可以做log存档记录：数据所在，预测所在，用户更正所在

0:42:50.780,0:42:58.060
然后，一天或一周结束时，我们可以要求自动或手动，让模型跑跑

0:42:58.340,0:43:04.280
怎么跑？微调模型，fine-tuning 模型，怎么做微调呢？

0:43:04.520,0:43:07.720
很好的提问引导，Rachel, 微调长得这个样子

0:43:08.180,0:43:17.500
假设这是我们刚刚训练好并保存的模型，接下来解冻这个模型，然后进一步训练它

0:43:17.940,0:43:29.040
这里我们用的是原有的数据，但你可以用新更正的数据再做一个DataBunch, 然后再训练

0:43:29.400,0:43:41.460
那些更正的数据，是需要特别关注的，我们可以调高学习率来训练，或者是训练更多轮次

0:43:41.880,0:43:50.440
所以，操作是一样的，喂给更正后的数据（图片和更正的标注），训练即可

0:43:50.740,0:43:56.280
这一操作能有效改进模型，当然还有其他的改进技巧

0:43:56.500,0:44:00.240
这里讲到的是最基础的技巧方法

0:44:00.460,0:44:07.460
（提问）是否可以讲讲data_block的设计原理，我不清楚到底该如何使用

0:44:07.620,0:44:13.580
它们必须遵循一定的顺序吗？有没有别的library库也用类似方法，我可以借鉴的？

0:44:13.820,0:44:25.320
是的，必须遵循一定的顺序

0:44:25.580,0:44:29.800
顺序，正如你在之间的范例中所见那样

0:44:30.260,0:44:46.400
什么类型的数据，来自哪个文件夹，如何提取标注，如何分割，要生成怎样的dataset，如何变形（可选），如何生成DataBunch

0:44:46.580,0:44:51.780
这些就是基本步骤或顺序

0:44:52.160,0:44:58.320
我们发明这个data_block API，我们不清楚其他人是否同时也发明了这个API

0:44:58.680,0:45:07.680
但是，背后的概念原型（pipeline, 用 . 连接不同的函数）的使用是很常见的

0:45:08.020,0:45:15.120
虽然python中不常见，但很多编程语言里都有，比如javascript

0:45:15.560,0:45:21.200
这里的每一步都会生产一个稍有不同的输出值

0:45:21.720,0:45:30.760
你会在ETL软件（用来做比如加载等，需要多步骤流程的工作）中看到这种用法

0:45:30.940,0:45:34.180
是的，data_block API 的灵感来自很多地方。

0:45:34.540,0:45:46.120
你需要做的，就是用这个例子来引导你，然后查询文档寻找你具体想用的功能

0:45:46.360,0:45:57.460
举个例子，你无法在data_block API文档中找到ImageFileList的文档说明，因为它在vision部分的文档中

0:45:57.680,0:46:06.980
所以，如果你要做的是某个领域里的应用，可前往application中找到具体领域，比如vision, text,去查看它们的文档

0:46:07.300,0:46:12.420
查看它们内部的data_block API 再进一步寻找你要的功能

0:46:12.640,0:46:21.840
当然，你还可以查看源代码，如果你面对新的应用，需要植入新的data_block 步骤，你可以自己写

0:46:22.220,0:46:28.300
这里的每一个步骤，其实都只有几行代码

0:46:28.560,0:46:30.560
也许我们可以去看些例子

0:46:30.680,0:46:45.960
我们先用ImageFileList.from_folder生成t, 再来看看t.label_from_csv的源代码

0:46:46.400,0:46:53.840
这样我们就能打开源代码，首先生成了一个df (dataframe), 然后在执行label_from_df这个函数

0:46:54.340,0:47:01.380
这些就已经很有帮助，因为如果你要从dataframe而不是CSV中生成标注

0:47:01.600,0:47:05.000
你可以直接执行label_from_df

0:47:05.320,0:47:19.420
你还可以看看label_from_df的源码，所以如你所见，大多数fastai的函数只有几行代码，你很容易看出所有需要的工具和输入值是什么，以及如何使用

0:47:19.780,0:47:28.700
只要多尝试多实验，你就会了解如何使用了

0:47:28.860,0:47:35.420
如果大家自己在下面练习时依旧有问题，请告诉我们，我们会想办法帮助你

0:47:35.900,0:47:46.660
（提问）你推荐怎样的软件来提取视频中的图片，然后在喂给模型？

0:47:47.140,0:48:06.280
这个因人而异。如果你是用网页提取，我猜这是多数人的选择，就用Web APIs

0:48:06.600,0:48:15.040
Web API 可以帮助你调取视频图片，再输送给模型

0:48:15.160,0:48:20.340
如果你用本地软件，多数人可能选择opencv

0:48:20.700,0:48:33.120
如果本周有人分享自己使用的软件心得，我们可以尝试在wiki中梳理这些软件和学习资源

0:48:33.260,0:48:39.380
也许这会帮助到大家

0:48:39.740,0:48:49.920
接下来还是常规操作：解冻模型，继续训练，模型表现提升到了92.9左右

0:48:50.220,0:49:01.160
有点需要注意的，在我们解冻之前，我们通常画出这样的图

0:49:01.400,0:49:14.760
记得我们之前就说过，学习率要取坡度最陡的一段，如果你取最低点的值，模型开始训练后损失值反而会上升

0:49:15.060,0:49:21.940
在解冻之后，再次画学习率的图

0:49:22.220,0:49:24.980
这是图通常会有所不同

0:49:25.460,0:49:49.740
面对这种形态的学习率，很难判断最佳学习率，因为上下坡都很浅，很平缓；我的做法是找到损失值最地点，在缩小10倍的学习率就是了，也就是1e-5

0:49:50.160,0:50:06.500
这是学习率的最小值，最大值设置为lr的1/5或1/10，lr则是我们封冻训练时采用的学习率

0:50:06.900,0:50:18.300
所以，简单原则是，学习率最低值我们取损失值最小位置所在学习率/10，学习率最大值取解冻前采用的学习率的1/5或1/10

0:50:18.640,0:50:27.920
这种方法通常管用，随后的课程中，我们进一步讲解这种方法，也就是discriminative learning rate

0:50:28.580,0:50:56.960
我们如何让成绩上到0.929或更高？kaggle排行榜里有1000队伍左右竞赛，前10%的成绩需要0.929

0:50:57.100,0:51:01.380
我们还没真正达标

0:51:01.580,0:51:11.160
这里是我们要用的技巧。大家还记得吗？我们在生成DataBunch时，让图片尺寸限定在128

0:51:11.360,0:51:16.300
但是kaggle给我们的图片是256的尺寸

0:51:16.520,0:51:25.620
我们缩小了图片，一个目的是为了更快的实验和训练

0:51:25.880,0:51:37.260
另一个目的是，我们可以获得一个针对128尺寸图片预测效果良好的模型

0:51:37.460,0:51:43.580
那么，如果我们要训练一个能良好预测256x256图片的模型，该怎么办呢？

0:51:43.880,0:51:54.340
为什么我们不用迁移学习？为什么不从一个已经训练好的擅长预测128x128图片的模型开始，对其微调，让它变得擅长预测256x256的图片？

0:51:54.580,0:51:58.300
而不是从头训练起。这样做是非常有意思的，因为

0:51:58.640,0:52:05.320
如果我们训练过多，我们有过拟合的风险，这是我们不想要的

0:52:05.760,0:52:18.640
所以，做迁移学习，使用256x256图片继续训练，我们构建一个长宽翻倍，面积变大4倍的DataBunch, 这样数据，对于CNN模型而言，其实是全新不同的数据。

0:52:18.900,0:52:24.340
基于此，当我们继续训练时，模型不会再有过拟合风险

0:52:24.960,0:52:34.580
那么，现在我们保持Learner学习器/模型不变，只需要再造一个256x256的DataBunch

0:52:35.020,0:52:49.340
这就是为什么我们将DataBunch的创建过程拆分为2部分，先构建src, 再修改图片尺寸大小为256, 生成新的DataBunch

0:52:49.780,0:53:03.660
我们来看看具体怎么操作：从src开始，生成dataset, 然后做变形，在变形时将图片大小改为256

0:53:04.020,0:53:16.100
（用这个数据训练）模型表现会更好，因为图片更大更清晰；同时我们用的是刚训练好的模型做迁移学习

0:53:16.240,0:53:25.400
接下来，我们将新数据移植给模型/学习器，然后在将模型封冻起来

0:53:25.640,0:53:32.580
也就是仅训练最后几层，再搜索一下学习率

0:53:32.780,0:53:46.820
我们的模型对128图片很好，对256图片应该也OK，所以学习率图没有之前的大斜坡区间段了

0:53:47.360,0:53:59.880
但是图上仍有损失值大福变坏的区间，找到变坏前的学习率，在缩小10倍，也就是1e-2/2

0:54:00.120,0:54:06.740
也就是再损失值抬头的前一段位置，然后再继续训练一段时间

0:54:07.060,0:54:22.140
因为封冻，我们只训练最后几层，训练过程中发现，很快模型就超越前期的0.929

0:54:22.420,0:54:28.960
因此，我们现在就正式进入了前10%阵营

0:54:29.300,0:54:43.500
那么现在，我们至少可以自信做卫星图识别分类工作了。但是，我们还可以继续解冻模型，接着进一步训练

0:54:43.920,0:54:49.840
使用相同的方法选择学习率

0:54:50.080,0:54:55.000
在进一步训练，结果上升到了0.9314

0:54:55.260,0:55:10.220
这样一来我们就进入了前20左右的阵营

0:55:10.600,0:55:22.140
大家看，一年前我和另一个朋友，耗费了数月时间，才做到了22名

0:55:22.300,0:55:36.460
所以，现在的fastai哪怕只用默认参数，加上一点放大图片尺寸的技巧，你可以快速进入这个竞赛的前端位置。

0:55:36.700,0:55:48.620
当然，我们并不知道现在这个成绩能让我们到什么排行榜的位置，因为需要用kaggle提供的测试集生成结果，再向kaggle提交结果来评判，你们可以点击late submission做提交

0:55:48.840,0:55:59.660
之后的课程会讲到怎么提交。但至少现在我们知道我们模型表现不错。

0:56:00.460,0:56:23.520
大家看到，每次训练后我都会保存模型，名字中的信息包含了是否冰冻模型，图片大小和模型结构，有了模型回头再实验就非常方便了。

0:56:23.740,0:56:30.020
这就是我们的planet数据集和多标注分类问题

0:56:30.220,0:56:40.080
我们来看看另一个数据集camvid

0:56:40.320,0:56:48.560
这是所谓的图片分割Segmentation任务。我们从这样的图片开始，然后要生成一张对每个像素按类别上色的新图

0:56:48.740,0:57:00.200
这里面所有的自行车者的像素是一种颜色，道路一种颜色，周边树一种颜色，建筑一种颜色，天空一种颜色，等等

0:57:00.380,0:57:11.100
我们不会给像素真正上色，我们给予每个颜色一个数字代码

0:57:11.340,0:57:17.700
这里建筑左上角数字对应位置是建筑所在，因此颜色代码这里应该对应的是4，右上角对应的位置上是树，矩阵中对应的位置上的颜色代码是26

0:57:18.120,0:57:42.240
，等等。所以，这个任务实际是对原图中每一个像素做分类。就像是宠物分类一样，比如我们对左上角的第一个像素做分类，识别它属于道路，树，自行车者，还是别的

0:57:42.520,0:57:56.220
所以，就这样我们对每一个图片的每一个像素来做分类，这就是Segmentation任务。

0:57:56.540,0:58:13.320
为了做Segementation像素分离任务，你要么下载要么自己做数据，数据里的图片每个像素都需要有颜色代码

0:58:13.700,0:58:26.680
可以想象自己做数据是一项巨大的工作，所以通常你不会自己做数据，而是去下载数据

0:58:27.000,0:58:38.380
这样的任务在医疗影像，生命科学领域很常见，比如当你看到一个细胞核的切片图片时，这是已经做过Segmentation处理了

0:58:38.560,0:58:46.480
如果你是在医疗射线部门，你可能有很多关于肿瘤的Segmentation图片

0:58:46.920,0:58:56.920
在不同领域里，有各自专业的工具来生成这些Segmentation 图片

0:58:57.160,0:59:03.620
从这张图可以看出，它在无人驾驶中也有应用。

0:59:03.780,0:59:08.660
来帮助查看周边有哪些物体以及所在位置

0:59:08.880,0:59:17.140
在这个例子中，有一个很棒的数据，叫camvid我们可以下载，

0:59:17.320,0:59:25.000
它里面有很多图片和分割数据（Segmentation masks）已经为我们准备好了。这个非常酷！

0:59:25.120,0:59:33.960
记住：我们用的所有数据都有对应的url

0:59:34.140,0:59:40.280
你可以在course.fast.ai/datasets中查看数据细节

0:59:40.400,0:59:45.380
它们几乎都是学术数据集

0:59:45.460,0:59:53.360
它们的存在，是因为一些大好人花费大量精力和麻烦，创造并整理了这些数据，方便我们使用

0:59:53.580,1:00:06.420
如果你用到这些数据做项目，那么一个好举措是：找到对应数据引用方式，注明感谢这些数据作者。

1:00:06.520,1:00:13.200
因为他们提供了数据，他们只要求被给予应有的荣誉(credit)

1:00:13.340,1:00:20.720
这里是camvid数据源，这里是如何引用，点击此处进入对应论文

1:00:20.880,1:00:25.580
好的，Rachel，现在是问答的好时机

1:00:25.920,1:00:32.520
是否有办法在用learn.lr_find时

1:00:32.700,1:00:35.200
让它直接返回学习率数值建议

1:00:35.200,1:00:36.980
而不是画一张图

1:00:37.140,1:00:40.000
再让我们用眼睛去图中挑选数值

1:00:40.360,1:00:45.900
还有一些问题也是要求提供更多寻找学习率的方法

1:00:47.280,1:00:49.280
这是个好问题

1:00:49.280,1:00:50.540
简答：不行

1:00:50.700,1:00:58.120
不行的理由是，当前状态依旧比我想要的状态，更经验化

1:00:58.320,1:01:04.340
如你所见，如何解读学习率图，取决于所在训练阶段

1:01:04.720,1:01:06.720
图形模样

1:01:07.260,1:01:13.300
我感觉，当我们训练模型前端（head头部）时

1:01:13.440,1:01:17.340
也就是在解冻之前，学习率几乎都长这样

1:01:17.580,1:01:21.840
当然，你可以做一个更平缓版本的图

1:01:22.020,1:01:29.620
找到最大坡度，取值，那么你的模型应该没问题

1:01:29.860,1:01:35.120
但，像这样的图

1:01:35.460,1:01:39.260
需要一定量的实验

1:01:39.520,1:01:41.960
但好消息是你可以实验

1:01:42.120,1:01:46.020
显然，你不应该选择翘起的线段部分

1:01:46.140,1:01:50.700
几乎确定的是，你不会要最低谷区间

1:01:50.860,1:01:53.480
因为你需要它是下坡状态

1:01:53.680,1:02:03.240
你可以先尝试比最低点小10倍的值，然后再小10倍，试一系列数值，看哪个最优

1:02:04.380,1:02:06.380
仅需几周时间

1:02:06.560,1:02:11.620
你会发现多数时间你都能选择很好的学习率值

1:02:11.840,1:02:17.600
所以，这个阶段，仍旧需要通过实验来找感觉

1:02:17.780,1:02:21.340
对于不同形态，如何反应挑选学习率

1:02:21.580,1:02:28.040
也许这个视频出来后，就有人找到稳定学习率选择器

1:02:28.320,1:02:30.320
我们目前还没走到这一步

1:02:30.600,1:02:34.500
这可能不是特别复杂的工作任务

1:02:35.260,1:02:37.260
可以算是有趣的项目

1:02:37.500,1:02:42.400
可以先获取一些数据集，比如我们页面上所以数据集

1:02:42.680,1:02:49.820
尝试想象一些方法，对比课程中的方法

1:02:49.900,1:02:56.320
这应该会是有趣的项目，但目前我们没有这样的方法工具。

1:02:56.520,1:03:03.020
我相信这是也可实现的，但我们还没有做。

1:03:03.180,1:03:09.800
那么，我们怎么做图像分割呢？

1:03:10.000,1:03:12.960
和其他方法是一样的

1:03:13.200,1:03:19.980
首先，构建文件路径

1:03:20.000,1:03:22.000
从untar_data开始

1:03:22.160,1:03:25.280
再通过path.ls看看里面有什么

1:03:25.440,1:03:29.180
这里有一个文件夹叫标注，一个文件夹叫图片

1:03:29.340,1:03:31.920
然后对这些文件夹做路径

1:03:32.200,1:03:34.720
再打开看看里面

1:03:35.100,1:03:46.620
我们还发现图片文件夹里的文件名里的编码，标注分割（Segmented masks）文件夹里文件名也有编码

1:03:46.840,1:03:49.500
我们需要想办法找到它们之间的关联

1:03:49.560,1:03:54.800
通常可以查看README文件或相关网页

1:03:55.040,1:03:57.820
通常关联比较明显

1:03:58.060,1:04:01.920
这些文件有这样的特殊格式

1:04:02.100,1:04:06.100
而这些文件名有完全一样的格式只是多了_P

1:04:06.160,1:04:11.880
说实话，我其实也是猜测（它们是对应关系）

1:04:12.360,1:04:14.360
所以我写了一个小函数

1:04:14.620,1:04:18.280
先打开文件名再加上_P

1:04:18.460,1:04:23.280
然后单独保存，再打开它们（作图），于是发现我们猜对了

1:04:23.580,1:04:31.640
因此，我写了这个函数，可以将图片文件名转化成对应的标注文件名

1:04:31.940,1:04:34.780
我打开这个文件（作图）确保工作正常

1:04:35.200,1:04:42.000
通常，我们用open_image将文件转化成图片，在用.show作图

1:04:42.140,1:04:50.040
之前已经提及，这张图不是常规图片，它包含整数

1:04:50.480,1:04:58.220
因此我们需要用open_mask而不是open_image，因为我们要返回整数而非浮点数

1:04:58.400,1:05:02.180
fastai知道如何处理mask

1:05:02.380,1:05:07.580
如果使用mask_show, 图片会按照预设方式上色

1:05:07.580,1:05:08.960
这是为什么要用open_mask原因

1:05:09.740,1:05:11.740
我们可以打开数据看看

1:05:11.740,1:05:13.100
看看数据大小

1:05:13.360,1:05:15.360
720x960

1:05:15.620,1:05:19.440
我们可以进一步看看里面的数据

1:05:19.520,1:05:21.520
等等

1:05:21.740,1:05:26.140
你还会发现，还有一个文件叫codes.txt

1:05:26.560,1:05:28.560
另一个叫valid.txt

1:05:28.780,1:05:31.280
codes.txt我们可以加载打开看看

1:05:31.540,1:05:36.340
不出所料，里面又一个列表，告诉我们，例如数值4

1:05:36.580,1:05:40.800
1，2，3，4，对应的是“建筑”

1:05:41.020,1:05:43.020
最上角的确是“建筑”，没问题

1:05:43.220,1:05:46.380
就好像我们的三只熊（grizzly, black teddy bears)案例

1:05:46.620,1:05:51.900
这里有所有像素对应的类别

1:05:52.300,1:05:55.740
我们需要构建DataBunch

1:05:55.900,1:06:00.000
构建DataBunch,我们可以走data_blockAPI 流程

1:06:00.320,1:06:03.860
我们有ImageFileList, 有某个folder(文件夹）

1:06:03.980,1:06:10.060
我们可以用get_y_fn来构建标注

1:06:10.360,1:06:13.220
然后分割成训练和验证集

1:06:13.220,1:06:15.180
但这里不是随机分割

1:06:15.400,1:06:20.680
为什么不？因为这里的图片来自视频的帧

1:06:20.820,1:06:22.820
所以如果随机取图片

1:06:23.040,1:06:27.460
我将有两张相邻图片，一张在训练集，一张在验证集

1:06:27.660,1:06:29.660
那就太简单了，其实就是作弊

1:06:29.900,1:06:38.900
所以数据作者给了我们一个文件里面有一系列文件作为验证集

1:06:39.180,1:06:42.400
它们来自不同视频段

1:06:42.700,1:06:49.080
我们用split_by_fname_file和这个文件来提取验证集

1:06:49.320,1:06:53.160
然后我们可以创建datasets

1:06:53.200,1:06:57.780
我们还有一个列表（list）类别名称

1:06:57.880,1:07:01.900
通常数据集，如宠物和planet地球数据集

1:07:02.100,1:07:10.860
我们有字符串，说明这是某类狗这是某类猫，这是多云，等等

1:07:11.060,1:07:15.160
这里的每个像素并没有对应一个字符串

1:07:15.340,1:07:17.340
如果那样就太低率了

1:07:17.380,1:07:19.380
每个像素被标注了一个数字

1:07:19.640,1:07:22.440
然后有独立的文件，告诉我们每个数字对应的类别

1:07:22.720,1:07:28.760
所以这里classes=codes是我们告诉data_blockAPI每个像素标注对应的类别是什么

1:07:28.980,1:07:32.560
这些是data_block API 的函数参数

1:07:32.780,1:07:36.200
这里是变形函数部分

1:07:36.220,1:07:38.220
这里很有趣

1:07:38.400,1:07:48.620
我们有说过随机翻转图片，如果我们翻转自变量（independent variable 非标注图片）图片会怎样呢？

1:07:48.860,1:07:52.300
但去不翻转这张图（标注图片）

1:07:52.300,1:07:54.020
这样它们就不对应了

1:07:54.220,1:08:05.060
所以，我们要告诉fastai（不仅翻转X）也要翻转y(标注图片）

1:08:05.280,1:08:08.580
不论X做怎样的变形，Y也接受同样的变形

1:08:08.760,1:08:11.620
这些参数都是我们可以实验的

1:08:11.880,1:08:15.880
然后构建DataBunch, 使用较小的批量

1:08:15.980,1:08:22.000
因为为每个像素做分类，需要大量的GPU内存

1:08:22.080,1:08:24.980
所以我发现批量为8是我的GPU能接受的数量

1:08:25.200,1:08:28.160
然后做常规的归一化处理（normalization）

1:08:28.320,1:08:35.320
这里看起来不错，因为fastai知道自己在做分割（Segmentation）任务

1:08:35.460,1:08:39.740
当你执行show_batch, fastai将两张图（X， Y）合在一起，

1:08:39.740,1:08:41.080
对图片像素做上色处理

1:08:41.140,1:08:45.460
看起来不错吧，你可以看到树是绿色

1:08:46.140,1:08:48.140
红色在线上

1:08:48.960,1:08:50.960
这种颜色给了墙体

1:08:50.960,1:08:51.980
等等

1:08:52.240,1:08:54.240
你可以看到这里的行人

1:08:54.680,1:08:56.680
这是行人的背包

1:08:56.960,1:08:59.440
这是真实XY数据呈现的样子

1:08:59.800,1:09:02.780
这些完成后

1:09:03.260,1:09:10.320
我们可以开始构建学习器(Learner）

1:09:10.360,1:09:12.360
稍后会有更多细节

1:09:12.360,1:09:14.240
我们执行lr_find

1:09:14.540,1:09:17.620
找到最陡峭的点的值，这里是1e-2

1:09:17.860,1:09:20.740
执行fit, 参数是slice(lr)

1:09:21.020,1:09:24.660
查看准确度，保存模型

1:09:24.820,1:09:28.900
解冻模型，再继续训练

1:09:29.280,1:09:31.980
这就是基本流程

1:09:32.080,1:09:39.020
我们先休息，回来后，演示更多技巧

1:09:39.200,1:09:43.320
同时还会讲解这里定制的函数

1:09:43.420,1:09:46.640
然后再继续了解别的很酷的东西

1:09:46.780,1:09:50.140
8点整回到这里

1:09:50.340,1:09:52.340
6分钟（休息）

1:09:52.340,1:09:56.720
欢迎大家回来

1:09:56.940,1:09:59.860
我们用休息期间的问题来开场

1:10:00.200,1:10:08.540
（提问）你能用无监督学习，像素分类，用于自行车的例子？

1:10:08.700,1:10:12.120
从而规避人工标记所有图片（像素）

1:10:13.940,1:10:17.080
我们这里不是无监督学习

1:10:17.260,1:10:23.800
你可以感受到，我们不需要标注也能大概知道各种物体所在位置

1:10:24.060,1:10:28.940
如果时间允许，我们会用些例子展示如何做到

1:10:29.180,1:10:37.920
但是可以确定的是，你肯定无法获得如此高质量细致的输出结果

1:10:38.120,1:10:41.100
如果你想要这种水准的图片分割（Segmentation mask）

1:10:41.260,1:10:46.400
你需要非常好的图片分割（Segmentation mask）的真实标注来训练模型

1:10:51.920,1:11:05.120
是否有理由不做：用很多小图片来微调训练模型？比如64x64, 128x128, 256x256 等等

1:11:06.820,1:11:08.820
是的，你绝对应该这么做！

1:11:08.820,1:11:10.060
真的很好用！

1:11:10.400,1:11:15.520
试试看，我发现这个想法

1:11:15.920,1:11:21.260
最早是在几年前的课程中

1:11:21.520,1:11:26.160
我一开始认为这是很显而易见的，作为一个不错的想法展示给大家

1:11:26.420,1:11:29.020
后来发现没人对此发表过论文

1:11:30.260,1:11:32.260
我们开始对此实验

1:11:32.420,1:11:36.620
这成为我们赢得Imagenet竞赛的主要技巧之一

1:11:36.800,1:11:39.380
Dawn Benchmark imagenet 训练竞赛

1:11:39.800,1:11:46.320
我们发现不仅这个技巧不是主流技巧，而是人们从未听说过这个技巧

1:11:46.600,1:11:53.900
截止目前，有少数论文将这个技巧用于某些任务

1:11:53.900,1:11:55.580
但是仍不为多数人所知

1:11:55.740,1:12:00.040
这个技术让你的模型训练更快，泛化效果更好

1:12:00.300,1:12:08.180
但是仍旧很多未知内容需要研究，比如（图片）应该多大，多小，每个层级多少量

1:12:08.340,1:12:17.980
这个技术现在已经有了一个名称，叫progressive resizing

1:12:18.060,1:12:22.580
我发现64x64用处不大

1:12:22.880,1:12:31.200
但是这是一个好技巧，我肯定会尝试不同的尺寸(看效果)

1:12:35.780,1:12:39.340
(提问）准确度对于像素分割/分类意味着什么？

1:12:39.520,1:12:44.320
（计算公式）是不是正确分类的像素数量/总像素数量？

1:12:45.920,1:12:47.840
是的，就是这样

1:12:48.200,1:12:54.760
如果将像素想像成图片分类，我们就是用相同的准确率公式

1:12:54.960,1:13:00.740
事实上，你可以直接输入accuracy这个指标

1:13:00.960,1:13:06.400
作为你的衡量指标，但这里我们并不用它

1:13:06.660,1:13:10.760
我们创造了一个新衡量尺度叫acc_camvid

1:13:10.960,1:13:19.460
原因是，当他们标注图片时，一些像素被标注成Void

1:13:19.640,1:13:21.880
不清楚为什么

1:13:22.140,1:13:24.420
也许因为一些未知因素

1:13:24.640,1:13:27.120
也许因为某些人的失误

1:13:27.420,1:13:29.500
反正一些像素是Void

1:13:29.700,1:13:33.600
camvid论文说当你公布准确度时

1:13:33.860,1:13:37.060
你应该去除Void像素

1:13:37.240,1:13:40.980
因此，我们创造了acc_camvid

1:13:41.120,1:13:46.880
所有评估工具(metrics)都使用神经网络输出值

1:13:47.180,1:13:49.960
作为函数的输入值

1:13:50.200,1:13:53.640
target是要预测的标注（真实）

1:13:53.820,1:13:56.580
然后，构建mask

1:13:56.720,1:14:02.200
也就是非void的target区域

1:14:02.380,1:14:08.480
然后，对输入值做argmax

1:14:08.640,1:14:11.780
这是标准的argmax操作

1:14:11.880,1:14:16.420
对比输入值mask区域和target的mask区域

1:14:16.420,1:14:18.180
再取均值

1:14:18.320,1:14:23.380
几乎与标准的准确度函数代码一样

1:14:23.460,1:14:27.280
只多了这行mask代码

1:14:27.460,1:14:32.060
这是经常发生的

1:14:32.220,1:14:36.620
某个特殊kaggle竞赛要求的评估工具

1:14:36.900,1:14:40.340
或则是你所在机构的某个特定的衡量积分标准

1:14:40.520,1:14:43.880
经常有些小修改你需要做

1:14:44.220,1:14:46.220
但都像这样非常简单

1:14:46.500,1:14:50.800
如你所见，要做到这些，你需要熟知的

1:14:51.020,1:14:57.080
就是如何用Pytorch做数学运算

1:14:57.460,1:15:00.360
这是你需要练习的

1:15:03.120,1:15:10.120
（提问）我发现多数模型训练时训练损失值大于验证损失值

1:15:10.240,1:15:12.600
修正这个现象的最好方法是什么

1:15:12.800,1:15:17.960
需要补充的是，在尝试不同版本的轮次(epochs)与学习率(learning rate)组合后，仍旧如此

1:15:19.860,1:15:21.860
好问题，还记得上节课

1:15:22.060,1:15:28.400
如果你的训练损失值对于验证损失值，那么你是欠拟合状态

1:15:28.500,1:15:33.740
你需要让你的训练损失值小于验证损失值

1:15:34.300,1:15:39.840
如果你欠拟合，你可以训练更久

1:15:40.180,1:15:45.460
可以对最后部分用更低学习率训练

1:15:46.060,1:15:50.240
如果还是欠拟合

1:15:50.460,1:15:55.320
你就不得不降低正则化（regularization)，这个我们还未谈及

1:15:55.500,1:16:02.440
这门课后半部分，我们会花费大量时间讲解regularization

1:16:02.680,1:16:07.280
会具体着重讲解过拟合与欠拟合

1:16:07.380,1:16:09.380
是如何用regularization来处理的

1:16:09.560,1:16:15.780
再往后，会学到权值衰减(weight decay), 抛弃层(dropout), 数据增强(data augmentation)

1:16:16.180,1:16:18.180
这些都会是核心内容

1:16:24.460,1:16:31.980
对图片分割，我们不需要CNN

1:16:32.100,1:16:37.640
我们可以用CNN，但我们用的是一个叫U-net的模型设计

1:16:37.740,1:16:41.300
这个模型效果更好

1:16:41.480,1:16:46.340
我们找到它来看看

1:16:49.500,1:16:52.460
这是U-net的模样

1:16:52.700,1:16:58.440
这是讲述U-net的大学网站

1:16:58.820,1:17:04.760
我们将在这里(part1)和第二部分课程（part2 如果你学的话)里都讲到U-net

1:17:05.000,1:17:11.780
基本上，这一部分（斜向下）的模型，就是一个基础的CNN模型结构

1:17:11.940,1:17:19.180
从起初的很大的图片，一层层变小，直到最后只剩一个预测值

1:17:19.380,1:17:24.160
U-net还要做的是将（预测值）逐层变大，

1:17:24.460,1:17:28.780
然后将每一个下行缩小阶段（stage的输出值）传递到对面

1:17:28.780,1:17:30.720
从而创造了这个U型结果

1:17:30.940,1:17:39.140
最初，（U-net）是作为生物医学图片分割方法发表的。

1:17:39.340,1:17:43.060
但它的用途远超出生物医药领域

1:17:43.260,1:17:49.440
(U-net) 发表在MiKAI主流医疗影像大会

1:17:49.820,1:17:58.020
截止昨天，这篇论文已经成为这个大会拥有了最高引用量的论文

1:17:58.020,1:18:01.360
（论文）非常有帮助，引用量超过3000

1:18:01.500,1:18:03.920
当前你不需要了解相关细节

1:18:04.220,1:18:12.580
目前只需要掌握一点，如果要做图片分割，你要使用Learner.create_unte

1:18:13.180,1:18:15.180
而不是创建CNN

1:18:15.540,1:18:24.060
但输入参数都一样，数据data，模型architecture，评估工具metrics

1:18:24.260,1:18:27.780
之后，所有操作也都一样

1:18:27.960,1:18:34.560
用lr_find，进一步训练，看到准确率accuracy上升

1:18:34.880,1:18:37.740
定期保存模型，解冻模型

1:18:38.040,1:18:44.160
（寻找学习率）还在上升中，所以继续往后缩小10倍

1:18:44.520,1:18:47.900
1e-5, lr/5

1:18:48.320,1:18:52.280
再继续训练一段

1:18:52.620,1:18:56.580
这里很有趣

1:18:56.920,1:19:01.860
learn.recorder帮助记录训练过程

1:19:02.020,1:19:04.760
包含一系列有用函数，其中一个是plot_losses

1:19:04.980,1:19:10.000
能画出训练和验证损失值

1:19:10.120,1:19:15.980
而且你会经常看到，（损失值）先上升在下降

1:19:16.180,1:19:22.380
为什么呢？因为，当你使用learn.recorder.plot_lr()画出学习率时

1:19:22.480,1:19:28.360
对应时间变化，发现学习率先上升再下降

1:19:28.500,1:19:30.500
为什么呢？

1:19:30.780,1:19:34.640
因为我们用了fit_one_cycle, 这就是它的工作行为特点

1:19:34.980,1:19:39.700
（fit_one_cycle) 让学习率从低处开始，然后上升，再下降

1:19:39.940,1:19:42.820
为什么这是一个好想法

1:19:42.960,1:19:45.340
要知道为什么这是个好想法，我们先来看看

1:19:45.940,1:19:50.720
一个非常酷的项目，

1:19:51.340,1:19:53.340
是Jose Femandez Portal本周做的

1:19:53.740,1:19:58.340
他将SGD（Stochastic Gradient Descent）Notebook

1:19:58.640,1:20:02.440
对模型参数作图

1:20:02.760,1:20:10.800
对应时间（为X轴）作图，不仅是对照真实值与模型作图，还尝试了一系列不同学习率值

1:20:11.080,1:20:16.700
还记得我们有两个参数 y = ax + b （参数是a,b)

1:20:16.940,1:20:22.440
或则如图参数分别是w0,w1

1:20:22.680,1:20:26.080
所以，我们可以看到，随着时间推移

1:20:26.120,1:20:28.120
模型参数有怎样的变化

1:20:28.260,1:20:30.260
我们知道这是准确答案

1:20:30.460,1:20:35.040
当lr=0.1, 损失值曲线像这样滑动到这里

1:20:35.260,1:20:38.620
你会看到需要花费点时间变到这里

1:20:38.920,1:20:44.480
你会看到损失值在改善；当损失值较大时如0.7

1:20:44.720,1:20:51.200
模型变到能拟合真实值状态的速度较快

1:20:51.480,1:20:54.800
模型参数更新到位非常快

1:20:55.080,1:21:00.340
如果学习率特别高会怎样

1:21:00.580,1:21:04.200
模型参数更新很久才能降低损失值足够低

1:21:04.340,1:21:08.260
或者如果学习率真的太高，损失值会diverge也就是越来越大

1:21:08.460,1:21:13.700
这里你会发现为什么合适学习率非常重要

1:21:13.920,1:21:18.860
当你有合适学习率时，损失值降到足够低会非常快

1:21:19.220,1:21:26.240
接下来，在你越来越接近最低损失值时

1:21:26.580,1:21:30.180
会发现有趣现象

1:21:30.540,1:21:34.340
我们希望学习率进一步下降

1:21:34.580,1:21:37.480
因为我们已经接近损失值最小值convergence

1:21:37.720,1:21:40.220
那么到底背后发生了什么

1:21:44.340,1:21:47.100
我们来看看背后的机制

1:21:48.040,1:21:50.040
我们只能画出2维

1:21:50.300,1:21:56.760
通常我们看不到这样的损失值曲面

1:21:56.980,1:22:02.620
记住，我们面对的往往是多维的，而且多数长得都非常bumpy颠簸

1:22:03.060,1:22:11.720
所以，我们希望学习率足够高从而能跳跃过颠簸bumpy部位

1:22:12.080,1:22:14.960
然而一旦接近中间部位

1:22:15.240,1:22:21.180
一旦接近最优答案（最低损失值convergence)所在，你就不希望它在bumpy部位来回跳跃

1:22:21.540,1:22:23.540
你会希望学习率下降

1:22:23.760,1:22:27.380
所以，越靠近（最优损失值），我们希望学习率越来越小

1:22:27.620,1:22:34.820
这就是为什么我们希望学习率到后来不断下降

1:22:35.320,1:22:41.240
这个让学习率不断变小的想法其实一直存在

1:22:41.540,1:22:43.960
称之为learning rate annealing学习率退火

1:22:44.180,1:22:49.340
但是学习率一上来很小，这个想法是崭新的

1:22:49.960,1:22:51.960
主要来自一个叫Leslie Smith的人

1:22:52.320,1:23:01.440
如果你下周在旧金山，可以参与到我和Leslie的采访（meetup），我们会探讨这些问题

1:23:02.040,1:23:05.680
Leslie 所发现的是

1:23:06.020,1:23:11.220
如果你逐步调大学习率，通常会发现

1:23:16.920,1:23:18.920
通常会发生的是

1:23:19.080,1:23:25.060
损失值曲面会长的这样，非常bumpy颠簸，颠簸，颠簸，然后平缓

1:23:25.060,1:23:27.020
然后继续颠簸, 颠簸, 颠簸

1:23:27.180,1:23:31.000
就像这样，有颠簸部位，有平缓部位

1:23:31.300,1:23:35.720
如果你落在一个颠簸部位的底部

1:23:35.920,1:23:38.980
这样的模型通常无法很好泛化(generalization)

1:23:39.320,1:23:42.860
因为你的解决方案（模型）擅长在某个点（特点）解决问题

1:23:43.160,1:23:45.600
但不擅长解决其他地方（特点）的问题

1:23:45.880,1:23:48.040
如果你的模型出现在平缓地段（模型损失值对应这里）

1:23:48.060,1:23:50.060
模型可能会泛化很好

1:23:50.260,1:23:53.760
因为这个模型不仅擅长解决所在一点的问题，也能解决周边区域的问题（因为都是平的，大家都一样）

1:23:54.480,1:23:57.020
如果你的学习率很低

1:23:57.380,1:24:04.580
那么通常会缓慢下降，然后困住这些地方

1:24:04.780,1:24:07.820
但如果逐步提升学习率

1:24:08.360,1:24:17.540
一开始会跳下一段，然后随着学习率上升，会反复大幅向上跳跃，就像这样

1:24:17.800,1:24:20.880
然后，学习率会在这个区间来回跳跃

1:24:21.040,1:24:23.440
最后，随着学习率逐步下降

1:24:23.780,1:24:27.840
通常会慢慢找到这些平缓的区间

1:24:28.280,1:24:38.800
我们发现，逐步调大学习率，能帮助模型探索更大范围的模型可能性（function surface）

1:24:39.220,1:24:46.640
从而找到一个区间，这里损失值低而且不颠簸

1:24:46.840,1:24:49.400
因为如果颠簸，（模型）会被提出这个区间

1:24:49.580,1:24:53.600
所以，这个方法运行我们有很大学习率训练

1:24:53.660,1:24:56.580
意味着我们能更快解决问题

1:24:56.740,1:25:00.160
意味着我们的解决方案更具泛化能力

1:25:00.620,1:25:03.560
所以，如果执行plot_losses()后

1:25:03.880,1:25:11.420
你发现，一开始损失值有点变差，但随后变得非常好，说明你找到了很好的最大学习率值

1:25:11.620,1:25:14.700
当你执行fit_one_cycle

1:25:14.960,1:25:16.960
你不是输入某个学习率

1:25:17.160,1:25:21.020
你是在输入最大学习率

1:25:21.240,1:25:26.860
如果损失值总是一路向下，尤其是在解冻之后

1:25:27.180,1:25:31.040
意味着你可以适度调高学习率

1:25:31.340,1:25:34.140
因为你会更希望看到这样的（先上后下）形态

1:25:34.340,1:25:37.020
因为这样会训练更快泛化更好

1:25:37.340,1:25:41.480
只需要一点上翘就可以，验证损失值上会更明显

1:25:41.480,1:25:42.960
橘黄色是验证集

1:25:43.400,1:25:49.820
理解理论和能上手操作的区别

1:25:50.200,1:25:52.720
就是多看这样的图

1:25:53.000,1:25:59.360
当你完成训练后，输入learn.recorder. 按下 tab

1:25:59.360,1:26:01.100
看看有些什么函数在里面

1:26:01.420,1:26:03.620
尤其是有plot的函数

1:26:03.840,1:26:08.140
逐步熟悉那些训练较好模型下画出的图

1:26:08.420,1:26:11.920
然后尝试让学习率更高更低

1:26:12.200,1:26:14.200
更多更少epochs轮次

1:26:14.200,1:26:15.360
积累这些条件下作图的样子

1:26:16.920,1:26:18.920
所以，在这里

1:26:20.660,1:26:28.320
我们在变形函数transform中使用size（大小）这个参数

1:26:28.640,1:26:30.640
一开始我们的size大小，是src_size的一半

1:26:30.840,1:26:34.700
//在python中的意思是整数除法 integer divide

1:26:34.940,1:26:38.920
因为我们不能接受半个像素之类的存在

1:26:39.200,1:26:42.980
这是为什么用//2的原因。 批量设置为8

1:26:43.220,1:26:46.740
这个大小合适我的GPU，但不一定合适你的

1:26:46.940,1:26:50.520
如果不合适，可以进一步下调到4

1:26:50.680,1:26:56.920
这并不能真正解决问题，因为问题在于对所有像素做分割

1:26:57.120,1:27:00.560
而不是一半的像素，所以我使用了之前用过的技巧

1:27:00.920,1:27:06.360
现在我们要将图片尺寸变成原有完整尺寸

1:27:06.700,1:27:11.780
所以我不得不进一步将批量减半，反正GPU内存将用尽

1:27:12.540,1:27:19.460
然后是设置Learner，我们可以这么设置learn.data = data

1:27:19.720,1:27:23.440
但我们在GPU内存方面有很多麻烦，所以

1:27:23.460,1:27:25.460
通常我们会重启kernel核

1:27:25.620,1:27:31.280
然后从这里开始，然后在新建一个模型，再加载之前训练好的模型

1:27:31.500,1:27:37.000
但关键是，我们的模型现在拥有之前训练好的权重参数

1:27:37.340,1:27:40.200
但数据现在是全尺寸了

1:27:40.980,1:27:42.980
现在我们可以做lr_find

1:27:43.400,1:27:47.340
找到损失值上升的前一段区间

1:27:47.600,1:27:49.600
所以我们有了lr = 1e-3

1:27:49.600,1:27:51.520
再进一步训练

1:27:51.720,1:27:55.680
在解冻模型，进一步训练

1:27:55.880,1:28:03.100
你可以用learn.show_results()来对比预测值predictions与真实值ground truth

1:28:03.580,1:28:09.660
毫无疑问，看起来非常不错

1:28:10.160,1:28:13.180
那么到底多好才是非常好呢

1:28:13.600,1:28:18.380
这里的准确度是0.9215

1:28:18.560,1:28:22.880
我所见过的最好的图片分割论文

1:28:23.200,1:28:32.320
是100层tiramisu这篇论文, 开发了卷积densenet, 时间就在2年前左右

1:28:32.520,1:28:36.120
今天训练完毕，我回头看了一下论文

1:28:37.080,1:28:43.500
发现他们的state of art 准确度是

1:28:44.920,1:28:46.920
在这里

1:28:47.040,1:28:50.040
我查对了一下，他们最好成绩

1:28:50.100,1:28:52.700
是 91.5

1:28:53.000,1:28:56.520
我们的是92.1

1:28:56.780,1:29:01.160
我的承认，当我今天的反应是: Wow哇

1:29:01.320,1:29:04.840
我不知道这篇论文出来后是否有更优秀的成绩出现过

1:29:05.020,1:29:08.260
但我记得当这篇出来时，业界为之一振

1:29:08.500,1:29:16.000
我的反应也是Wow，对比之前图片分割成绩，的确非常棒

1:29:16.000,1:29:17.520
这是一个巨大的进步

1:29:18.000,1:29:22.440
所以在去年课程中，我们花费大量时间

1:29:22.900,1:29:24.900
重头写了100层tiramisu模型

1:29:25.100,1:29:34.540
现在用我们fastai的默认值设置，我们轻松打败了这些（成绩，当年的最高水平）

1:29:34.820,1:29:38.400
我还记得当初取得这样的成绩需要很多个小时的训练

1:29:38.700,1:29:42.520
然而今天，我们只需要训练几分钟

1:29:42.720,1:29:47.940
这是一个超强大的图片分割模型设计

1:29:48.220,1:29:53.360
我无法承诺这个（我们取得的）肯定是当前的最高水平

1:29:53.520,1:29:57.120
因为我还没有做彻底的对过去2年的论文查阅

1:29:57.460,1:30:07.160
但至少打败了在去年课程时期的我们了解到的世界最高水平

1:30:07.280,1:30:13.580
这是都是我们在课程中积累下来的技巧

1:30:13.740,1:30:22.620
比如如何训练得更好，比如用已训练好的模型，比如使用one-cycle convergence等等

1:30:23.020,1:30:26.100
它们都超好用！

1:30:26.580,1:30:40.920
我们感觉很酷的是， 在课程中展示这些技术，其中一些不同版本的技术的工作原理，我们还未发表，

1:30:41.160,1:30:46.900
如果你回来上part 2的课程，我们会在其中讲解具体的工作原理

1:30:46.980,1:30:53.920
但对你而言，在这个阶段，只需要会用Learner.create_unet，就能有不错的表现

1:30:59.800,1:31:02.200
这里还有一个技巧你可以用。

1:31:02.460,1:31:07.180
如果你经常发现GPU内存不够用

1:31:07.580,1:31:13.020
你可以尝试mix precision training

1:31:13.560,1:31:21.900
所谓的mixed precision training 就是，对于有计算机专业背景的人而言，比较容易理解，就是与其做单一浮点数计算

1:31:22.300,1:31:27.920
你可以用半个浮点数来完成大多数的计算量

1:31:27.920,1:31:29.900
所以，是16bit, 而不是32bit

1:31:30.180,1:31:37.660
这个想法，只是在过去两年时间里出现流传的

1:31:37.820,1:31:40.800
比如，一些硬件，能用这个运行得非常快

1:31:40.960,1:31:48.480
fastai 库是第一个且可能是唯一一个让它（mixed precision training）特别易用的库

1:31:48.720,1:31:53.380
如果在任何Learner.模型构建后面加上.to_fp16()

1:31:53.720,1:31:58.980
你将获得一个基于16浮点计算的模型

1:31:59.220,1:32:06.160
因为（这个技术）很新，所以你需要最新的cuda驱动

1:32:06.160,1:32:07.980
等等，让它正常运行

1:32:08.100,1:32:13.080
今早我尝试其他平台，结果是kernel无法持续工作

1:32:13.260,1:32:15.880
所以，必须确保拥有最新的cuda 驱动(drivers)

1:32:16.340,1:32:21.980
但如果你有非常新的GPU，如2080Ti

1:32:22.140,1:32:26.220
这个技术不仅能正常运行，而且可以让速度翻倍

1:32:26.480,1:32:29.160
之所以提到这个

1:32:29.340,1:32:33.400
因为它会减少GPU内存使用量

1:32:33.940,1:32:36.620
但即便你没有2080Ti,

1:32:37.220,1:32:46.220
你会发现不适合你的GPU运行的代码，在有.to_fp16()之后反而能正常运行了

1:32:46.580,1:32:53.440
我从未见人用.to_fp16()在图片分割任务中

1:32:53.820,1:32:57.360
抱着玩笑心态，我实验了一下

1:32:57.640,1:33:02.980
却发现实验出了更好的成绩

1:33:03.340,1:33:07.680
我也是今早才做的实验，所以没有更多可以和大家深入分享的了

1:33:07.840,1:33:14.900
除了这一点：通常在深度学习中如果运算不精确，往往可以泛化更好一点

1:33:15.000,1:33:21.760
但是我从未见过（如此高的精度）0.9247出现在camvid数据集中

1:33:21.960,1:33:27.220
所以，这个技术不仅让我们更快，还能让批量更大

1:33:27.380,1:33:31.840
而且效果还会更好

1:33:32.240,1:33:39.040
这就是我所说的“小技巧”，你只需要确保每次生成Learner时加上.to_fp16()

1:33:39.160,1:33:43.420
如果你的kernel核停止工作，说明可能你的cuda驱动过时了需要更新

1:33:43.600,1:33:48.300
或者可能是你的显卡太老了

1:33:48.440,1:33:51.580
但我并不知道哪种显卡支持.to_fp16()

1:33:54.420,1:33:59.920
在总结本节内容前，还有一个（数据集）

1:33:59.920,1:34:03.900
不好意思，还有两个（数据集）

1:34:04.060,1:34:10.260
第一个，我要展示给你们的，叫做BIWI head pose数据集

1:34:10.620,1:34:16.420
Gabriele Fanelli 非常慷慨给我们许可让我们在课程中使用这个数据集

1:34:16.580,1:34:22.260
他的团队创建了这个数据集，这是数据的模样

1:34:22.380,1:34:27.440
这是图片，事实上里面有不少内容，我们这里只用一个简单版本

1:34:27.680,1:34:33.760
其中一个内容是，在图片中有一个点，意思是面部的中心点

1:34:34.020,1:34:36.680
我们要就此构建一个模型

1:34:37.100,1:34:39.100
能够找出面部中心

1:34:39.280,1:34:49.560
针对这个数据集，有些特定的内容需要做，我也不是特别了解相关细节

1:34:49.780,1:34:52.200
我只是通过README.md发现这是些不得不做的事

1:34:52.400,1:34:56.240
他们使用了深度探测设备，他们应该用的是kinect

1:34:56.420,1:35:00.740
有一些测量数字，通过一个文件提供

1:35:00.740,1:35:02.300
我需要调取其中内容

1:35:02.620,1:35:14.340
他们还提供了一个函数，你需要借助它来提取坐标；从而将深度感应测量的数据转化为图片中的坐标点

1:35:14.600,1:35:17.920
当你打开这个函数，看到这些转换流程

1:35:18.200,1:35:22.660
我们就只是在做他们让我们做的事情，没什么特别的

1:35:22.860,1:35:26.620
与深度学习没什么关系，在获取这个点的过程中

1:35:26.980,1:35:29.740
有趣的地方在于

1:35:30.160,1:35:34.060
我们创造了一个并非图片的东西

1:35:34.320,1:35:37.140
也不是图片分割，而是图片点ImagePoint

1:35:37.660,1:35:40.960
我们主要会在后面学到

1:35:41.300,1:35:45.740
基本上，图片点ImagePoint

1:35:45.980,1:35:49.280
借用了坐标点的概念

1:35:49.500,1:35:53.400
它们不是像素值，而是（x, y) 坐标值，2个数值

1:36:06.660,1:36:08.660
这里是一个例子

1:36:08.880,1:36:12.040
我们取了一个具体的图片

1:36:12.300,1:36:14.300
在这里

1:36:14.700,1:36:18.320
这里是面部点的坐标

1:36:19.000,1:36:21.000
（263， 428）

1:36:21.960,1:36:23.960
就在这里

1:36:23.960,1:36:25.300
就只有2个数值

1:36:25.940,1:36:27.940
来表示图片中的脸的中心

1:36:28.340,1:36:32.160
如果我们要创建一个模型来寻找脸的中心

1:36:32.400,1:36:35.660
我们需要一个神经网络能输出2个数值

1:36:35.940,1:36:39.480
但注意，这不是分类模型

1:36:39.860,1:36:46.360
这不是2个数字，可对照列表来判断是公路或建筑，或猫狗，等等

1:36:46.840,1:36:50.600
它们是真实的位置

1:36:52.700,1:36:55.420
目前为止，我们做的都是分类模型

1:36:55.740,1:36:57.740
输出的是标注或类别

1:36:58.040,1:37:01.280
这是第一次我们做回归模型regression model

1:37:01.480,1:37:04.640
很多人认为回归就是线性回归，其实不然。

1:37:05.100,1:37:11.560
回归模型，就是任意模型，它的输出值是一个或多个连续值

1:37:11.940,1:37:15.080
我们要创建的是图片回归模型

1:37:15.360,1:37:17.920
能够预测这样的两个数值

1:37:17.920,1:37:19.840
我们该怎么做呢？

1:37:21.260,1:37:23.260
同样的方法

1:37:23.640,1:37:27.360
我们可以说，这里有一些图片文件

1:37:27.360,1:37:28.400
在这里文件夹里

1:37:28.480,1:37:30.480
我们要对图片做标注

1:37:30.920,1:37:32.920
用这个函数

1:37:33.120,1:37:39.060
这是我们基于README文件写的函数，从text文件中提取坐标

1:37:39.200,1:37:41.860
每个图片会返回给我2个数值

1:37:42.000,1:37:45.720
然后根据某个函数将数据分割成（训练和验证集）

1:37:45.940,1:37:51.840
在这里，他们给我们的文件（图片）是源于视频

1:37:51.980,1:37:55.340
所以我选择了一个文件夹

1:37:55.540,1:37:57.540
来作为我的验证集，换言之

1:37:57.540,1:37:58.720
选择了不同的人

1:37:58.940,1:38:02.680
所以，我当时在想，如何公正地验证（模型）

1:38:02.840,1:38:08.040
一个公正的验证应该是模型能够在一个未见过的人的图片上作出准确的预测

1:38:08.280,1:38:12.180
所以我的验证集就是一个不同于训练集的人物图片

1:38:12.420,1:38:18.460
创建一个Dataset, 并告知这是一个关于点的Dataset

1:38:18.680,1:38:21.180
所谓点，就是特定的坐标

1:38:21.680,1:38:26.500
然后在做变形，将tfm_y设置为 true

1:38:26.680,1:38:30.700
因为当图片翻转或旋转时，红色的点也需要随着移动

1:38:30.820,1:38:35.240
选择了一个图片大小，进而让处理速度更快

1:38:35.240,1:38:37.020
创建DataBunch再做归一化

1:38:37.200,1:38:39.920
然后show_batch于是就有了这些图片

1:38:40.360,1:38:46.040
我发现红点并不总是在脸部上，但并不清楚为什么

1:38:46.240,1:38:51.020
不清楚他们的内在定位红点的算法是什么

1:38:51.200,1:38:55.100
看起来有时在鼻子上，有时又不是

1:38:55.320,1:38:59.500
算了，反正就是在脸部或鼻子中间的位置

1:38:59.500,1:39:00.720
那么如何构建模型呢？

1:39:01.160,1:39:03.160
我们构建一个CNN

1:39:03.700,1:39:10.040
但是，我们会学到很多损失函数

1:39:10.140,1:39:12.140
在后几节课中

1:39:12.480,1:39:15.280
通常，所谓损失函数就是

1:39:15.540,1:39:19.100
生成一个数字告诉我们模型的好坏程度

1:39:19.220,1:39:21.220
对于分类问题

1:39:21.380,1:39:24.120
我们用一个叫cross-entropy loss交叉熵损失值的损失函数

1:39:24.440,1:39:28.900
基本上是说，还记得之前课程，

1:39:29.120,1:39:31.120
你有预测正确的类别吗

1:39:31.320,1:39:33.820
你对此自信吗？

1:39:34.040,1:39:36.200
但我们无法将它们用在回归问题上

1:39:36.420,1:39:39.640
所以我们采用了一个叫Mean Squared Error的损失函数

1:39:39.860,1:39:42.920
如果你还记得上节课

1:39:43.040,1:39:45.040
我们从头写了MSE

1:39:45.140,1:39:49.140
也就是将差值平方在求其均值

1:39:49.400,1:39:53.680
所以，我们要告诉模型不是在做分类，所以要用MSE

1:40:03.020,1:40:07.440
所以，我们要告诉模型不是在做分类，所以要用MSE

1:40:07.760,1:40:11.100
在我们构建了模型Learner, 采用了MSE，

1:40:11.100,1:40:12.780
可以继续做lr_find

1:40:12.780,1:40:14.760
然后fit训练模型

1:40:15.040,1:40:18.840
然后你可以看到在1分半的时间里

1:40:19.140,1:40:23.020
MSE已经是0.0004

1:40:23.500,1:40:27.140
MSE 非常棒的一点是易解读

1:40:27.460,1:40:33.320
我们要预测某些值，值的大小是数百左右

1:40:33.740,1:40:38.020
我们的MSE损失值只有0.0004

1:40:38.160,1:40:40.940
因此，我们可以自信这是非常不错的模型

1:40:41.200,1:40:44.240
我们可以通过learn.show_results()来查看结果

1:40:44.620,1:40:51.000
预测值（左）真实值（右），模型准确无误，几乎完美

1:40:51.220,1:40:56.600
这就是做图片回归的方法

1:40:56.980,1:41:00.300
所以，以后如果你有预测连续数值

1:41:00.560,1:41:02.560
你要用类似这样的方法

1:41:07.320,1:41:11.080
在我们看基础理论之前，来看最后一个案例

1:41:11.720,1:41:13.720
自然语言处理 NLP(natural language processing)

1:41:13.900,1:41:17.500
下周我们会看到更多NLP

1:41:17.760,1:41:23.260
现在我们要做相同的事情，不是图片分类模型

1:41:23.500,1:41:27.540
我们要做的是文本分类

1:41:27.820,1:41:32.660
下周我们会具体深入细节，现在我们快速过一遍

1:41:32.840,1:41:38.700
不再做fasta.vision, 而是用fastai.text

1:41:38.880,1:41:43.300
在这里可以找多所有做文本分析的工具

1:41:43.440,1:41:48.200
这里我们要用的数据集是IMDB

1:41:48.400,1:41:52.440
IMDB 含有很多电影评论

1:41:52.620,1:41:55.280
每个大概有1000个单词

1:41:55.420,1:42:02.340
每个影评要么是正面要么是负面

1:42:02.500,1:42:07.460
它们存在CSV文件中，我们可以用pandas打开来看看

1:42:07.680,1:42:10.600
也可以看看影评内容

1:42:10.740,1:42:17.940
基本上我们通常可以采用集成函数factory methods

1:42:18.020,1:42:20.900
或则是data_block API 来创建DataBunch

1:42:21.000,1:42:25.960
这里是一个快速基于CSV文本来构建DataBunch的方法

1:42:25.960,1:42:28.760
TextDataBunch.from_csv就行

1:42:30.600,1:42:37.340
到这一步，我们可以创建一个Learner开始训练

1:42:37.540,1:42:41.680
这里我们稍稍看点细节，但主要会在下周讲到

1:42:41.880,1:42:47.420
创造这些DataBunch的步骤，就是这几步

1:42:47.620,1:42:50.100
第一步是叫做分词化Tokenization

1:42:50.400,1:42:56.320
也就是提取文本，在转化为标准的形式的tokens分词

1:42:56.620,1:43:01.180
基本是说，每一个token表示一个词

1:43:01.380,1:43:06.600
但是你看，这里didn't 被拆分成了2个词

1:43:06.820,1:43:09.560
还有所有词都变成了小写

1:43:09.860,1:43:13.100
you're 也被分成了2个词

1:43:13.340,1:43:16.360
Tokenization分词化的目标是确保

1:43:16.620,1:43:26.160
每一个token，每一个基本单位，代表一个单一的语言概念

1:43:26.520,1:43:33.820
同时，它还会找出特别少见的词

1:43:33.960,1:43:35.960
比如特别少见的名称，等等

1:43:36.100,1:43:39.500
然后用特殊token（'unknown') 来代替

1:43:39.680,1:43:43.520
任何以XX开头的都是特殊的token

1:43:43.740,1:43:46.800
这些就是Tokenization

1:43:46.980,1:43:50.740
会返回给我们一系列分词化tokenized的单词

1:43:51.000,1:43:54.920
还会看到标点符号周边也有空格

1:43:54.920,1:43:56.660
以确保它们也是独立的tokens

1:43:56.780,1:44:05.260
下一步是，我们提取所有独特的tokens

1:44:05.260,1:44:06.920
称之为Vocab单词表

1:44:07.140,1:44:09.520
它是自动生成的

1:44:09.680,1:44:12.160
这里看到的是前10个单词

1:44:12.260,1:44:18.520
它们是出现在所有影评中的独特单词token的其中10个

1:44:18.780,1:44:23.740
然后我们将每个影评转化成一列数值

1:44:23.920,1:44:42.860
这一列数字中的每个数值对应着一个token的位置，比如这里的6对应的是Vocab中的第6个token 'a'；比如这里的3对应的是Vocab中的第3个token ',' , 诸如此类

1:44:43.060,1:44:47.800
所以通过tokenization和Numericalization数值化，这是NLP的标准操作

1:44:47.980,1:44:50.880
来将一个文本转化成一列数字

1:44:51.040,1:44:55.520
我们可以用data_block API 来完成一些工作

1:44:55.740,1:44:59.520
这一次我们不是用ImageFileList

1:44:59.660,1:45:02.340
而是TextSplitData

1:45:02.560,1:45:06.000
从CSV中提取文本，再转化成datasets

1:45:06.180,1:45:09.180
再做tokenization转化，然后是numericalization转化

1:45:09.360,1:45:13.900
再生成DataBunch。到这里

1:45:14.080,1:45:18.200
我们就可以开始创建模型了

1:45:18.360,1:45:24.480
下周，当我们做NLP分类任务时

1:45:24.480,1:45:26.080
我们实际创造了2个模型

1:45:26.220,1:45:29.220
第一个被称之为语言模型

1:45:29.440,1:45:35.980
如你所见，我们用常规方法训练

1:45:35.980,1:45:37.460
我们这里是要训练一个language_model_learner

1:45:37.560,1:45:42.160
我们训练，保存，解冻，继续训练

1:45:42.300,1:45:45.700
在我们完成了language model语言模型之后

1:45:45.760,1:45:48.460
我们微调fine-tune 来创造分类器

1:45:48.620,1:45:50.620
这里我们为分类器创造DataBunch

1:45:50.660,1:45:55.820
构建一个Learner来做分类(text_classifier_learner)

1:45:55.820,1:46:01.760
我们训练模型，然后这里是我们的准确度

1:46:01.860,1:46:06.520
这是一个快速版本，下周我们会深入细节

1:46:06.740,1:46:15.480
你们已经看到训练NLP分类器的流程和之前模型的流程非常相似

1:46:15.660,1:46:23.000
目前IMDB数据集的state of art准确度是

1:46:23.220,1:46:31.200
我们和同事Sebasthra Ruder创造的算法实现的，并将其在论文中发表

1:46:31.280,1:46:35.080
所以我展示给大家看到的就是state of art的算法

1:46:35.200,1:46:39.880
如果再加上点小改动（技巧），你可以提升至95%准确度，如果你非常努力调试的话

1:46:39.960,1:46:45.600
所以这个结果(0.944)已经非常接近我们的state of art水平

1:46:46.840,1:46:50.180
现在是提问的好时机

1:46:50.520,1:46:59.760
（提问）对于不同于Imagenet数据集，如卫星图和基因图片（第二课里）

1:46:59.900,1:47:01.480
我们应该用自己的stats

1:47:01.620,1:47:08.540
Jeremy说过如果使用预先训练好的模型，我们需要用预先训练数据的stats. 为什么要这样呢？

1:47:08.640,1:47:14.140
难道自己的数据做完归一化后，与Imagenet（归一化）后的stats不一样吗？

1:47:14.220,1:47:17.360
我能想到的，让它们不同的地方，只有skewness

1:47:17.460,1:47:21.640
是skewness 还是其他的因素，导致了你（Jeremy）的这种说法？

1:47:21.740,1:47:26.480
这是不是意味着你不建议用其他数据训练好的模型？

1:47:26.700,1:47:30.280
比如one point mutation（你在第二课提到的）

1:47:32.360,1:47:35.560
不是的，如你所见

1:47:35.600,1:47:37.960
我们用预先训练好的模型做所有不同的任务

1:47:38.060,1:47:43.360
每次我用imagenet预训练模型，我就使用imagenet.stats

1:47:43.500,1:47:47.960
为什么呢？因为这个模型就是用这样stats的数据训练而来的

1:47:48.220,1:47:55.120
比如，你要构建一个分类器来区分各种不同的绿色青蛙

1:47:55.200,1:47:59.580
如果你要使用自己图片的各个channel的均值

1:47:59.680,1:48:07.040
你会将每个channel（红蓝绿）转化为均值为0标准差为1的分布

1:48:07.160,1:48:10.440
这就意味着它们（图片）不再像绿青蛙了

1:48:10.440,1:48:12.380
它们现在像灰色青蛙

1:48:12.480,1:48:15.800
但是Imagenet期待的青蛙是绿色的

1:48:15.940,1:48:22.740
所以，你需要用imagenet训练者采用的stas

1:48:22.840,1:48:26.040
否则你的数据的独特特征将不再显现

1:48:26.220,1:48:29.840
因为你已经将它们"归一"丢失了，至少针对这个红蓝绿channels中的特征而言

1:48:29.920,1:48:33.940
所以你必须使用预训练模型采用的数据的stas

1:48:41.120,1:48:44.960
在每一个案例中，我们做的

1:48:45.140,1:48:50.460
都是用Gradient Descend 加上小批量

1:48:50.560,1:48:52.940
来训练模型的参数

1:48:53.120,1:48:59.400
这些参数都是在做矩阵乘法(matrix multiplication)

1:48:59.400,1:49:04.180
part1中的第二部分里，我们会学一个这个matrix multiplication的小改动，叫卷积

1:49:04.260,1:49:07.080
这也是另一种matrix multiplication的版本

1:49:07.460,1:49:20.400
没有任何matrix multiplication足够强大到对IMDB对情绪分类

1:49:20.600,1:49:25.180
或者看着卫星图就能识别是否有路在其中

1:49:25.340,1:49:28.280
这些远超出线性分类模型的能力

1:49:28.420,1:49:34.920
我们知道这是深度神经网络，包含很多这样的matrix multiplication

1:49:34.920,1:49:40.560
但每个matrix multiplication都是线性模型

1:49:40.720,1:49:46.260
线性模型叠加线性模型依旧是线性模型

1:49:46.440,1:49:52.540
如果回到高中数学，你可能记得

1:49:52.680,1:50:01.880
如果你向y=ax+b上套入cy+d

1:50:01.880,1:50:05.780
它依旧是一条斜线加上intercept

1:50:05.940,1:50:10.760
所以，无论叠加多少matrix multiplication都不会有什么帮助

1:50:11.080,1:50:17.080
那么我们在这些模型中到底在做些什么？

1:50:17.180,1:50:21.860
有趣的是，我们所做的

1:50:22.240,1:50:29.480
的确是matrix multiplication或则matrix multiplication的修改版（卷积）之后会学到

1:50:29.620,1:50:38.080
但每次做完matrix multiplication后我们都会加一个非线性运算non-linearity或者叫激活函数activation function

1:50:38.420,1:50:45.100
激活函数的职能在于，将matrix multiplication的输出值，带入的另一个函数里

1:50:46.580,1:50:51.520
这里看到的都是我们会用的激活函数

1:50:51.700,1:51:01.580
早期我们常用的激活函数是这样的形态

1:51:01.660,1:51:05.520
这些形态函数叫做sigmoid函数或S函数

1:51:05.620,1:51:13.040
它们有其特定的数学定义

1:51:13.200,1:51:20.360
当下，我们几乎完全不用它们（在每两个matrix multiplication之间做运算）

1:51:20.420,1:51:24.980
今天，我们几乎都有这个函数

1:51:25.200,1:51:27.600
叫做Rectified Linear Unit

1:51:27.820,1:51:32.320
做深度学习，很重要的一条，是要用特夸张夸张特酷的名称

1:51:32.440,1:51:35.860
否则普通人也会认为自己也能行（调侃学界在名称上故弄玄虚，吓退大众学生）

1:51:36.160,1:51:45.960
仅限于你我之间，我（偷偷）告诉大家，其实所谓Rectified Linear Unit就是这样的一个函数

1:51:46.280,1:51:48.600
max(x,0), 这就是全部

1:51:48.720,1:51:53.040
如果你想将大多数公众排除在外，

1:51:53.320,1:51:59.520
你可以将名称缩短，叫它ReLU, 以显示你属于非常私密高端的一小撮人

1:51:59.640,1:52:03.060
这就是一个ReLU 激活函数(activation function)

1:52:03.380,1:52:06.280
这里是很疯狂的事情。

1:52:06.440,1:52:11.920
如果你将三色图片包装好给到matrix multiplication

1:52:12.160,1:52:16.560
然后，用0代替负数

1:52:16.560,1:52:18.420
然后在做一遍

1:52:18.460,1:52:22.220
然后持续这么做，一遍又一遍

1:52:22.280,1:52:24.680
于是，你就有了深度神经网络

1:52:24.880,1:52:27.060
就这么简单

1:52:27.400,1:52:29.400
那它到底是如何工作的？

1:52:29.700,1:52:33.900
一个非常酷的人，叫Michael Nelson

1:52:34.100,1:52:36.780
展示了其工作原理

1:52:37.040,1:52:41.160
他有一个非常棒的网页，也是一本书

1:52:41.280,1:52:44.400
neuralnetworkanddeeplearning.com

1:52:44.400,1:52:49.820
他还有这些漂亮的Javascript小工具你可以动手实验

1:52:49.940,1:52:53.480
因为写于很早时期，所以用的还是sigmoid函数

1:52:53.840,1:53:00.900
他展示了，如果你有足够的matrix multiplication

1:53:00.960,1:53:04.660
紧接着sigmoid函数

1:53:04.740,1:53:08.680
这样的结构换成ReLU后也同样适用

1:53:08.840,1:53:12.140
你可以创造出任意形态的图

1:53:12.640,1:53:24.060
这个想法：将matrix multiplication与非线性激活函数结合在一起

1:53:24.200,1:53:27.100
可以创造任意函数形态，其实有自己的名称

1:53:27.220,1:53:30.700
名字是universal approximation theorem

1:53:30.840,1:53:37.580
它的意思是，如果你有一组matrix multiplication和非线性激活层

1:53:37.920,1:53:44.740
那么你可以逼真模拟任意函数

1:53:45.080,1:53:51.240
你只需要确保有足够大的矩阵或足够多的矩阵（matrix）

1:53:51.400,1:53:57.220
如果你现在已经有了这个函数

1:53:57.340,1:54:00.560
也就是一连串的matrix multiplication和非线性激活层

1:54:00.680,1:54:03.840
非线性函数，可以是任意这些函数

1:54:04.000,1:54:07.820
我们通常用这个。如果能模拟任意函数，

1:54:07.980,1:54:14.680
那么你只需想办法要找到你matrix multiplication中的参数矩阵

1:54:14.720,1:54:16.720
来解决你想用解决的问题

1:54:16.820,1:54:20.200
而我们已知如何找到那些参数矩阵值

1:54:20.220,1:54:22.220
我们可以用梯度下降（Gradient Descent)

1:54:22.360,1:54:24.740
就这么简单

1:54:25.020,1:54:31.220
然而我发现跟学生解释非常困难的事情是：

1:54:31.220,1:54:34.340
我们已经学完了（深度学习核心理论）

1:54:34.580,1:54:37.920
学员在这堂课结束时，经常来找我说

1:54:38.060,1:54:42.080
剩下的部分呢？请跟我解释深度学习剩下的部分

1:54:42.220,1:54:44.500
但是没有剩下的部分

1:54:44.760,1:54:48.040
我们有一个函数，接收输入值（如图片像素）

1:54:48.220,1:54:50.220
乘上某些参数矩阵

1:54:50.220,1:54:51.960
用0取代负数

1:54:52.140,1:54:54.720
乘上另一个参数矩阵，再用0取代负数

1:54:54.720,1:54:56.340
重复几次

1:54:56.560,1:54:58.560
我们查看（预测值）与真实值之间距离

1:54:58.700,1:55:02.820
然后有梯度下降（借助导数）来更新参数矩阵

1:55:03.040,1:55:05.720
重复数次

1:55:05.900,1:55:10.140
最终我们会得到一个能做影评分类的模型

1:55:10.320,1:55:14.780
或者能对猫狗图片分类的模型

1:55:14.960,1:55:18.160
这就是全部了

1:55:18.320,1:55:23.640
构建直觉化的理解的难点在于

1:55:24.020,1:55:32.200
我们所说的参数矩阵，当全部加在一起，有数百万之多

1:55:32.340,1:55:34.940
这是非常大的参数矩阵

1:55:35.300,1:55:46.760
所以，我们的直觉：尝试对矩阵相乘，再以0取代负数，重复数次，这一过程的直观理解，是并不存在（无感性基础的）

1:55:46.960,1:55:51.760
你必须接受，真相是，从客观现实角度，

1:55:51.880,1:55:53.880
这么操作，就是有不错的表现

1:55:54.020,1:56:00.560
所以，在part2课程里，我们会从头来构建这些模型

1:56:00.740,1:56:09.140
到时，你会发现非常简单，似乎只有5行代码似的，

1:56:09.240,1:56:12.380
可能会有一个for loop, 写到

1:56:12.520,1:56:19.260
t  = x @ weight_matrix1

1:56:19.400,1:56:23.200
t2 = max(t, 0)

1:56:23.200,1:56:27.240
把它们放入for loop循环，来更新所有参数矩阵

1:56:27.480,1:56:30.440
最后，计算损失函数

1:56:30.500,1:56:34.760
当然，我们自己不会计算导数，因为Pytorch会自动完成

1:56:34.880,1:56:38.980
这就是全部

1:56:42.980,1:56:44.980
好的，提问

1:56:45.320,1:56:48.900
（提问）有一个关于Tokenization分词化问题。我很好奇，

1:56:48.940,1:56:53.060
如何分词化那些相互依赖的词，如San Fransisco

1:56:53.820,1:57:00.880
嗯，好的

1:57:01.380,1:57:08.820
分词化（Tokenization)，如何对San Fransisco做分词

1:57:09.180,1:57:17.920
San Fransisco 有2个分词， San 一个 Fransisco 一个，就这样

1:57:18.140,1:57:24.580
这样的问题，可能来自比较熟悉传统NLP的人

1:57:24.820,1:57:28.560
他们经常要用到叫N-gram的东西

1:57:28.980,1:57:36.060
早期NLP大多建立在线性模型上

1:57:36.240,1:57:41.280
基本上，你数某些字符串出现的次数

1:57:41.420,1:57:47.100
比如，一组字符串 San Fransisco 就是一个bi-gram,相对于n-gram而言，n这里是2

1:57:47.360,1:57:52.300
很酷的是，有了深度学习，我们再也不必为此(n-gram)烦恼

1:57:52.420,1:57:57.820
诸如此类，很多复杂的特征工程，在深度学习中消失了，不被需要了

1:57:58.020,1:58:03.280
对深度学习而言，每个分词Token，就是一个单词

1:58:03.440,1:58:09.620
例如，you're 看上去是一个词，其实在分词里被看成2个词

1:58:09.900,1:58:17.340
我们接下来要做的是，让深度学习模型去

1:58:17.580,1:58:20.140
发现最佳合并单词的方法是什么

1:58:20.320,1:58:24.120
当我们说，让深度学习模型去发现，我们的本意是

1:58:24.320,1:58:28.900
找到参数矩阵，借助梯度下降（Gradient Descent)

1:58:29.040,1:58:33.320
从而得到正确答案，就这样，没别的了。

1:58:33.700,1:58:39.040
当然，还有些小改进/技巧，在part1的后半段，

1:58:39.200,1:58:42.720
我们会学到专门针对图片模型的卷积(Convolution)，也就是CNN

1:58:42.860,1:58:50.300
对语言模型，我们会用到的技巧是，Recurrent 模型RNN

1:58:50.420,1:58:54.760
但它们都是非常小的改动

1:58:55.260,1:59:04.780
所以，我们发现RNN能学到san + fransisco能有不一样的含义

1:59:05.060,1:59:07.060
当2个分词合在一起

1:59:09.640,1:59:17.040
（提问）有些卫星图有4个channels，我们如何在预训练模型下，使用4个或2个channels的数据？

1:59:20.740,1:59:28.300
这是好问题，这是我们希望通过丰富fastai库来解决的问题

1:59:28.480,1:59:32.180
所以，希望当你看到这里时（视频观众），已经有简单的解决方案了。

1:59:32.560,1:59:40.900
简单思路是这样，一个预训练模型，期待的数据含有红蓝绿像素

1:59:41.020,1:59:47.280
如果你只有2个channels, 你可以有以下选择

1:59:47.480,1:59:51.200
基本上，你需要创造第三个channel

1:59:51.320,1:59:58.400
你的第三个channel的值，要么全是0值，要么是其它2个channels的均值

1:59:58.740,2:00:06.400
你可以用常规的Pytorch函数来创建

2:00:06.600,2:00:12.140
要么，你可以用循环loop来生成第三层channel，在开始训练前

2:00:12.300,2:00:17.700
或者你可以创造一个定制的dataset来构建这层channel

2:00:17.920,2:00:21.940
对于4个channels

2:00:22.060,2:00:25.120
你应该不会想要删除那第四个channel

2:00:25.260,2:00:30.380
所以，取而代之的是，修改你的模型

2:00:30.560,2:00:34.840
我们会在未来几节课里理解如何来做修改

2:00:34.960,2:00:36.960
基本思路是，

2:00:37.660,2:00:39.660
最初的参数矩阵

2:00:39.800,2:00:43.040
weight matrix 其实不是正确的用语

2:00:43.040,2:00:45.180
它们不是参数矩阵（weight matrix）而是参数张量（weight tensor）

2:00:45.240,2:00:47.760
所以它们可以有多于2维的维度

2:00:48.200,2:00:53.880
所以，神经网络的初始参数，其实是张量tensor

2:00:53.940,2:01:00.860
其中一个轴axis会有3个分叉slices

2:01:01.020,2:01:05.400
而你需要做的是将3slices 变成4slices

2:01:05.480,2:01:09.740
我通常会（给新增的slice里的数值）赋予0或随机数

2:01:09.980,2:01:12.580
这是简单版本

2:01:12.780,2:01:15.840
但要真正理解我的意思，

2:01:15.900,2:01:19.020
我们还需要继续完成后面几节课

2:01:20.180,2:01:23.340
好的，总结一下

2:01:23.420,2:01:25.420
我们今天学了什么呢？

2:01:25.600,2:01:31.220
基本上，我们开始时

2:01:32.820,2:01:39.940
我们发现做网页APP已经非常简单了

2:01:40.100,2:01:44.280
我们为你准备好了启动代码（模型）

2:01:44.420,2:01:50.360
来教你如何做网页APP，而且已经有学员用目前所学做成了很酷的APP

2:01:50.440,2:01:53.140
可以用来预测单一类别的分类

2:01:53.320,2:01:55.920
但很酷的是

2:01:56.100,2:02:06.180
我们可以用单一标注分类的工作流程来做多标注分类问题

2:02:06.240,2:02:09.660
比如Planet数据集问题

2:02:09.920,2:02:16.920
或者你还可以用来做图片分割问题

2:02:17.080,2:02:28.420
或者你还有用来做任意的图片回归问题

2:02:28.740,2:02:34.580
或者，现在可能有点早，但如果你还没有尝试了的话，你可以做NLP分类问题

2:02:34.580,2:02:36.440
还有更多

2:02:36.640,2:02:41.480
在每个案例中，我们所做的无非是

2:02:42.320,2:02:44.320
梯度下降(GD)

2:02:44.500,2:02:47.780
不是仅用于2个参数

2:02:47.920,2:02:53.960
而是用于数百万个参数，还只用了单纯无添加的梯度下降

2:02:54.120,2:02:59.240
还有就是非线性激活函数，通常用这个

2:02:59.940,2:03:04.220
通过Universal Approximation Theorem告诉我们

2:03:04.580,2:03:16.240
让我们能模拟对任意函数，包括将人的语音转化为语言文字的函数

2:03:16.360,2:03:20.040
或者将一句日文转化为英文的函数

2:03:20.140,2:03:23.280
或者将狗的图片转化为“狗”这个字

2:03:23.460,2:03:28.480
这些都是我们可以通过这种方法（深度学习）来学到的函数

2:03:28.760,2:03:35.840
那么，这周，看看你能否找到一个有趣的想法或问题任务

2:03:36.120,2:03:46.120
比如图片多标注分类，图片分割，图片回归等等

2:03:46.240,2:03:52.080
诸如此类，看看你能否解决这些问题

2:03:52.180,2:03:57.940
你可能会发现，其中最难的内容是构建自己的DataBunch

2:03:58.200,2:04:05.260
那么你就需要深入研究data_block API 来想办法构建你的DataBunch

2:04:05.640,2:04:13.820
通过练习，你会变得非常熟练，毕竟它不是一个很大的API，只有为数不多的函数

2:04:14.040,2:04:16.720
并且非常容易植入你自己的函数

2:04:16.900,2:04:21.900
现在，你如果你遇到问题，可以去论坛提问

2:04:22.740,2:04:30.420
好的，下周，我们将学习更多NLP内容

2:04:30.620,2:04:37.680
我们还会学习更多如何让随机梯度下降（SGD）训练更快的方法

2:04:37.820,2:04:41.820
我们会学到Adam， RMSprop等等

2:04:42.020,2:04:49.260
希望我们还能展示更多大家本周自己做的有趣的网页APP和模型

2:04:49.280,2:04:52.620
下周见，谢谢大家！
