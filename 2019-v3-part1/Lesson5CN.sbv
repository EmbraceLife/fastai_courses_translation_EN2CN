0:00:00.000,0:00:03.100
欢迎大家来到第五课

0:00:03.100,0:00:11.820
我们已经正式完成了一半的课程（爬坡），从现在开始另一半的课程（下坡）

0:00:11.900,0:00:15.580
（准确的说，前半部分就是在上节课的前半段完成的）

0:00:15.660,0:00:20.420
我们从机器视觉开始

0:00:20.480,0:00:29.720
因为这是最成熟的，开箱即用，的深度学习应用

0:00:29.780,0:00:35.280
换句话，（这类应用）如果你不用深度学习，很难有优秀的表现

0:00:35.320,0:00:43.420
所以，学了第一课与不学第一课的差异是，你将拥有之前没有的能力

0:00:43.560,0:00:50.200
你能看到很多技巧技艺

0:00:50.260,0:00:53.100
来更高效地训练模型

0:00:53.160,0:00:55.780
然后，我们开始学习NLP

0:00:55.820,0:01:03.140
因为，文本，是另一个不用深度学习就没有优异表现的领域

0:01:03.220,0:01:11.000
而且，现在的（深度学习NLP技巧）已经非常成熟

0:01:11.000,0:01:16.980
纽约时报就在昨天，专题报道了NLP领域的深度学习最新进展

0:01:17.020,0:01:20.760
大篇幅报道了我们在这个领域的贡献

0:01:20.880,0:01:27.380
也谈及了OpenAI，谷歌，以及Allen Institute of Artificial Intelligence的贡献

0:01:27.900,0:01:30.660
然后

0:01:32.100,0:01:39.120
我们用表格数据和协同过滤这两个应用，结束了所有的应用案例的学习旅程

0:01:39.140,0:01:47.020
一部分原因是表格数据和协同过滤，即便没有深度学习，也能有不错的表现

0:01:47.040,0:01:52.080
所以，没有什么巨大的突破，不存在之前做不到的现在可以实现的情况

0:01:52.080,0:01:57.180
还有一个原因是

0:01:57.280,0:02:03.040
在接下来的课时中，我们将理解这些应用的每一行源代码

0:02:03.140,0:02:10.560
而表格数据和协同过滤的源代码，要比机器视觉和自然语言处理的源代码更简单（没那么错综复杂）

0:02:10.580,0:02:14.040
现在我们（的学习路径是）要反过来往回走

0:02:14.100,0:02:17.440
要学习之前所有的应用的背后到底是如何工作的

0:02:17.540,0:02:23.560
我们会从上半部分结束的地方开始，也就是先协同过滤，在表格数据

0:02:23.640,0:02:29.620
在今天课程结束时，我们将学习它们两的源代码到底在做什么

0:02:29.680,0:02:32.400
这是我们的目标

0:02:33.480,0:02:37.300
对于这节课，你不应该预期

0:02:37.640,0:02:42.580
会学到一些新的应用案例，做一些你之前不能做的应用

0:02:42.620,0:02:48.740
这节课给你的是，我们到底是如何在解决之前所学的应用和案例

0:02:48.760,0:02:53.260
其中重点要学习的是，regularization 正则化

0:02:53.340,0:02:57.380
也就是，我们如何管控过拟合与欠拟合状态

0:02:57.420,0:03:03.520
我们的希望是，大家能从中学到一些技巧，并将它们用于你们之前的项目和案例

0:03:03.520,0:03:05.200
从而改进提升你的模型

0:03:05.220,0:03:10.000
或者是帮助你们应对感觉数据量不够时模型如何提升表现

0:03:10.020,0:03:12.340
或者是当你出现欠拟合状况

0:03:12.340,0:03:13.040
等等

0:03:13.580,0:03:17.740
同时，我们还会学到CNN的理论概念

0:03:17.780,0:03:22.200
在接下来的2节课中，我们还会深入学习RNN

0:03:22.220,0:03:25.320
在这个过程中，我们还会学习一些新应用

0:03:25.360,0:03:28.720
一些新的机器视觉和自然语言处理的应用

0:03:32.360,0:03:36.560
我们从上节课结束的地方开始

0:03:37.640,0:03:40.620
在上周

0:03:40.620,0:03:43.160
还记得

0:03:43.240,0:03:47.200
这张图

0:03:47.220,0:03:53.380
展示给我们是，一个神经网络长什么样子

0:03:53.440,0:03:59.440
我们有多个层

0:03:59.500,0:04:01.620
第一个知识点是

0:04:01.700,0:04:08.100
神经网络里只有2种/类层结构

0:04:08.180,0:04:15.080
一类层里储存着参数值，另一类层里存储的是激活值

0:04:15.160,0:04:21.360
参数，是模型学习所得

0:04:21.440,0:04:26.340
我们用梯度下降来

0:04:26.400,0:04:32.900
计算：参数 -= 学习率*参数的梯度

0:04:32.980,0:04:36.820
这就是神经网络的基本工作原理

0:04:36.900,0:04:47.000
我们让输入激活值与参数做点积

0:04:47.060,0:04:51.740
黄色的是我们的参数矩阵

0:04:51.760,0:04:54.740
广义上说是，参数张量（weight tensor）

0:04:54.740,0:04:56.480
但这两种说法没什么区别

0:04:56.480,0:05:05.020
我们让输入激活值或（前一层）激活值与参数矩阵做点积，来生成本层的激活值

0:05:05.020,0:05:10.600
激活值，也是数值，是（通过刚才运算）计算而来的

0:05:10.660,0:05:16.780
在我们的学习小组中，总有提问，问这些数值（激活值）是怎么来的

0:05:16.840,0:05:22.100
我总是反问：你告诉我，这是参数，还是激活值

0:05:22.100,0:05:23.660
因为非此即彼

0:05:23.660,0:05:25.660
这就是（激活值）的由来

0:05:25.700,0:05:28.980
我想输入值/层算是特殊的激活值/层

0:05:29.060,0:05:32.640
它们不是计算得来的，而是(预先准备好的)

0:05:32.640,0:05:34.440
所以才说它/输入层是特殊的

0:05:34.560,0:05:37.180
所以，（神经网络的层）可以是输入值/层，参数（层），或激活（层）

0:05:37.300,0:05:42.540
激活值不单单来源于矩阵乘法

0:05:42.640,0:05:45.200
也可以来自激活函数的计算结果

0:05:45.220,0:05:48.680
我们需要记住的关于激活函数最重要的事情是

0:05:48.740,0:05:59.620
它是一个element-wise function, 意思是每一个输入数据进入这个函数都会产生对应的一个输出数据

0:05:59.620,0:06:04.760
所以，如果输入一个长度为20的数组，函数会输出一个长度20的数组

0:06:04.800,0:06:09.380
工作原理是函数会对每一个输入值做处理，进而生产出一个输出值/激活值

0:06:09.380,0:06:11.640
所以，element-wise 就是一个接一个的意思

0:06:11.760,0:06:14.000
ReLU (线性整流函数 ) 是我们重点学习过的

0:06:14.500,0:06:18.600
事实上，关于激活函数的选择，不用考虑太多，

0:06:18.720,0:06:25.700
我们之所以不花时间在激活函数上，是因为主要用ReLU, 绝大多数时候效果都不错

0:06:25.880,0:06:30.440
然后，我们学了一个组合运算，即

0:06:30.500,0:06:35.120
矩阵相乘再加上ReLU, 然后这样的组合反复叠加

0:06:35.120,0:06:37.040
产生了一种非常棒的属性

0:06:37.160,0:06:40.100
叫做Universal Approximation Theorem 通用近似定理

0:06:40.140,0:06:52.540
即，如果你有足够大，足够多的参数矩阵，那么你就能解决任意难度的数学函数，实现任意精度水平

0:06:52.620,0:07:01.680
基于这个前提，你可以开始训练参数，当然还需要一定的时间和数据量，等等

0:07:01.740,0:07:09.020
这一块内容是我认为相对高级别一点

0:07:09.040,0:07:17.120
计算机专业的学员，容易感到混乱，总是询问“那么然后呢？核心技巧是什么呢？背后原理呢？”

0:07:17.120,0:07:18.920
但这就是全部

0:07:19.060,0:07:27.760
你就做这些内容，将计算出来的梯度，结合学习率，一起来更新参数，就这些了。

0:07:27.880,0:07:41.500
然后我们计算损失值，我们需要将真实目标值与输出层的激活值给到损失函数来计算，

0:07:41.600,0:07:51.680
基于损失值，我们计算所有黄色数值（参数矩阵里的参数值）的梯度，然后更新这些参数，使用参数 -= 学习率*参数梯度

0:07:51.720,0:07:57.660
这个计算梯度以及通过减去梯度的算法更新参数的过程，叫反向传递backpropagation

0:07:57.720,0:08:03.820
当你听到这个名称时

0:08:03.900,0:08:11.480
当你听到backpropagation这个名称时

0:08:11.520,0:08:18.520
这是其中一个神经网络专业人士特别爱用的词汇，因为听起来特别震撼

0:08:18.560,0:08:27.560
但是，在你的脑海中，可以翻译成: 参数 = 参数 - 学习率*参数梯度

0:08:27.620,0:08:31.680
我应该使用参数parameters, 内涵更广，而非权重weights。

0:08:31.700,0:08:35.940
这些就是我们上周所学

0:08:36.000,0:08:40.600
上周我提到，这周还会覆盖更多知识点

0:08:40.760,0:08:44.640
我们今天稍后会学到cross-entropy交叉熵和Softmax函数

0:08:44.740,0:08:48.520
现在我们来看看微调fine-tuning

0:08:48.600,0:08:53.500
我们来看看将会发生什么，如果我们拿来一个Resnet34,

0:08:53.500,0:08:54.720
来做迁移学习。

0:08:54.720,0:08:56.360
那么背后到底发生了什么呢？

0:08:56.360,0:08:58.160
第一个需要注意的是

0:08:58.280,0:09:02.900
我们在Imagenet中训练使用的Resnet34

0:09:02.960,0:09:06.040
在模型末尾，有着非常具体明确的参数矩阵

0:09:06.100,0:09:09.980
这是一个拥有1000列的参数矩阵

0:09:10.020,0:09:20.920
为什么会这样？因为Imagenet竞赛的要求是：找出任意一张图片所对应的类别（从1000个类别中寻找）

0:09:21.020,0:09:28.780
这就是为什么模型需要1000个，因为Imagenet的目标值target的长度是1000

0:09:28.860,0:09:33.860
你需要找到1000个类别概率值中最高的那个

0:09:33.940,0:09:40.160
有几个原因，说明这个参数矩阵对你无用

0:09:40.160,0:09:42.380
当你做迁移学习时。

0:09:42.440,0:09:44.900
第一个原因是，你的数据中没有1000个类别

0:09:45.040,0:09:47.900
我之前做的是，泰迪，棕熊，黑熊（的区分）

0:09:47.920,0:09:50.840
所以，我不需要1000个类别

0:09:50.880,0:09:55.060
第二个原因是，即便我有1000个类别，它们不是Imagenet中的1000个类别

0:09:55.080,0:09:59.320
所以，基本上，这整个参数矩阵

0:09:59.320,0:10:01.060
对我而言，就是浪费时间

0:10:01.160,0:10:03.460
所以，我们该怎么办？我们把它扔掉

0:10:03.500,0:10:07.840
所以，当你使用fastai中的create_cnn时，将会自动删除它（这个参数矩阵）

0:10:07.920,0:10:09.920
那（这个函数）会做什么呢？

0:10:10.040,0:10:14.820
取而代之，它会为你填入两个新的参数矩阵

0:10:19.120,0:10:21.840
它们中间会放一个ReLU线性整流函数

0:10:22.020,0:10:28.820
还有一些默认设置，关于（参数矩阵）的大小

0:10:28.880,0:10:32.860
会给到第一个参数矩阵，但是第二个参数矩阵

0:10:33.020,0:10:37.440
它的大小，是由你来设定的

0:10:37.520,0:10:40.480
在你的DataBunch数据堆里（你将DataBunch输入给学习器learner）

0:10:40.500,0:10:44.660
从这里(DataBunch)，我们就知道需要多少激活值

0:10:44.820,0:10:47.860
如果你做的是分类，这意味这类别的数量

0:10:47.880,0:10:53.060
如果你做的是回归，那么就是你需要预测的数值的数量

0:10:53.100,0:10:59.060
如果你的DataBunch叫data，那么用data.c就是预测值的数量

0:10:59.060,0:11:05.180
所以，我们fastai帮助大家添加这个参数矩阵，根据上一层尺寸将其大小规格储存在data.c里

0:11:05.240,0:11:13.080
那么现在我们需要训练它们（这两个参数矩阵）

0:11:13.160,0:11:15.740
因此一开始时这些参数矩阵内都是随机值

0:11:16.560,0:11:21.240
因为新参数矩阵总是随机设置的（毕竟是新的嘛）

0:11:21.340,0:11:24.560
所以，它们也是新的，随机设置的

0:11:24.560,0:11:26.660
所以，我们需要训练它们

0:11:27.380,0:11:30.040
但其他层

0:11:30.460,0:11:33.780
则不新，它们擅长别的事情

0:11:33.980,0:11:36.460
它们擅长的事情

0:11:36.540,0:11:40.500
还记得Zeiler and Fergus论文

0:11:40.620,0:11:46.620
这里是一个filter过滤器的可视化图

0:11:46.680,0:11:49.120
或者说是第一层的参数矩阵的可视化图

0:11:49.180,0:11:52.380
（这里）是一些用这个过滤器找到的东西/例子

0:11:52.440,0:11:58.760
所以，第一层参数矩阵的部分过滤器擅长寻找这个方向延伸的对角线边角（其他的9宫格，可以看出是其他方向和形态的边角）

0:11:58.800,0:12:04.960
而第二层，其中一个过滤器擅长寻找左上角的拐角区域

0:12:05.720,0:12:09.880
而在第三层，其中的一个过滤器擅长寻找

0:12:09.940,0:12:15.120
重复的特征和图案，另一个过滤器擅长寻找圆形且橘色的东西

0:12:15.200,0:12:19.720
另一个擅长找毛茸茸或植物群类似质地的东西

0:12:19.760,0:12:25.760
但随着层数的推进，能找的东西变得越来越复杂

0:12:25.780,0:12:28.940
但同时也越来越具体

0:12:28.980,0:12:33.220
比如第四层，我认为是在寻找眼球

0:12:33.280,0:12:40.200
那么，如果你希望迁移到别的领域，比如学习组织病理切片

0:12:40.640,0:12:43.300
那么很可能图片中是不存在眼球的，对吧

0:12:43.360,0:12:46.200
所以，之后的层对你没有帮助

0:12:46.300,0:12:50.040
（之前的层）肯定会有帮助的是，反复出现的特征图案，和对角线边角

0:12:50.140,0:12:57.040
所以，越早期的层，越有可能拥有你需要的特征图案，你越会需要它们继续存在下去

0:12:57.040,0:13:05.000
作为训练的第一步，我们必须训练这些新参数矩阵，因为它们是随机的

0:13:05.040,0:13:09.360
让我们暂时不用管（这两个参数矩阵之前）的层的参数矩阵

0:13:09.480,0:13:20.100
所以，让我们freeze所有这些层的参数矩阵

0:13:20.100,0:13:23.220
这是什么意思呢？这里意味着

0:13:23.220,0:13:26.900
我们要求fastai和pytorch

0:13:26.980,0:13:31.480
当我们训练时，使用fit, 训练多少个epochs(迭代次数）

0:13:31.500,0:13:34.740
不要对参数做backpropagate反向传递

0:13:34.760,0:13:39.200
不要做反向传递将梯度给到这些参数矩阵中做更新

0:13:39.320,0:13:45.960
换言之，当你用参数 = 参数 - 学习率 *梯度时

0:13:46.020,0:13:49.880
只是作用于新层的参数矩阵，而不作用于之前的所有参数矩阵

0:13:49.900,0:13:53.800
这就是freeze的含义，即不要更新参数

0:13:54.180,0:13:57.200
这样训练速度会更快

0:13:57.280,0:14:01.660
因为少了很多（参数更新的）运算

0:14:01.740,0:14:06.380
也会占用较少内存，因为需要存储的梯度也少了

0:14:06.440,0:14:16.300
最重要的是，我们不会修改那些层的参数矩阵值，因为这些值至少好过（nothing 随机值）

0:14:16.440,0:14:23.960
这就是freeze的工作原理，它不会封冻全部参数矩阵，会留下fastai为我们添加的2个层的参数矩阵来训练

0:14:24.320,0:14:26.320
那么下一步是什么

0:14:26.340,0:14:29.940
训练一段时间后，我们发现模型表现不错

0:14:30.140,0:14:33.640
我们可以应该是时候训练整个模型的参数了

0:14:33.980,0:14:36.620
所以，我们使用unfreeze解冻

0:14:37.380,0:14:42.600
那么，现在，我们要训练所有参数

0:14:42.820,0:14:50.020
但我们心里明白，这2个新增的参数矩阵，会需要更多的训练

0:14:50.100,0:14:54.060
而这些早期层的参数（可能是那些识别对角线边角diagonal edges的过滤器)

0:14:54.340,0:14:56.340
可能根本不需要训练

0:14:56.360,0:15:04.480
所以我们将模型分解成多个部分

0:15:04.480,0:15:12.800
让我们给模型的不同部位，不同的学习率

0:15:12.840,0:15:21.520
比如，我们可以给这部分模型一个学习率lr = 1e-5

0:15:21.580,0:15:25.240
这一部分模型，我们可以给一个学习率，

0:15:26.380,0:15:28.820
1e-3

0:15:28.900,0:15:34.780
接下来要发生的是，我们继续训练整个模型

0:15:34.800,0:15:38.840
因为前一部分的模型的学习率更小

0:15:38.860,0:15:43.020
所以参数更新幅度会更小，因为我们认为它们已经被训练得很好了

0:15:43.140,0:15:46.800
同时，如果它们已经相对于最优参数值更接近了

0:15:46.860,0:15:50.840
如果我们使用较大的学习率，参数会被踢出最优参数值附近，反而使模型变差

0:15:50.840,0:15:52.600
这是我们不愿意看到的

0:15:52.700,0:15:57.980
这个过程，我们叫做使用 discriminative learning rate判别学习率

0:15:59.540,0:16:01.540
你找网上找不到什么资料

0:16:03.100,0:16:05.500
因为我们是首批使用它的人

0:16:05.620,0:16:10.080
（至少是）按这种方法来使用，而且反复谈及这个概念

0:16:10.080,0:16:12.080
可能也有别人也使用这个方法，但是没有分享到网上

0:16:12.160,0:16:15.460
所以，你能在网上找的相关的内容，应该都来自fastai的学员

0:16:15.460,0:16:19.760
但是这个概念已经开始变得越来越受关注了

0:16:19.820,0:16:27.440
但是这是一个非常重要的技巧，如果迁移学习不使用这个技巧，我们无法获得如此优异的效果的

0:16:27.480,0:16:32.500
那么，我们如何在fastai中使用判别学习率呢discriminative learning rate？

0:16:32.600,0:16:41.980
在任意一个可以使用学习率的函数中，例如fit

0:16:42.100,0:16:45.660
首先给出的（函数输入值func args）epochs（迭代次数）

0:16:45.720,0:16:48.300
第二个（给出的函数输入值func args）就是学习率

0:16:48.300,0:16:50.260
这里是fit(1, ...) 训练1轮次

0:16:50.340,0:16:53.580
学习率，你有多种给出方式，可以是一个单一数值

0:16:53.700,0:16:56.420
比如1e-3

0:16:56.440,0:17:03.440
你可以用slice， 用slice(1e-3), 来面对单一数值的学习率

0:17:03.440,0:17:09.080
或者，我们可以用slice，并给它2个数值

0:17:13.400,0:17:15.400
这些不同写法分别代表什么？

0:17:15.460,0:17:24.060
第一种写法，只用一个数值，意思是所有层都用相同的学习率

0:17:24.140,0:17:26.140
所以，你并没有在使用判别学习率

0:17:27.080,0:17:32.020
如果你给到slice一个单一数值

0:17:32.140,0:17:36.720
意味着最后一层获得这个数值的学习率

0:17:38.160,0:17:41.120
这里所给出的数值是1e-3

0:17:41.220,0:17:47.820
然后所有其他层得到的学习率是，这个值/3

0:17:48.160,0:17:54.540
所以所有其他层的学习率是1e-3/3, 而最后一层的学习率是1e-3

0:17:55.260,0:17:57.260
最后一种情况

0:17:57.400,0:18:02.360
最后一层的学习率，仍旧是1e-3

0:18:02.620,0:18:07.240
第一层的学习率将是1e-5

0:18:07.340,0:18:13.840
其他层的学习率将得到这两个值之间均匀分布的值

0:18:13.860,0:18:24.100
具体而言，递增递减的乘数相同（multiplicatively equal），比如，总共有三层，那么学习率将是1e-5, 1e-4, 1e-3

0:18:24.280,0:18:26.880
所以，是每层都乘上相同的数（这里是乘上了10）

0:18:27.500,0:18:33.660
一个简单的技巧，让这个过程更简单

0:18:33.800,0:18:37.740
我们并不是给所有层一个不同的学习率

0:18:37.840,0:18:41.140
我们给不同layer_group 层组一个不同的学习率

0:18:41.240,0:18:45.280
我们决定将不同层组合在一起给到大家

0:18:45.360,0:18:49.480
具体而言，我们所做的是，将那些增加的含有随机值的层

0:18:49.620,0:18:51.900
叫做一个layer_group层组

0:18:52.100,0:18:54.480
这是默认值，你可以对其修改

0:18:54.580,0:18:58.800
剩余的层，我们将它们一分为二，变成两个层组

0:18:58.800,0:19:01.840
所以，默认状态下，至少对CNN而言，你将获得3个layer groups层组

0:19:01.940,0:19:05.560
所以，当你说slice(1e-5, 1e-3)

0:19:05.600,0:19:12.200
你的第一个层组的学习率是1e-5, 第二层组学习率是1e-4, 第三层组的学习率是1e-3

0:19:12.220,0:19:17.800
现在，如果你回头看看我们的训练，希望这次大家对训练中的学习率能有更多的理解

0:19:17.860,0:19:22.140
这个除以3，看起来有点怪

0:19:22.220,0:19:25.600
我们会在part2中解释

0:19:25.660,0:19:31.760
这是针对Batch Normalization 批量归一化的一个小技巧

0:19:31.820,0:19:35.180
我们可以在论坛的advanced topic栏目中讨论，如果有人感兴趣的话

0:19:36.060,0:19:39.620
好的，这是

0:19:41.700,0:19:47.000
微调，希望这些解读能让它不那么神秘

0:19:54.680,0:19:58.260
上周，我们学习的是协同过滤

0:19:58.360,0:20:01.920
在协同过滤的例子中

0:20:02.000,0:20:06.060
我们执行fit_one_cycle函数，给了它一个值作为学习率

0:20:06.080,0:20:13.920
这很容易理解，因为在协同过滤案例中，我们只有一个层

0:20:14.000,0:20:23.980
模型里有多个组成部分，但是没有所谓的矩阵相乘+激活函数+矩阵相乘（层结构）

0:20:24.040,0:20:28.120
我要介绍另一个专业术语

0:20:28.200,0:20:33.620
它们并不是真正意义上的矩阵相乘

0:20:33.780,0:20:45.420
它们其实很相似，并且是线性函数组合在一起，它被给予比矩阵相乘更广的内涵，叫做Affine functions 仿射函数

0:20:45.500,0:20:53.860
所以，当你听到我说Affine functions 仿射函数, 你可以在脑海里用matrix multiplication矩阵乘法代替

0:20:54.760,0:20:56.980
但当我们开始学习卷积神经网络CNN时

0:20:57.080,0:21:05.500
Convolutions 卷积就是矩阵乘法，其中的一些参数被束缚了(tied)，所以叫它们仿射函数更恰当

0:21:05.560,0:21:09.020
我喜欢每节课引入一些新的专业术语

0:21:09.100,0:21:18.260
所以，当你读书，啃论文，看课程视频，或看文档时，你会看懂更多相关专业词汇

0:21:18.360,0:21:22.660
所以，当你看到仿射函数时，其实就是线性函数

0:21:22.680,0:21:28.360
意味着与矩阵乘法非常相似的运算

0:21:28.400,0:21:32.960
而矩阵乘法则是最常见的仿射函数，至少在深度学习里是这样

0:21:33.660,0:21:39.680
好的，具体就协同过滤而言

0:21:40.500,0:21:43.540
我们所使用的模型

0:21:44.560,0:21:50.180
是这个，在这里我们有一系列参数

0:21:51.100,0:21:53.520
这里也有一系列参数

0:21:53.880,0:21:57.420
在这里生成两个数组的点积

0:21:57.560,0:22:07.960
因为这里是一行数据，这里是一列数据，我们也可以把点积当作是一种（单行单列的）矩阵乘法，用MMULT来计算

0:22:08.120,0:22:14.440
上周我们用Excel的solver来（计算梯度）训练模型

0:22:14.540,0:22:18.840
我们还没有回头看看训练结果，那么现在我们来看看训练得如何？

0:22:19.260,0:22:26.160
（我们看到最后的）RMSE（均方根误差）是下降到0.39

0:22:26.200,0:22:30.660
我们是尝试预测的值的范围是在0-5之间，

0:22:30.720,0:22:34.580
所以，平均来看，我们的误差在0.4，这是非常棒的表现

0:22:34.620,0:22:39.520
你可以看出模型效果不错，通过看这里

0:22:39.740,0:22:41.740
这里是3，5，1是目标值

0:22:41.800,0:22:46.360
这里是3.25， 5.10， 0.98，这些预测值是非常接近了

0:22:47.140,0:22:49.140
大概就是这个样子

0:22:49.720,0:22:54.720
然后，我开始讲到嵌入矩阵（embedding matrices)

0:22:54.800,0:23:00.680
为了更好的理解，让我们把这个worksheet放一边

0:23:00.820,0:23:02.960
看看另一个worksheet

0:23:04.280,0:23:06.280
这是另一个worksheet

0:23:06.400,0:23:14.060
这里我所做的，是将用户与电影的参数矩阵，粘贴到了这里

0:23:14.340,0:23:18.660
这个是用户的，这个是电影的

0:23:18.660,0:23:24.560
电影参数矩阵，我做了transpose转置矩阵处理，所以现在两个矩阵有相同的Dimensions维度

0:23:24.660,0:23:29.340
这里就有2个参数矩阵

0:23:29.380,0:23:33.800
它们初始值是随机的，我们用梯度下降来训练它们

0:23:33.940,0:23:39.920
在原始数据集中，用户和电影ID长得这个样子

0:23:39.980,0:23:49.800
为了更方便起见，我将这些ID转化成1-15的数字

0:23:49.820,0:24:00.300
对应原始数据的ID组合，我们有了新数字组合（它们是从开始1开始的相邻contiguous的数字），来对应电影评级

0:24:00.400,0:24:08.960
接下来，我要用这个数组来代替用户ID1

0:24:08.960,0:24:12.900
这个数组（的特点）是第一个值是1，其他都是0

0:24:12.900,0:24:21.680
用户ID2将用这个数组来代替：第一个值是0，第二个是1，其他都是0

0:24:21.760,0:24:30.460
电影ID14（这些都是电影ID14）也都被一个数组所代替

0:24:30.520,0:24:33.880
这个数组只有第十四个值是1，其他都是0

0:24:33.940,0:24:37.940
这些都叫做one-hot encoding（one-hot编码，或者一位有效编码）, 顺便说一下

0:24:39.980,0:24:46.440
这些不是神经网络的一部分，而是某种输入数据预处理

0:24:46.500,0:24:54.800
实际上是更换了新的输入值，这里是我的新生成的电影（ID）数据和新生成的用户（ID）数据

0:24:54.900,0:24:58.040
它们都是输送给神经网络的输入值

0:24:58.040,0:25:08.060
我接下来要做的是，要用这个输入矩阵，来做矩阵乘法，

0:25:08.160,0:25:12.040
对象是这个参数矩阵

0:25:12.120,0:25:18.940
这个矩阵乘法能成功，是因为这个参数矩阵有15行，而这个输入矩阵有15列

0:25:19.000,0:25:22.320
所以，我能让它们做矩阵乘法，因为它们维度对应上了

0:25:22.420,0:25:25.260
你可以在Excel里做矩阵乘法

0:25:25.320,0:25:27.920
需要用MMULT函数

0:25:27.960,0:25:34.460
使用时一定要小心，因为这是一个返回多个数值的Excel函数

0:25:34.500,0:25:39.780
所以，你不能用enter回车键来执行，而是要用ctrl + shift + enter来执行

0:25:39.900,0:25:43.520
意思是，这是一个数组函数，会返回一组值（不是单一值）

0:25:43.540,0:26:00.160
所以，这里（看到的）是数组相乘的结果（激活矩阵），由用户ID的输入矩阵，与用户参数矩阵，相乘得到的

0:26:00.220,0:26:04.820
这就是一个常见的神经网络层

0:26:04.900,0:26:08.480
无非就是一个矩阵乘法

0:26:08.580,0:26:10.580
我们能对电影做同样的事

0:26:10.580,0:26:14.720
这里看到的电影矩阵乘法的结果

0:26:15.580,0:26:18.860
要注意的是

0:26:21.140,0:26:26.480
这一行，是用户1的one-hot编码

0:26:27.000,0:26:29.140
而这些（新生成的）激活值（数组）

0:26:29.900,0:26:33.160
就是用户1（原本）的激活值（数组）

0:26:33.260,0:26:35.540
为什么呢？因为你试想一下

0:26:35.680,0:26:41.220
在one-hot 编码数组与某个矩阵（原本参数矩阵）之间的矩阵相乘

0:26:41.300,0:26:48.900
结果是找到这个参数矩阵的第n行，如果one-hot 编码数组的第n个值是1的话

0:26:49.760,0:26:53.720
这个能理解吗？我们在这里做的，其实是

0:26:55.420,0:26:58.160
用了一个矩阵乘法

0:26:58.200,0:27:01.300
创造这个激活值矩阵

0:27:01.300,0:27:03.220
但用了一个有趣的方法

0:27:03.260,0:27:06.740
也就是通过寻找收集某个特定矩阵的行数组汇集而成的矩阵

0:27:06.820,0:27:15.220
这一步完成之后，我们可以让这两个激活数组（一个用户，一个电影）做点积

0:27:15.420,0:27:20.220
然后，计算损失值

0:27:20.220,0:27:21.900
平方

0:27:21.980,0:27:24.400
然后取均值

0:27:24.520,0:27:27.680
各就各位，大家请看，这个数值0.39

0:27:31.880,0:27:33.880
与这个数值一摸一样

0:27:34.460,0:27:36.460
因为它们计算实质是一样的

0:27:37.480,0:27:39.740
所以，这个数值

0:27:40.220,0:27:43.220
可以说是在寻找

0:27:44.340,0:27:49.960
这个用户的嵌入数组

0:27:50.260,0:27:57.240
而这里是在做矩阵乘法，所以我们知道它们的实质是一样的

0:27:57.300,0:28:01.860
让我们从头回顾一遍，这是最后一个解说版本

0:28:02.240,0:28:05.060
这里是（用户和电影的）与之前相同的参数矩阵

0:28:05.260,0:28:07.480
完全一样，直接复制粘贴得来

0:28:08.040,0:28:11.340
这里是用户和电影的ID数值（1-15）

0:28:11.560,0:28:20.880
但这次我将它们按照表格形式展示出来，也是你会在输入矩阵里看到的数据一样

0:28:21.100,0:28:26.180
但这次我将（之前计算所得的）激活数组放在了旁边

0:28:26.440,0:28:32.620
这些数组（或矩阵，如果作为整体），是原先按照这里的矩阵乘法得来的

0:28:32.620,0:28:37.620
但这次，这些激活矩阵是通过Excel的OFFSET函数计算得来

0:28:37.620,0:28:39.460
其实就是array lookup (数组查找）

0:28:39.660,0:28:44.760
（第一个值/行）其实在说，找出这个嵌入矩阵的第一行值/数组值

0:28:45.220,0:28:48.040
所以说，这是用数组查找方式在生成这个激活矩阵

0:28:48.200,0:28:52.420
所以，这个lookup 版本的激活矩阵与这个矩阵乘法得到的激活矩阵，是一样的

0:28:52.460,0:28:56.500
但很明显，前者计算更省内存，速度更快

0:28:56.560,0:29:01.480
因为我们不必生成one-hot编码，也不必做矩阵乘法

0:29:01.580,0:29:05.920
因为矩阵乘法几乎都在做与0相乘的计算，所以完全是浪费时间

0:29:07.240,0:29:14.840
所以，换言之，乘上one-hot编码与数组查找的实质是一样的

0:29:15.640,0:29:19.340
所以，我们应该总是选择采用数组查找的方法

0:29:19.620,0:29:31.720
因此，我们有一个独特的关于one-hot编码乘法的说法：我想对one-hot编码做矩阵乘法，但却无需创建one-hot编码，

0:29:31.860,0:29:36.580
我只需输入一系列的整数，然后假装它们是one-hot编码

0:29:36.920,0:29:40.540
这就是所谓的n-embedding

0:29:40.900,0:29:49.460
你可能在很多地方都听到过嵌入embedding这个词，就好像某种神奇且高端的数学术语

0:29:49.740,0:29:54.440
但嵌入矩阵embeddings其实就是在数组中查找的意思

0:29:55.480,0:29:58.560
但有趣的是

0:29:58.580,0:30:07.080
在数组中查找（在数学层面上）与（和one-hot编码矩阵的）矩阵乘法的本质和结果相同

0:30:07.080,0:30:13.760
因此，嵌入矩阵与我们标准的神经网络工作原理很融洽

0:30:14.580,0:30:18.540
所以，突然间，我们似乎都有了一个新类别的层

0:30:18.820,0:30:21.620
这种层的工作原理是去数组/矩阵中查找

0:30:21.920,0:30:24.800
但我们没做什么特殊的

0:30:24.920,0:30:29.300
我们只是加上了这个计算偷懒路径，叫做n-embedding

0:30:29.440,0:30:34.640
比one-hot编码矩阵乘法要更快更省内存

0:30:35.020,0:30:41.580
所以，这个很重要，因为当你听到别人说嵌入embeddings

0:30:41.940,0:30:46.360
你需要在脑海里出现数组查找

0:30:46.740,0:30:52.780
我们知道它与one-hot编码的矩阵乘法的数学本质相同

0:30:53.340,0:31:03.480
值得注意的是，语言层面上有趣的点在于，当你做one-hot编码矩阵乘法时

0:31:03.660,0:31:19.080
你获得一个很好的特征，那就是，用户参数矩阵的行（例如第一行的数组）与根据用户整数ID（序号数字1）查找到的输入值数组是对应吻合的

0:31:19.080,0:31:30.860
换言之，最后得到是这个参数矩阵，其中某行参数，对应某个输入值（整数序号）

0:31:31.480,0:31:38.700
这其实很有趣，尤其有趣的是，如果我们回到更便捷理解的展示版本

0:31:39.740,0:31:48.800
因为，唯一能计算输出激活值的方法是对这两个输入数组做点积

0:31:49.160,0:31:51.860
这意味着

0:31:52.700,0:31:56.700
它们需要有（内容本质上的）对应

0:31:56.700,0:32:05.620
需要有某种方式来表达，如果这个用户（参数数组）的这个值（第一个）比较高，并且这个电影（参数数组）的这个值（第一个）比较高，

0:32:05.620,0:32:08.200
那么这个用户就会喜欢这个电影

0:32:08.340,0:32:16.080
唯一能解释这一关系现象的说法，就是这个用户的参数数组本质上代表这用户的个性特征

0:32:16.440,0:32:24.060
这个则对应的是这个电影的相关特征，比如（假设）这个电影有John Travolta(第一个值比较大1.49）

0:32:24.300,0:32:30.280
而（假设）这个用户（第一个参数值1.55）有喜欢John Travolta的特征

0:32:30.340,0:32:32.740
那么，你就会喜欢这个电影（输出激活值）

0:32:33.180,0:32:38.980
我们并没有人为决定参数矩阵的某一行的内涵意义

0:32:38.980,0:32:41.560
我们也没有做什么让每个参数数组产生什么具体含义

0:32:41.580,0:32:45.540
唯一能让梯度下降生成出好的结果

0:32:45.640,0:32:54.160
就是（模型参数矩阵）能够找到用户的电影口味特征，以及对应的电影特征

0:32:54.200,0:33:00.840
这些隐藏在下面的特征，叫做latent factors潜在因子

0:33:01.440,0:33:05.340
或者叫latent features 它们是隐藏的特征，但一直都在这里（数据中）

0:33:05.440,0:33:08.680
一旦我们训练模型参数，这些特征突然就出现了

0:33:08.800,0:33:10.800
但这里有一个问题

0:33:11.560,0:33:14.180
没人会喜欢Battlefield Earth这部电影

0:33:14.340,0:33:18.100
这不是部好电影，即便John Travolta在里面

0:33:18.780,0:33:21.980
那么我们怎么应对这个问题

0:33:21.980,0:33:30.160
目前（参数矩阵乘法的）的逻辑是，（这个特征/参数值说）你喜欢John Travolta, （这个特征值/参数值说）这个电影有John Travolta，所以（这个激活值说）你喜欢这部电影

0:33:30.380,0:33:36.220
但你需要补充一句，除了Battlefield Earth这部电影，或者是除非你是scientologist 科学教（山达基教）的教徒

0:33:36.300,0:33:41.680
那么，我们该怎么做？我们需要（在参数数组中）加入bias偏置/差

0:33:41.860,0:33:57.740
所以，这里又是相同的模式，相同的结构，只是在原来参数矩阵里增加了一行参数

0:33:58.080,0:34:05.420
所以，现在这个值（输出激活值）不再（单单）是它们（这个用户参数数组和这个电影参数数组）的点积

0:34:05.640,0:34:11.060
而是同时还有加上这个数值(偏差）和这个数值（偏差）

0:34:11.340,0:34:18.580
这意味着每一个电影，都可以有一个（广义/整体性）的评价：这是一个好看的电影或这不是一个好看的电影

0:34:18.760,0:34:25.320
每个用户同样也拥有了一个广义/整体性的评价：这个用户通常给这个电影高评价或者通常给这个电影低评分

0:34:25.980,0:34:28.600
这些值叫偏差/偏置

0:34:28.940,0:34:32.760
希望这些看起来很熟悉，这是相同的常规的

0:34:33.020,0:34:39.600
线性模型概念， 或者是线性层概念（来自神经网络）：包含矩阵乘法和偏差

0:34:39.740,0:34:43.140
还记得第二课中的SGD notebook中

0:34:43.380,0:34:49.720
你实际上不需要偏差，你可以添加一列数值，全部是1，给到输入数据

0:34:49.780,0:34:52.100
这样就有了我们的偏差值

0:34:52.100,0:34:54.100
但这种做法很低效

0:34:54.260,0:34:59.840
所以所有深度学习library库都提供偏差的概念

0:34:59.960,0:35:02.380
我们并不手动添加一列值为1 的数

0:35:02.380,0:35:04.340
那么（偏差）是如何设置的呢？

0:35:04.460,0:35:11.000
就在今天开始讲课之前，我让Excel data solver跑这个workbook

0:35:11.240,0:35:13.480
还真有这个worksheet

0:35:13.580,0:35:17.680
这里的RMSE（均方根误差）值是0.32

0:35:18.060,0:35:23.540
对比没有偏差的版本，损失值是0.39（比较高）

0:35:23.560,0:35:26.980
你可以看到

0:35:27.300,0:35:31.180
这个模型更好一点，生成的结果也更好一点

0:35:31.280,0:35:34.420
更好的原因是，（偏差）给予模型更高的灵活度

0:35:34.700,0:35:38.960
同时，语言层面上也符合逻辑

0:35:39.220,0:35:45.440
你（的模型）需要具备如下的判别力：我（某个用户）喜欢这个电影，不仅仅是基于一个组合综合效果（什么演员

0:35:45.500,0:35:50.260
，对话驱动风格，多少动作片段）而且要参考的（电影本身）的另一个重要因素是：这个电影是不是好电影

0:35:50.420,0:35:54.000
或者还有另一个因素，我（用户）是否是一个倾向给（所有）电影打高分的人

0:35:54.140,0:36:03.540
这里就是全部关于协同过滤的相关细节

0:36:04.100,0:36:06.800
怎么样，是否有任何问题？

0:36:06.840,0:36:10.140
我们有三个问题

0:36:10.400,0:36:14.980
第一个问题是

0:36:15.440,0:36:17.920
当我们加载一个预训练好的模型

0:36:18.420,0:36:23.220
我们是否能探索激活层，来了解模型擅长识别怎样的物体

0:36:24.220,0:36:26.220
是的，你可以

0:36:26.260,0:36:29.560
我们将在下节课中学习如何探索

0:36:30.580,0:36:37.580
能否解释一下fit_one_cycle函数的第一个函数参数是什么？

0:36:37.600,0:36:39.740
是不是就是epochs(迭代次数）？

0:36:39.800,0:36:46.440
是的，fit_one_cycle或fit的第一个函数参数就是epochs迭代次数的数量

0:36:46.540,0:37:00.640
换言之，an epoch 迭代次数是指（模型）将所有输入值看一遍，所以，如果（模型）做10遍epochs, （模型）将输入值看10遍

0:37:00.760,0:37:03.620
所以，有可能模型会过拟合

0:37:03.720,0:37:06.100
如果模型有很多参数且学习率很高

0:37:06.760,0:37:08.760
如果只做一个epoch迭代次数

0:37:09.220,0:37:11.220
不太可能过拟合

0:37:11.440,0:37:15.480
所以，记住做了多少个epochs是有用的

0:37:18.860,0:37:24.180
（提问2）什么是仿射函数？

0:37:24.600,0:37:28.540
仿射函数，就是一个线性函数

0:37:28.740,0:37:32.800
不确定是否有必要给出比这更详细的讲解

0:37:32.900,0:37:38.240
如果你先做乘法再做加法，这就是仿射函数

0:37:38.560,0:37:47.080
我不会去理会它的精准数学定义，一方面因为我不擅长数学，另一方面，因为这些定义并不重要

0:37:47.280,0:37:50.740
如果你就记住，仿射函数就是先做乘法再做加法

0:37:51.080,0:37:54.080
这就是最重要的事，这就是线性

0:37:54.080,0:37:59.380
所以，当你将一个仿射函数套用在另一个仿射函数上，得到的仍旧是一个仿射函数

0:37:59.420,0:38:01.720
这么叠加仿射函数，你没有“赢得”任何东西（你没有获得任何新东西）

0:38:01.720,0:38:03.140
绝对的浪费时间

0:38:03.200,0:38:05.660
所以你需要做“三明治”式的处理

0:38:06.300,0:38:09.600
加入任何非线性函数就能产生“新东西”

0:38:09.720,0:38:12.460
包括用0取代负数的函数，也就是ReLU

0:38:12.620,0:38:18.680
如果你这么叠加：仿射函数，ReLU，仿射函数，ReLU，仿射函数，ReLU，你就有了深度神经网络

0:38:25.280,0:38:29.640
让我们回到协同过滤的notebook

0:38:29.940,0:38:34.960
这次，我们要用Movielens 100k数据集

0:38:35.000,0:38:38.760
当然，还有2000万个影评的数据集版本

0:38:39.140,0:38:45.180
所以，这是非常棒的数据集项目，有grouplens团队建设并提供

0:38:45.420,0:38:49.580
它们定期会更新这个数据集

0:38:49.580,0:38:57.560
但他们也提供原始数据集版本，我们这里用原始版本，因为方便对比模型基准表现

0:38:57.700,0:39:02.780
因为所有人都会告诉你，如果你要对比基准表现，你应该用相同的原始数据

0:39:02.780,0:39:04.620
这是你需要用的数据集

0:39:04.700,0:39:08.420
不幸的事，我们不得不局限在1998之前的电影

0:39:08.500,0:39:12.080
所以，你可能没看过这里面的一些电影

0:39:12.200,0:39:14.200
但这是我们要付出的代价

0:39:14.300,0:39:22.520
你可以用更新的数据来代替，如果你想要尝试和实验更新的电影数据

0:39:23.400,0:39:32.080
更新的电影数据是CSV格式，特别方便使用

0:39:32.120,0:39:35.020
原始版本稍微比较麻烦

0:39:35.100,0:39:38.120
首先，这个数据的delimiter分隔符不是“，”而是“tab"

0:39:38.200,0:39:41.300
因此，在pandas里说用的是tab，再提出数据

0:39:41.620,0:39:45.620
其次，这个CSV数据不提供header每列数据的标题

0:39:45.680,0:39:53.260
所以，你要告诉pandas没有标题行；因为没有标题行，你要告诉pandas每一列的标题名

0:39:53.360,0:39:55.660
除此之外，无需做别的

0:39:56.240,0:40:00.360
我们可以看看head(), 展示给我们前几行数据

0:40:00.400,0:40:04.840
这里可以看到用户ID，电影ID，评级

0:40:04.920,0:40:08.600
做点有趣的事，让我们看看有哪些电影

0:40:08.760,0:40:13.180
我需要指出的是

0:40:13.220,0:40:20.420
这里有一个叫encoding=的代码，如果删除，然后就有这样的报错

0:40:20.580,0:40:25.160
我就想指出来，因为你的代码人生的某个时候你肯定会遇到它

0:40:25.600,0:40:31.020
codec can't decode ...这句话的意思是，这个CSV文档不是unicode

0:40:31.080,0:40:35.260
这个问题常出现于较早期的数据文档

0:40:36.500,0:40:46.160
之前，西方程序设计人士没有关注到其他非英语数据文本的。

0:40:46.520,0:40:54.380
今天，我们要更擅长处理不同的语言，我们使用一个标准叫做unicode

0:40:54.500,0:40:58.720
有方便的是，python 将unicode作为默认设置

0:40:58.740,0:41:01.680
但当你尝试加载一个非unicode文档时

0:41:01.740,0:41:04.140
你事实上需要猜测

0:41:04.140,0:41:06.740
这个文档是如何编码的

0:41:06.860,0:41:16.600
因为极可能是被某些欧美编程人士创建

0:41:16.720,0:41:19.640
他们几乎肯定使用latin-1

0:41:19.740,0:41:25.460
所以，在这里加入encoding='latin-1', 或者使用python中的file.open

0:41:25.500,0:41:31.080
或者pandas.open，等等，这基本上能解决这个问题

0:41:31.100,0:41:37.360
同样的，这里没有提供标题行，需要在这里提供每一列的标题

0:41:37.500,0:41:42.840
有趣的是，数据集为每一个电影主题提供了单独一列数据

0:41:42.840,0:41:45.120
这里有19种题材

0:41:45.280,0:41:49.480
它们看上去像是one-hot编码，但其实是n-hot编码

0:41:49.560,0:41:52.600
也就是一个电影可以有不同的题材

0:41:52.720,0:42:00.820
我们不会使用题材数据，只是因为这种数据展示形式（n-hot 编码）来储存题材信息很有趣，所以在这里指出来

0:42:00.940,0:42:05.680
在更新的版本中，数据集直接将题材信息展示出来，变得更加方便

0:42:06.200,0:42:15.860
好的，我们这里有10万个影评数据。 我发现如果对数据做去归一化，可免去很多麻烦

0:42:15.980,0:42:18.780
我想让电影名称与电影评级在一起

0:42:18.780,0:42:20.640
所以可以用到pandas.merge函数

0:42:20.780,0:42:25.360
来完成合并。这样一来， 我们有评级在这里，这里是电影名称

0:42:26.020,0:42:35.900
接下来，常规操作，生成DataBunch, 为了协同过滤应用设计的DataBunch, 可以用CollabDataBunch.from_df, 这里是我们的dataframe

0:42:36.440,0:42:40.060
分割验证集

0:42:40.540,0:42:48.840
我们应该用他们的方式来做验证集分割和交叉验证，如果我们想和基准效果做公平合理的对比

0:42:49.620,0:43:01.220
在协同过滤的DataBunch数据堆的默认设置里，第一列是用户userId，第二列是电影movieId，第三列是评级rating

0:43:01.320,0:43:07.800
现在我们要用电影名称作为item_name, 所以我们在这里设置成item_name=title

0:43:07.900,0:43:14.780
所以的数据堆都支持.show_batch函数，可以用来展示数据里面有什么

0:43:14.900,0:43:22.500
我会尝试获取更高的模型表现

0:43:22.620,0:43:27.920
我会尝试各种技巧来提升效果

0:43:27.940,0:43:32.300
其中一个技巧是使用y_range

0:43:32.420,0:43:39.160
y_range让最后一个激活函数成为S函数

0:43:39.360,0:43:45.720
具体而言，上周我们说让我们用一个S函数输出值在0到5之间

0:43:45.800,0:43:52.280
这将确保神经网络模型的预测值在合适的区间

0:43:52.340,0:43:58.220
我并没有在Excel版本中这么做

0:43:58.320,0:44:00.600
所以你可以看见我有负值

0:44:00.880,0:44:02.880
以及大于5的值

0:44:02.960,0:44:04.960
所以，如果你要在Excel里打败我的成绩

0:44:05.020,0:44:11.900
你可以在Excel中加入S函数后再训练，你会有略微更好的成绩

0:44:12.020,0:44:21.900
但问题是，S函数是渐进的，不论其最大值是多少，这里的设置为5

0:44:22.020,0:44:24.740
这意味着你无法预测出5这个值

0:44:24.840,0:44:27.620
但不少电影都有5这个评级，所以这是一个问题

0:44:27.860,0:44:34.660
所以，更好的做法是将y_range的最大值设置的更大一点

0:44:34.760,0:44:42.040
这里的y_range 最小值是0， 最大值是5.5， 所以这个区域比[,0,5] 更大一点

0:44:42.200,0:44:49.220
这是一个获取更多精度的方法技巧

0:44:49.380,0:44:52.700
另一个我用的方法是使用一个叫weight decay权值衰减的技术

0:44:52.880,0:44:57.120
这是我们下一个用学习的

0:44:57.340,0:45:05.340
然后，我们需要多少个factors. 什么是factors?

0:45:05.360,0:45:09.980
factors的数量等于嵌入矩阵的宽度width

0:45:10.020,0:45:15.520
为什么我们不用embedding_size而非n_factors, 也许我们应该这么改，但是

0:45:15.580,0:45:22.620
在协同过滤的世界里，人民不用这个词，而是用factors，因为latent factors潜在因子的原因

0:45:22.700,0:45:29.260
因为标准解决协同过滤的方法一直以来都叫做matrix factorization 矩阵分解

0:45:29.320,0:45:33.500
事实上，我们所见的

0:45:33.500,0:45:35.880
恰好是一种做矩阵分解的方法

0:45:35.900,0:45:40.200
所以，我们实际上今天碰巧学会了如何做矩阵分解

0:45:40.300,0:45:45.060
所以，这个名称是专门针对协同过滤这个领域的

0:45:45.120,0:45:49.480
但你可以把它理解为嵌入矩阵的宽度

0:45:49.560,0:45:51.640
为什么设置为40？

0:45:51.640,0:45:55.580
这是模型结构设计层面的决定，你只能不断尝试和实验看看什么（数值）效果更好

0:45:55.620,0:45:59.860
我尝试了10，20，30，40，发现40通常表现更好

0:45:59.880,0:46:07.600
而且训练很快。所以你可以把它们放入一个for loop循环中，看看（刚才这些数值）哪个效果更好

0:46:07.680,0:46:12.460
对于学习率，这是我们常见的lr_finder

0:46:12.460,0:46:18.200
5e-3看起来效果不错

0:46:18.200,0:46:24.220
记住，这个值只是rule of thumb 经验法则，5e-3比Sylvian 和我的方法提取的值都更小

0:46:24.280,0:46:31.600
Sylvian的方法是，先找到最低点，再缩小10倍，也就是2e-2, 我觉得

0:46:31.780,0:46:39.600
我的方法是找到最陡峭的一段，也就是这里，通常和Sylvain的结果相同，所以也是差不多2e-2

0:46:39.620,0:46:44.820
我试过了，我喜欢尝试，小十倍，大10倍

0:46:44.880,0:46:48.080
就是为了确认，但实际上我发现（比我们的方法产生的值）更小一点（5e-3)，效果更好

0:46:48.100,0:46:55.800
所以，对于问题：我应该尝试。。。吗？回答总是：试试看

0:46:55.980,0:46:59.540
这会让你成为优秀深度学习从业者

0:46:59.580,0:47:02.940
这样模型效果到了0.013（损失值）

0:47:03.000,0:47:08.740
同样，你可以保存模型，把你就是30多秒（不必重复劳动）

0:47:08.780,0:47:16.760
这里有一个library叫做librec, 在这里会发布

0:47:16.820,0:47:20.600
Movielens100K的基准成绩

0:47:20.660,0:47:27.100
这里有一个RMSE区域（3列），大概0.91似乎是他们找到的最后成绩

0:47:27.160,0:47:35.280
0.91是RMSE均方根误差，我们用的是MSE，没有平方根，所以（0.91）**2得到0.83

0:47:35.340,0:47:46.040
我们的成绩是0.81，这已经很棒了，毕竟我们的这个模型很简单，实际上好很多

0:47:46.100,0:47:53.440
尽管如此，我们依旧应该抱着怀疑的态度，毕竟我们没用一样验证分割和交叉验证

0:47:53.480,0:47:59.080
但至少我们的模型是非常有竞争力的，与他们的模型相比

0:47:59.120,0:48:08.880
一会，我们将查看这个模型的python的源代码

0:48:08.920,0:48:17.840
但现在，请相信我，我们会看到的代码将在做这些事情

0:48:17.900,0:48:26.000
在数组中查找，在做矩阵乘法，再做加法，再做均方误差的损失函数计算

0:48:26.060,0:48:41.320
基于我们知道唯一有效/有趣的方法，是找到这些潜在因子，而且去看看这些潜在因子所找到的特征是什么，也是理所当然的事情

0:48:41.320,0:48:50.720
尤其是，除了潜在因子，我们还有偏差因子给予到每一个用户和电影

0:48:50.800,0:49:03.620
你可以说，让我们看看每个电影的平均评级，但这有些问题，尤其是动画片这类电影

0:49:03.660,0:49:11.440
喜欢动画片的人，就是特别喜欢，所以他们看很多动画片，并且都给较高的评分

0:49:11.660,0:49:22.260
所以，很多时候，电影评分的图表的前几位都是动画片，尤其是一些有几百集的动画片

0:49:22.260,0:49:27.880
你会发现每一集都出现在这前1000个电影的列表中

0:49:27.920,0:49:31.260
我们该如何应对这种情况呢？

0:49:31.440,0:49:36.700
庆幸的是，当我们注意到电影偏差时

0:49:36.760,0:49:41.920
电影偏差会说，一旦我们加入了用户偏差

0:49:41.940,0:49:47.380
对于动画片用户而言，这些偏差会很高，因为他们（习惯）会给高分

0:49:47.440,0:49:53.600
一旦当我们综合了这些其他特征（喜欢动画片的用户的其他特征）

0:49:53.640,0:49:59.220
这个电影所剩下的特征，就是关于这个电影本身的特征（与动画片无关）

0:49:59.260,0:50:03.240
所以，查看电影的偏差，会更有趣

0:50:03.260,0:50:05.900
而不是（简单的问）最受欢迎的电影是什么

0:50:05.980,0:50:17.520
人们喜欢怎样的电影，即便是对于那些不喜欢打高分的用户，或者是对于那些不具备某些特征的电影

0:50:17.580,0:50:26.280
这样一来，通过使用偏差参数，我们就有了一个"unbiased"无偏差/偏见的电影评分（虽然我在这点上本身就可能biased)

0:50:26.300,0:50:29.020
我们怎么做呢（在Notebook中）

0:50:29.240,0:50:32.660
为了更有意思/趣

0:50:32.760,0:50:38.500
部分原因是这个数据截止到1998

0:50:38.580,0:50:42.760
让我们看看那些很多人观看的电影

0:50:42.760,0:50:46.680
我们用pandas来提取rating表格

0:50:46.720,0:50:52.320
groupby(title/标题这一列数据)，然后数一数每一个电影获得多少个用户的评分

0:50:52.420,0:50:56.080
我们不是查找最高评分，而是计数有多少（用户）给评分

0:50:56.160,0:51:04.900
所以，前1000个电影是获得最多用户给评分的电影

0:51:04.940,0:51:07.760
所以，里面有可能有我们看过的电影

0:51:07.820,0:51:09.820
这是我这么做的唯一原因

0:51:09.880,0:51:12.340
我称之为top_movies

0:51:12.340,0:51:15.600
不是指好电影，而是我们极有可能看过的电影

0:51:15.700,0:51:23.020
不出意外，这里有Star Wars星球大战，是这个数据集中被最多用户打分的电影

0:51:23.220,0:51:29.200
还有《独立日》

0:51:29.320,0:51:35.620
我们可以用训练过的学习器

0:51:35.640,0:51:44.340
让它给我们bias偏差，给我们top_movies中的items/电影的偏差

0:51:44.380,0:51:51.520
is_item = True 的意思是我们要电影的bias, False则是要用户的bias

0:51:51.540,0:52:07.000
这里是非常常见的协同过滤中的专业术语nomenclature, 这些ID通常叫做users用户，这些ID通常叫做items物品/电影

0:52:07.040,0:52:11.420
哪怕你的问题不涉及任何用户和物品

0:52:11.460,0:52:15.100
我们这么起名，主要为了方便

0:52:15.120,0:52:17.780
它们就只名称而已

0:52:17.820,0:52:22.420
这里，我们需要items， 这里是我们的items列表

0:52:22.420,0:52:25.560
我们需要bias, 这是协同过滤专有的提取bias的函数

0:52:25.600,0:52:28.500
这就能返回给我们1000个值

0:52:28.560,0:52:31.820
因为我们用1000个电影作为我们的items

0:52:31.980,0:52:53.760
为了比较，我们用groupby(title)['rating'].mean()来计算每个电影的平均分，然后用zip将每个电影的名称，偏差和均分并列摆放在一起

0:52:53.840,0:53:03.800
然后，我们可以用(0 index 的lambda函数, 输出结果也就是)bias偏差的大小对电影从新排序

0:53:03.880,0:53:08.200
这里看到的是根据最小的值（偏差值）的电影排序

0:53:08.400,0:53:14.620
我们可以说，Mortal Kombat: Annihilation不是一部好电影

0:53:14.760,0:53:16.920
Lawmower man 2 不是一个好电影

0:53:17.000,0:53:23.580
我没有看过Children of the Corn, 但我们在SF学习小组里做了很久的讨论，看过的人都同意这不是一部好电影

0:53:23.660,0:53:31.360
你会发现其中的一些电影实际有不错的评分

0:53:31.420,0:53:36.020
比如，这个电影有2+的评分

0:53:36.020,0:53:38.840
这个的评分（居然）比下一个更高

0:53:38.920,0:53:49.660
这可以理解为，根据这部电影的演员，题材，观看者的喜好，你会预期（矩阵乘法）它的评分会更高

0:53:49.700,0:53:53.900
这里是从大到小（电影bias)的排序

0:53:54.040,0:53:58.260
Schindler's list, Titanic, Shawshank Redemption, 看起来很合理

0:53:58.440,0:54:06.120
你可以找到一些电影，比如这个，评分不是特别高，但bias值很高

0:54:06.160,0:54:11.700
（可能是）在1998年，人们还不是特别喜欢Leonardo DiCaprio

0:54:11.740,0:54:15.820
或者人们还不是特别喜爱对话多的电影

0:54:15.840,0:54:18.840
或者人们不是那么喜欢爱情剧，等等

0:54:18.860,0:54:20.860
但整体上人们对它的喜爱程度超出预期

0:54:20.920,0:54:27.060
所以，以这种方式来解读模型是很有意思的

0:54:27.100,0:54:32.480
我们可以再进一步，这次调取的不是参数偏差，而是参数权重

0:54:32.580,0:54:37.280
也就是这些数值

0:54:37.340,0:54:41.620
同样的，我们调取电影的权重

0:54:41.660,0:54:44.240
针对的依旧是top_movies

0:54:44.300,0:54:47.960
得到的矩阵是（1000， 40），因为我们设置的n_factors是40

0:54:48.020,0:54:52.420
所以，不再是5个列/参数值，而是40个参数值（来描述一个电影的具体特征）

0:54:52.560,0:55:04.360
通常，从我们能理解层面上，并不没有所谓的40个潜在因子，对应不同的电影口味

0:55:04.460,0:55:10.440
所以，查看每一个潜在因子，是无法对它们能有什么直观的理解的

0:55:10.460,0:55:16.060
我们要做的是，将这40个潜在因子，压缩到只有3个

0:55:16.120,0:55:21.360
有一个技术我们不会深入了解，它叫PCA (Principal Component Analysis) 主成分分析

0:55:21.440,0:55:25.900
movie_w 是一个torch 张量

0:55:25.940,0:55:30.980
fastai为torch 张量增加了这个pca函数

0:55:31.000,0:55:37.080
pca的工作是，作为一个线性变形函数

0:55:37.100,0:55:47.540
吃进一个输入值矩阵，尝试从中寻找少量的列，但能覆盖这个输入矩阵尽可能多的空间

0:55:47.580,0:55:54.980
如果你觉得pca很有趣，（它的确很有趣），那么你应该试试我们的computational linear algebra的课程

0:55:55.060,0:56:04.780
Rachel教授这门课，我们会教你从头开始计算PCA，以及为什么要用它，等等

0:56:04.820,0:56:10.060
当然，这些都不是学习本课的前提

0:56:10.220,0:56:19.440
但很有必要的知道的是，将神经网络的层，放入PCA做压缩，通常都是个好主意

0:56:19.520,0:56:23.180
因为，通常，你有过多的激活值在每个层结构里

0:56:23.240,0:56:26.620
而且你会有各种理由需要实验这些层中的参数和激活值

0:56:26.720,0:56:38.900
比如，Francisco（和我一起，坐在那边）一直做的项目是图片相似度

0:56:39.020,0:56:46.140
对于图片相似度，一个很好的方法是，对比模型的激活值，但多数情况下，激活矩阵都非常大

0:56:46.140,0:56:53.540
因此跑起来会非常慢和困难，因此人们在做图片相似度比较时，会用PCA来压缩

0:56:53.620,0:56:57.240
这个方法地区很酷

0:56:57.280,0:57:03.720
这里，我们也直接这么来操作，将40个因子压缩到了3个因子，希望这样能让我们更容易去解释理解潜在因子

0:57:03.820,0:57:09.680
我们可以这样提取潜在因子，叫它们fac0, fac1, fac2

0:57:09.940,0:57:16.560
将fac0与top_movies合并在一起，然后再重新排序

0:57:16.620,0:57:20.900
但我们不知道，这样排序出来的结果代表什么。

0:57:20.980,0:57:27.840
但我们相信这会是某些风格/口味的电影特征

0:57:27.900,0:57:31.560
如果我们将头部和尾部的数据打印出来

0:57:31.620,0:57:42.700
我们可以看到头部的电影的特征，你可以将其描述成，鉴赏级别/专业类电影，我觉得

0:57:42.780,0:57:47.660
比如，Chinatown 非常经典的Jack Nickolson的电影

0:57:47.740,0:57:56.380
所有人都知道Casablanca, 甚至wrong Trousers也是经典的泥塑动画电影，等等

0:57:56.400,0:58:02.660
这些电影，都是那类会被判定为经典级别的电影

0:58:02.660,0:58:08.380
很可能（尾部电影）Home Alone 3可能不是特别经典级别的电影

0:58:08.440,0:58:17.500
不是说人们不喜欢这类电影（尾部电影），但喜欢它们的观众与那些喜欢（头部）Secrets & Lies的观众非常不同

0:58:17.560,0:58:25.240
所以，（这个潜在因子）似乎发现了某个电影特征对应上了人们对于电影的某种喜好

0:58:25.440,0:58:27.440
我们来看看另一个特征（潜在因子fac1）

0:58:27.500,0:58:37.500
这次我们似乎发现了，你看这些电影，都是你能和家人看的点击率很高的电影

0:58:37.560,0:58:42.040
这些很明显则不是

0:58:42.120,0:58:46.260
Trainspotting, 是那种非常贪婪类的

0:58:46.340,0:58:51.520
这个潜在因子找到了一个有趣的特征/口味

0:58:51.520,0:58:58.400
我们甚至可以将这些因子在图上画出来，我给予电影随机颜色，看起来更方便

0:58:58.400,0:59:05.040
这里是最受欢迎的前50部电影

0:59:05.080,0:59:08.360
根据电影被给予评分的次数

0:59:08.420,0:59:12.520
你可以看到terminator这部电影位置很高（Y=fac2的值很高)

0:59:12.560,0:59:15.660
而English Patients 和 Chindler's list在另一端的位置很高（X = fac0的值很高）

0:59:15.760,0:59:22.820
Godfather 和 Monty python在这一端（fac0很高，fac2较低）， 而Independent day和liar liar 则是fac2一半高，但fac0很低）

0:59:22.820,0:59:26.120
这会非常有趣，如果

0:59:26.180,0:59:34.900
你们可以在工作的数据中做类似的实验，或者是你能从其他数据里提取一些有趣的特征来做实验

0:59:40.320,0:59:45.880
好了，怎么样，有什么问题吗？

0:59:46.040,0:59:48.660
有一个，好的

0:59:48.760,0:59:56.700
（提问）为什么有时我会有负损失值，在训练时？

0:59:57.120,0:59:59.120
你不应该有（负损失值）

0:59:59.260,1:00:04.120
所以，肯定什么地方出错了

1:00:04.300,1:00:13.620
尤其是大家持续点赞这个问题，所以其他人肯定也遇到了这个问题，请把这个问题提在论坛里，

1:00:13.720,1:00:24.100
他们说用的是negative log likelihood 负对数似然，我们在休息之后会学到cross-entropy 交叉熵和negative log likelihood 负对数似然

1:00:24.120,1:00:30.460
它们都是损失函数，对你给它们的输入值有具体的要求

1:00:30.500,1:00:33.860
如果你的输入值不符合要求，它们会返回给你奇怪的结果

1:00:33.900,1:00:39.360
所以，你“按下了错误的按钮”， 不要这么做

1:00:41.200,1:00:52.520
我们讲过collab_learner

1:00:52.560,1:00:58.460
这里是collab_learner函数定义

1:00:58.540,1:01:11.140
这个函数，和很多其他函数一样，需要吃进数据堆

1:01:11.260,1:01:17.360
通常learner还会要求你给它们一个具体的模型结构

1:01:17.360,1:01:23.640
这里只有一个东西能做到这一点，这就是use_nn，问你是要用多层神经网络还是传统的协同过滤

1:01:23.700,1:01:26.880
我们今天只会用到协同过滤

1:01:26.960,1:01:29.580
可能会稍稍看看另一个

1:01:29.800,1:01:33.540
所以，这里实际发生的是，

1:01:33.540,1:01:39.740
基本上，我们在这里要构建一个EmbeddingDotBias模型设计

1:01:39.800,1:01:46.980
然后给到CollabLearner, 这里也有数据，所以实际上所有有趣的事情都发生在EmbeddingDotBias里面

1:01:47.040,1:01:49.780
所以，让我们继续深入看看它

1:01:49.800,1:01:54.280
我按错了键，（先找到）EmbeddingDotBias, （按下ctrl + ], 我猜测)， 好了

1:01:54.380,1:02:02.480
这里是我们的EmbeddingDotBias模型

1:02:02.540,1:02:08.200
它是一个nn.Module

1:02:08.320,1:02:16.960
回顾一下，所以的pytorch层和模型都是nn.Module

1:02:17.040,1:02:25.060
一旦你创建了它们，它们看起来就像是一个函数，你是用（）来执行它们并给它们函数参数

1:02:25.120,1:02:27.920
但它们不是函数

1:02:28.000,1:02:39.380
通常在python里，让一个东西长得像函数，你需要给它一个method叫dunder call, 即__call__()

1:02:39.500,1:02:46.600
但这里并没有，原因是pytorch期望你使用forward(不而是__call__)

1:02:46.620,1:02:51.100
这是pytorch将为你执行的method（forward())，当你要求执行(EmbeddingDotBias())时

1:02:51.280,1:02:58.200
所以，当这个模型被训练后，然后要求做预测，就会执行forward这个method

1:02:58.440,1:03:08.040
这是我们计算的代码，来计算预测值

1:03:08.080,1:03:15.640
这里你会看到，为什么这里是users而不是user?

1:03:15.760,1:03:18.260
因为所有计算都是用的min-batch

1:03:18.320,1:03:31.680
但我读forward代码时，在一个pytorch module里，我倾向与忽视小批量 mini-batch，假装只用了一个数据样本

1:03:31.840,1:03:38.420
因为pytorch自动进行所有批量数据要接受的处理

1:03:38.460,1:03:41.900
所以，让我们假装这里只有一个数据样本

1:03:41.940,1:03:47.220
我们拿着这个用户数据，给到self.u_weight(),这是什么呢？

1:03:47.220,1:03:51.460
self.u_weight是一个embedding嵌入数组

1:03:51.600,1:04:01.060
我们基于以下内容分别创造一个embedding，（n_users, n_factors）为一个内容（构建一个embedding), (n_items, n_factors)为第二个内容，(n_users, 1)为第三个内容， （n_items, 1）为第四个内容

1:04:01.080,1:04:12.180
这很合理，n_users by 1 指的是这里用户的偏差

1:04:12.240,1:04:16.740
n_users by n_factors指的是这一块参数数组

1:04:16.900,1:04:25.120
（n_users, n_factors)构建的embedding进入到u_weight里

1:04:25.240,1:04:30.720
（n_users, 1) 构建的embedding给入到u_bias

1:04:30.740,1:04:38.020
记得，当pytorch创建nn.Module时，会执行__init__

1:04:38.040,1:04:41.600
这时我们就创建了刚才看到的这些参数矩阵

1:04:41.680,1:04:46.100
我们通常不会自己创建参数矩阵张量

1:04:46.200,1:04:49.680
我们通常用pytorch的快捷函数来为我们实现这一任务

1:04:49.720,1:04:51.960
休息之后，我们会看到

1:04:51.980,1:04:59.740
现在，只需要记住，这句代码会为我们创造这些嵌入矩阵

1:04:59.780,1:05:04.660
这也是一个pytorch nn.Module

1:05:04.740,1:05:11.900
因此，将数据输入到那些嵌入矩阵里，从而计算返回激活值

1:05:11.920,1:05:15.120
我们将这个u_weight(本质上是embedding matrix)当作一个函数来用

1:05:15.180,1:05:17.380
（给它括号和users）

1:05:17.420,1:05:20.020
如果你去看pytorch的源代码

1:05:20.240,1:05:22.240
然后找到nn.Embedding

1:05:22.260,1:05:25.300
你会发现也有.forward在那里

1:05:25.360,1:05:27.720
为我们做数组查找

1:05:27.740,1:05:36.880
这里是我们获得用户的参数，这里是我们获取电影的参数

1:05:37.000,1:05:40.340
这样一来，我们就有了用户和电影的嵌入/参数矩阵

1:05:40.400,1:05:49.380
那么到这一步，我们就有了用户和电影的各自的权重/参数准备好了

1:05:49.420,1:05:53.960
然后，我们对它们做数组/矩阵乘法，再相加，

1:05:54.000,1:05:59.060
然后，再加上用户偏差和电影偏差

1:05:59.140,1:06:04.480
如果我们设置了y_range, 那么我们就做S函数的技巧处理

1:06:04.520,1:06:12.740
那么，到这里，你已经能理解整个模型的构建和计算流程

1:06:12.760,1:06:17.500
这不是随意的一个模型，这是我们刚刚创建的

1:06:17.740,1:06:31.240
至少非常有竞争力，而且可能更卓越

1:06:31.240,1:06:32.620
我们做得不错

1:06:32.700,1:06:41.480
这里是一个合适的休息的时机

1:06:41.740,1:06:50.920
休息回来之后，我们来学习另一个技巧，也就是wd=1e-1的功效

1:06:51.020,1:06:56.000
让我们7:50PM回来

1:07:00.220,1:07:08.060
关于解读嵌入矩阵的想法，是非常有意思的

1:07:08.120,1:07:12.020
这节课的后续部分，我们会看到

1:07:12.240,1:07:19.100
为类别变量，表格数据，以及嵌入矩阵而创建的东西，

1:07:19.120,1:07:25.140
（这些东西）就是与one-hot编码做矩阵乘法

1:07:25.180,1:07:31.340
然后采用了更有效的方法免去了内存负担

1:07:31.340,1:07:36.340
然后得到了这些有趣且出人意料的Semantics（嵌入矩阵）

1:07:36.380,1:07:45.840
这里有一篇非常有趣的论文，作者是Guo and Berkhahn,  他们在一个Kaggle竞赛中得了第二

1:07:46.160,1:07:48.160
（竞赛是Rossmann)

1:07:48.240,1:07:52.280
我们很可能在Part2里会深入学习Rossmann 竞赛

1:07:52.360,1:07:55.380
我觉得Part1的时间是不够用了

1:07:55.580,1:08:02.380
基本上都是一些基础的表格数据建模，有趣的内容主要在数据预处理

1:08:02.460,1:08:15.420
有趣的是，他们成为第二名，但是第一名和其他所有人都做了巨量的特别具体的特征工程

1:08:15.420,1:08:19.560
但是他们（第二名）所做的特征工程要远远少于其他人

1:08:19.640,1:08:25.600
不同与他人的是，他们使用了神经网络，这是2016，当时没有人这么做

1:08:25.620,1:08:28.120
没有人用神经网络做表格数据问题

1:08:28.160,1:08:32.940
我们目前所谈及的这些东西，

1:08:34.080,1:08:38.720
可以说是从这里萌发和变得受欢迎的

1:08:38.740,1:08:43.400
当我说“受欢迎”，其实只是极少数人在用，多数人少有问津

1:08:43.540,1:08:48.260
但是这个方法很酷，因为他们在论文中展示了MAPE（mean average percentage error)的成绩

1:08:48.320,1:08:54.380
对比了不同方法（下的MAPE的分数），有KNN，随机森林，Gradient Boosted Trees 梯度提升树

1:08:55.060,1:08:58.520
首先，神经网络的效果要好很多

1:08:58.520,1:09:04.900
然后，使用了Entity Embeddings 实体嵌入的方法，也就是用了Entity matrices实体矩阵和表格数据

1:09:04.900,1:09:10.920
事实上，你可以将实体嵌入植入到上述所有的方法中

1:09:10.920,1:09:14.120
训练过后，他们的效果都有大幅提升

1:09:14.120,1:09:21.180
虽然神经网络+EE仍旧最优，但随机森林+EE的成绩也相差不远

1:09:21.360,1:09:33.620
这看起来很棒，因为你可以训练这些实体嵌入矩阵，用于产品，商店，基因序列等等

1:09:33.660,1:09:37.060
可以用于不同模型中

1:09:37.060,1:09:40.420
用在比较快的模型中，如随机森林

1:09:40.520,1:09:42.580
会很有帮助

1:09:42.640,1:09:55.160
有趣的是，他们对德国各个州的嵌入矩阵做了2维的投射

1:09:55.160,1:09:58.280
因此这是一个德国的连锁超市

1:09:58.400,1:10:03.080
采用了与我们相同的方法，但我不记得他们用了PCA，或别的方法

1:10:03.180,1:10:09.280
然后，有趣的是，我在这里画了一个圈

1:10:09.380,1:10:15.280
embedding 空间中的一些东西，然后在这里用同种颜色画了一个圈

1:10:15.420,1:10:19.120
这里画了一个圈，同样的在这里

1:10:19.120,1:10:26.960
突然间我发现，embedding投射实际上发现了地理知识

1:10:27.400,1:10:37.760
他们没做什么，embedding找到相邻的东西，基于它们相似的日常购物特征

1:10:37.800,1:10:41.320
因为这个数据模型是为了预测销售额

1:10:41.440,1:10:45.760
居然就此发现了一些地理特征

1:10:46.380,1:11:01.100
实际上，这里是任意2个embedding数组之间的距离，你可以取一个embedding数组，然后计算它和其他embedding数组的欧几里得距离

1:11:01.180,1:11:09.560
将这个embedding数组之间的距离数据与现实世界里连锁超市之间的实际距离，组合起来作图，得到如图所见的强相关性

1:11:09.940,1:11:17.980
这里是embedding空间，针对的是周一到周日，如你所见，它们之间存在明显路径

1:11:17.980,1:11:19.880
这里是embedding space，针对一年的12个月，

1:11:19.960,1:11:23.360
同样的，它们之间构成有明显的路径

1:11:24.980,1:11:37.980
嵌入矩阵非常有趣，我不认为有人已经对Embedding的解读做了充分的探索和挖掘

1:11:38.020,1:11:48.780
如果你有基因序列或植物科目或你的商场销售的商品等等

1:11:48.880,1:11:58.220
如果训练几个模型，微调它们的嵌入矩阵，然后开始用相似的方法解读，比如

1:11:58.280,1:12:05.220
embedding数组之间的相似性，对它们做归类，对它们做二维投射，等等

1:12:05.220,1:12:06.420
这会非常有趣

1:12:08.620,1:12:17.060
之前我们努力尝试理解这个非常棒的协同过滤学习器的每一行代码

1:12:17.140,1:12:22.540
唯一没讲的是这个wd, wd代表的是weight decay权值衰减

1:12:22.820,1:12:28.940
什么是权值衰减？它是一种正则化方法（regularization）

1:12:29.360,1:12:37.020
什么是正则化？让我们回头看看这个很棒的图表，这是Andrew Ng在他的极棒的机器学习课程中画的

1:12:37.460,1:12:42.600
画了一些数据点，画了几条线穿越这些数据点

1:12:42.920,1:12:45.200
这条线

1:12:45.500,1:12:48.660
因为Andrew来自斯坦福大学，所有他必须使用希腊字母

1:12:48.800,1:12:51.940
我们可以就说它是a + bx

1:12:52.100,1:12:56.040
如果你非要用希腊语，那么就是theta_0 + theta_1x

1:12:56.200,1:12:58.200
这是一条线

1:12:58.260,1:13:01.220
即便它用了希腊语

1:13:01.420,1:13:08.320
这是2次多项式 2nd degree polynomial, a + bx + cx**2

1:13:08.680,1:13:11.560
这是一条曲线

1:13:11.640,1:13:16.740
这是n次多项式，是一条任意弯曲的曲线

1:13:16.900,1:13:26.500
含有很多参数的模型，比较像这样的曲线，所以在传统的统计学里

1:13:26.800,1:13:30.980
我们说让我们使用少量的参数

1:13:31.240,1:13:37.780
我们不希望模型长得这个样子，因为如果长得这个样子，这里和这里的预测都会是错的

1:13:38.280,1:13:41.860
模型不能很好泛化

1:13:41.860,1:13:43.680
这样我们就过拟合了

1:13:43.960,1:13:46.880
所以，我们通过使用少量参数来规避过拟合

1:13:47.020,1:14:02.940
所以，如果你们不幸的被心理统计学或计量经济学等等课程给予类似的“洗脑”，你将不得不卸载这些所谓“使用少量参数”的想法

1:14:03.020,1:14:16.720
你需要意识到，你被喂给了这样的“谎言”因为这是一个很方便的想象假说，但真相是，你不希望你的函数/模型太过复杂

1:14:16.900,1:14:21.960
使用少量参数，是一种使模型不那么复杂的方法

1:14:22.260,1:14:30.040
但如果你有1000个参数，而999个参数都是1e-9

1:14:31.440,1:14:37.620
如果它们都是0，它们就不存在了，但是1e-9，它们（存在）几乎不存在了，

1:14:37.660,1:14:41.740
为什么不能拥有很多很多参数，如果它们的值都非常小？

1:14:42.520,1:14:45.120
回答是，你可以

1:14:45.420,1:14:56.800
这个通过数参数数量来管控模型复杂度的想法，事实上，非常非常狭隘

1:14:56.900,1:14:59.960
这是一个有很多问题的”科幻小说“

1:15:00.060,1:15:08.580
所以，如果你的大脑里，模型的复杂度是基于参数的数量，你特错大错了

1:15:08.660,1:15:13.620
为什么我需要在意？为什么我需要用更多的参数？

1:15:13.860,1:15:18.680
更多参数，意味着更多非线性函数处理

1:15:18.780,1:15:29.540
更多（interactions）计算，更多弯曲（曲线），现实生活里充满了弯曲部分，现实生活不长这样

1:15:30.060,1:15:45.660
但我们不需要它过度弯曲，或过度（interacting）计算，所以让我们使用很多参数，然后对过度复杂做处罚

1:15:46.180,1:15:56.640
一种处罚过度复杂的方法是，正如我之前建议过的，让我们对参数做加法求和

1:15:56.780,1:16:07.160
这种做法并不好用，因为有些参数大于0，有些小于0，那么求和参数的平方，怎么样呢？

1:16:08.340,1:16:10.340
事实上，这是一个很棒的想法！

1:16:10.680,1:16:19.260
让我们创建一个模型，在它的损失函数里，让我们加上所有参数的平方和

1:16:19.540,1:16:32.400
但这里又有一个问题，也许这个值（平方和）过大，大到让最优的损失值逼迫所有的参数都归0

1:16:32.680,1:16:38.620
但这对我们没任何好处，所以我们要确保这样情况不会发生，

1:16:39.000,1:16:47.920
所以，让我们不仅仅只加上参数平方和，而同时再乘上一个值，我们可以选择这个值的大小

1:16:48.080,1:16:54.380
这个我们可选的值，在fastai里，我们叫它wd

1:16:55.520,1:17:10.060
好，接下来，在我们的损失函数里，先加上参数平方和，然后这个平方和再乘上一个值，wd

1:17:10.820,1:17:18.280
这个值应该是多少？通常，它应该是0.1

1:17:18.900,1:17:36.000
拥有花式头衔的机器学习PHD们，过度质疑和否定任何关于学习率可以在多数情况下是3e-3或者wd可以在多数情况下是0.1的观点

1:17:36.060,1:17:44.700
但事实是，我们做了很多实验，用了很多数据集，但却很难找到权值衰减=0.1表现不好的情况

1:17:45.240,1:17:53.360
然而，我们并不将它设置为默认值，实际上，我们的默认值是0.001

1:17:53.760,1:18:05.780
为什么？因为在极少数你有过多的权值衰减的情况下，不论你训练多久，都不会有足够好的拟合

1:18:06.420,1:18:09.340
如果你的权值衰减过少，

1:18:09.440,1:18:11.760
你可以依旧训练得不错，

1:18:11.880,1:18:14.900
但你会开始过拟合，所以你需要提前停止训练

1:18:15.120,1:18:18.220
所以，我们对默认值做了保守设置

1:18:18.260,1:18:25.320
但我的建议是：既然你已经知道，所有的学习器都有一个wd函数参数，

1:18:25.500,1:18:29.260
我需要提醒的是，你不是总能通过这里（定义）看到wd

1:18:29.340,1:18:39.640
因为这里有**kwargs，在python里，指的是那些在后续执行的函数中的参数

1:18:39.960,1:18:49.160
基本上，所有学习器，最终都会要执行Learner 这个constructor 构造函数，

1:18:49.240,1:18:51.580
这个构造函数里包含了wd

1:18:51.580,1:19:02.620
这些东西，你可以要么在文档中查看，要么既然你已经知道了，那么每当你构建一个Learner（从任何fastai函数中构建）

1:19:02.620,1:19:04.040
你都可以直接设置wd

1:19:04.080,1:19:10.520
所以，设置为0.1，而不用默认的0.001,  通常表现更好

1:19:10.520,1:19:11.900
所以，去试试吧

1:19:13.560,1:19:18.100
那这背后到底发生了什么？

1:19:20.700,1:19:29.640
会有帮助的是，回头看看第二课的梯度下降的Notebook

1:19:29.780,1:19:33.740
因为今天剩下的内容都是基于此

1:19:34.160,1:19:37.420
这里是我们构建的数据

1:19:37.860,1:19:41.780
然后加上一个损失函数，MSE均方误差

1:19:42.060,1:19:45.580
然后，构建了一个函数叫update

1:19:45.680,1:19:53.360
里面计算了预测值，也就是一个矩阵乘法，因为只有一个层，所以没有ReLU

1:19:53.420,1:19:56.600
然后就是损失值，用的是均方误差

1:19:56.780,1:20:00.420
然后计算梯度，用的是loss.backward

1:20:00.580,1:20:05.280
然后用，sub_来减去lr * a.grad （也就是学习率*梯度）

1:20:05.400,1:20:12.160
这就是梯度下降。如果你还没有复习第二课的SGD

1:20:12.380,1:20:20.000
请去复习，因为我们要从这里开始，如果你不明白这里的起步，后面很难理解

1:20:20.040,1:20:24.920
如果你在看这个视频，先暂停，去复习一下第二课的SGD，确认你理解了

1:20:25.260,1:20:33.980
还记得吗？a.sub_ 基本上就是 a -= ,

1:20:34.060,1:20:42.600
因为a.sub_是做减法，同时pytorch里所有函数，如果你加上一个 _ ，意思是就地做减法（不另外在赋值）

1:20:42.700,1:20:51.020
所以，这里是在对参数a做更新，a 从最初的[-1, 1]开始（是我们随意设置的）

1:20:51.260,1:20:53.260
然后，一步步让它变得更好（更新）

1:20:53.320,1:20:56.700
那么，让我们把它写下来

1:20:58.660,1:21:09.560
我们尝试计算，参数，但我在这里就叫它们权重

1:21:15.160,1:21:24.920
w_t 代表的是在时间t或epoch t下的权重，它的值等于前一刻（t-1)的权重

1:21:25.140,1:21:43.100
减去，学习率乘上损失函数值的导数/在t-1时刻下权重的导数

1:21:43.260,1:21:49.620
刚才这些就是这些代码所做的实质内容

1:21:49.800,1:22:00.520
我们不必计算梯度，因为这个计算很无聊，也因为计算机可以代劳而且很快，然后将结果存在这里

1:22:00.520,1:22:02.480
这样就好了

1:22:02.560,1:22:16.260
确保你能很好的理解这个公式或者这行代码，因为它们本质是一样的

1:22:18.860,1:22:22.280
接下来要说什么呢？

1:22:25.840,1:22:28.480
什么是我们的损失值

1:22:28.720,1:22:45.620
损失值，是一个函数（的结果），函数里面要用到自变量和参数

1:22:45.660,1:22:51.880
我们这里要使用的是均方误差MSE

1:22:52.140,1:22:58.160
MSE要用到的输入值是预测值和actuals目标真实值

1:22:58.240,1:23:02.200
那么，X和w如何进入到MSE的函数计算中来呢？

1:23:02.200,1:23:12.440
预测值，是来源于对某个模型的计算执行，我们叫这个模型m, 预测值 = m(X, w)

1:23:12.460,1:23:19.080
这就是我们的一种损失函数，损失函数种类很多，今天我们还会看到其他损失函数

1:23:20.320,1:23:30.060
这就是梯度的计算，对应的是a.grad

1:23:30.420,1:23:40.860
接下来我们要做的是，加上权值衰减wd，也就是0.1

1:23:41.180,1:24:00.020
乘上，每个参数的平方之和，

1:24:00.060,1:24:02.060
让我们来试试

1:24:03.660,1:24:06.680
同时让它更有趣

1:24:07.280,1:24:11.100
我们不用合成简易数据，而是用真实数据集

1:24:11.200,1:24:22.260
我们要用MNIST，手写数字图片数据，但我们要按照标准全联接层设计的模型，而不是卷积神经网络模型

1:24:22.480,1:24:26.800
我们还没有学习如何从头手写CNN

1:24:26.940,1:24:43.680
deeplearning.net提供了MNIST数据集，采用了python pickle文件格式，换言之，python可以直接打开这个文件，直接给你numpy.arrays, 并且自动为我们flatten整平 numpy.arrays

1:24:43.800,1:24:56.400
首先，从这里下载数据集，得到的是gzip文件，所以可以gzip.open直接打开文件，然后可以直接pickle.load

1:24:56.420,1:25:02.340
同样的，encoding='latin-1', 理由你知道的（前面讲过的，旧数据集的特点）

1:25:02.400,1:25:13.900
这样就能输出训练集和验证集，我不在意测试集，通常在python中，如果是你不在意的内容，就用 _

1:25:13.980,1:25:18.760
不是必须的，但这样做可以让别人知道你不在意测试集的内容

1:25:18.780,1:25:23.420
所以，这里只会输出训练集XY和验证集XY

1:25:23.700,1:25:31.000
如你所见，直接给到我们的x_train的维度是（5000，784）

1:25:31.060,1:25:35.600
但784列其实对应的是一张28x28的图片

1:25:35.720,1:25:43.140
所以，如果reshape 784 成为 （28，28）并且作图，我们就可以看出它是一张手写数字5的图片

1:25:43.160,1:25:50.660
所以，这就是我们的数据，我们之前见过MNIST，之前的是已经做过reshape的版本，这里的是flattened（未reshape）版本

1:25:50.760,1:25:55.860
我在这里还是用平整flattened 版本

1:25:55.860,1:26:00.360
目前数据样本都是numpy.arrays数组, 我需要它们是tensors 张量

1:26:00.600,1:26:06.240
我可以用map, 将torch.tensor格式，映射到训练集和验证集中

1:26:06.240,1:26:07.480
这样它们将变成张量了

1:26:07.700,1:26:23.660
我还可以构建一些变量，n指数据总量，c指代所需的激活值的数量，抱歉，这里不是激活值的数量，而是列的数量

1:26:23.760,1:26:27.020
（这些变量名称）不是好名称，抱歉。

1:26:27.120,1:26:35.380
好的，然后Y标注，不出意料，最小值是0，最大值是9

1:26:35.460,1:26:37.460
因为这是目标值（也是我们要预测的值）

1:26:37.520,1:26:48.600
所以，在第二课SGD，我们构建了数据，加上一列1，来避免担心偏差问题

1:26:48.660,1:26:52.980
我们不再这么做了，我们用pytorch间接为我们构建偏差

1:26:52.980,1:26:56.660
我们之前需要手写MSE函数，现在也不用了；

1:26:56.740,1:27:01.340
我们之前需要手写矩阵乘法，现在也不用了；

1:27:01.380,1:27:03.380
我们现在有pytorch来做所有这些。

1:27:03.480,1:27:12.800
剩下的，也是更重要的，是我们要做小批量，因为这是一个不小的数据集，我们不希望一次性处理全部数据

1:27:12.920,1:27:24.340
如果你要做小批量，我们在这里不会用过多fastai工具，pytorch有tensorDataset

1:27:24.420,1:27:34.160
基本上，可以吃进2个张量，来构建一个Dataset

1:27:34.220,1:27:41.180
记住，Dataset可以做到，如果你给一个序号，Dataset能返回给你一个X，一个Y，就是一个 样本组合

1:27:41.180,1:27:48.640
它看起来很像list, 一个聚集（X， Y）的list

1:27:48.720,1:27:55.980
一旦你有了一个Dataset, 你可以得到更多一点点的便捷，就是用DataBunch.create

1:27:56.060,1:28:08.400
这个函数，将为我们构建DataLoaders , 有了DataLoader, 你无需说：我要第一个或第五个（东西），你只需要说：我要下一个（东西）

1:28:08.480,1:28:12.580
它会自动给你一个批量（东西），批量大小是根据你的要求来的

1:28:12.680,1:28:22.320
它会返回你(x, y) 这是一个批量数据，我们通过next(iter(data.train_dl))来获取，这是标准的python

1:28:22.320,1:28:23.840
如果你有用过python 中的iteration 的话

1:28:24.640,1:28:28.460
这是训练集的dataloader,是databunch.create构建的

1:28:28.540,1:28:42.440
如你所期待的，x自变量是（64， 784），列是784像素被平整了， 批量数是64，y 就是64个数值

1:28:42.540,1:28:44.860
它们是我们要预测的标注labels

1:28:44.940,1:28:52.440
如果你查看DataBunch.create的源代码，你会看到其实没有多少代码，请去看看，

1:28:52.440,1:29:03.840
我们只需要训练集被随机打乱顺序，并且让数据加载到GPU上，就是一些让工作更便捷的操作

1:29:03.940,1:29:10.960
它们没什么神奇的，如果感觉它们很神奇，去查看它们的源代码，确保你了解背后的操作

1:29:11.040,1:29:19.140
好的，与其做y_hat = x@a, 我们构建了一个nn.Module

1:29:19.220,1:29:26.760
如果你想构建一个nn.Module来做与已有的不一样的事情，你需要在它基础上再构建一个子类

1:29:26.780,1:29:38.780
构建子类，在pytorch中是非常常见的操作，如果你不是很习惯python中的子类操作，去读几篇tutorials来熟悉

1:29:38.900,1:29:47.400
主要操作是，你需要编辑覆盖constructor构造函数，__init__, 确保你执行上一级大类的__init__()

1:29:47.480,1:29:51.240
因为，上一级大类，也就是nn.Module会做好大量的设置准备

1:29:51.300,1:29:54.460
从而为你提供一个标准的nn.Module

1:29:54.580,1:30:02.960
所以，当你构建一个子类但报错时，几乎可以确定你忘了这一行代码

1:30:03.000,1:30:13.880
唯一一行我们想添加的代码是，构建一个属性，来储存一个线性层结构

1:30:13.900,1:30:17.660
也就是一个nn.Linear。那么，什么是nn.Linear?

1:30:17.700,1:30:28.300
它的功能是做这个x@a, 但不仅仅如此，还要加上b；换言之，我们无需加一列1了

1:30:28.300,1:30:30.200
这就是nn.Linear的全部功能

1:30:30.260,1:30:40.620
如果你想实验代码，为什么不构建你自己的nn.Linear类，你可以构建一个类叫my_Linear

1:30:40.660,1:30:48.020
这会花费你（多少时间），取决与你的pytorch技能掌握情况，可能1-2小时；

1:30:48.040,1:30:54.340
我们不希望任何这些内容感觉很神奇，现在你已经知道所有构建这些函数/类的知识

1:30:54.840,1:30:58.200
这些可以成为你的作业项目，

1:30:58.200,1:31:05.040
本周没什么新应用，尝试手写这些函数和类，

1:31:05.040,1:31:09.180
让它们能正常工作，学习如何debug排除故障，查看输入和输出值，等等

1:31:09.440,1:31:19.820
好的，我们可以直接用nn.Linear, 它会有一个def forward 里面有a@x + b

1:31:19.860,1:31:24.460
那么在我们的forward里，我们如何计算呢？

1:31:24.460,1:31:28.240
要记住，每一个nn.Module可以看作是一个函数，

1:31:28.240,1:31:33.360
我们可以输入一个x 小批量，我习惯用xb 代表一个x的小批量

1:31:33.680,1:31:36.580
给到self.lin()

1:31:36.640,1:31:40.220
然后，就能返回给我们a@x+b的结果

1:31:40.220,1:31:41.980
在这个小批量上

1:31:42.080,1:31:45.060
这就是一个逻辑回归模型logistic regression model

1:31:45.160,1:31:50.400
逻辑回归模型，也被称作，没有隐藏层的神经网络

1:31:50.400,1:31:53.480
所以，是一层结构的神经网络，没有非线性激活函数

1:31:54.260,1:32:03.400
因为我们在手动搭建模型，所以我们需要手动将参数矩阵放在GPU上，

1:32:03.440,1:32:06.600
所以，输入.cuda()就行了

1:32:06.680,1:32:14.860
这就是我们的模型，如你所见，nn.Module自动给到我们模型的样子

1:32:14.880,1:32:18.900
模型已经自动储存了.lin这个类/函数，并告诉我们里面有什么

1:32:19.020,1:32:21.600
类似这样的便捷函数pytorch里面不少

1:32:21.760,1:32:26.740
现在如果你打开model.lin, 毫不意外，你可以看到这些内容

1:32:26.880,1:32:30.840
可能最有趣的是，

1:32:30.840,1:32:38.300
我们的模型，自动生成一系列methods/函数 和属性，

1:32:38.300,1:32:54.800
可能其中最有趣的一个是parameters, 它们包含了我们所有的黄色方框（参数矩阵），储存这我们的参数，（权重和偏差矩阵）

1:32:54.880,1:32:59.200
如果我们运行[p.shape for p in model.parameters()],

1:32:59.200,1:33:04.420
有一个维度是（10，784）的东西，有一个长度是10的东西；

1:33:04.500,1:33:05.560
它们是什么？

1:33:05.640,1:33:13.380
（10，784）这是说明它会接受784个输入值，返回10个输出值，

1:33:13.380,1:33:20.340
这看起来很便利，因为我们的输入值是784个维度，我们需要输出10个概率值。

1:33:20.620,1:33:23.260
这之后，我们有10个激活值

1:33:23.480,1:33:25.780
然后我们需要对其加上偏差，

1:33:25.840,1:33:28.360
这样一来，我们有了一个长度为10的数组

1:33:28.400,1:33:35.900
这样，你可以看出，为什么这个模型拥有所有我们需要的东西

1:33:35.960,1:33:39.600
来计算a@x + b

1:33:39.780,1:33:42.840
那么，让我们采用一个学习率

1:33:42.980,1:33:44.920
我们稍后再来看看这个损失函数

1:33:45.020,1:33:46.300
我们不能用MSE

1:33:46.400,1:33:54.600
我们这里不能用MSE，我们不是要计算多么接近，比如，我们预测了3， 但实际答案是4

1:33:54.600,1:33:56.140
天啊，我们非常接近了

1:33:56.180,1:34:00.880
不是的，在这里，对我们而言，3与4的差距，和0与4的差距，是一样的

1:34:00.900,1:34:03.860
当我们的目标是预测图片的数字

1:34:03.920,1:34:07.160
所有，我们不能用MSE，而要用交叉熵

1:34:07.160,1:34:08.440
我们稍后会学习

1:34:08.760,1:34:10.760
这里是我们的update 函数

1:34:10.840,1:34:12.840
我从第二课的SGD中复制出来的

1:34:13.040,1:34:16.580
现在我们要执行模型model(x)

1:34:16.600,1:34:21.620
不是a@x+b，而是执行model(x), 就好像这是一个函数，来获取y_hat (预测值)

1:34:21.920,1:34:27.240
我们执行loss_func(y_hat, y), 而不是MSE，来获得损失值

1:34:27.480,1:34:29.840
这和之前的操作很相似

1:34:29.920,1:34:39.840
不是一个参数一个参数的做: 参数 = 参数 - 学习率*梯度，而是loop 每一个参数，

1:34:40.120,1:34:50.700
因为，非常棒的是，pytorch会自动生成一个list 里面都是参数，这些参数是__init__()里构建的

1:34:50.880,1:34:52.880
我还加入了别的

1:34:53.260,1:35:03.820
这里有一个叫w2, 我们逐一调取每一个参数矩阵，让参数矩阵平方和与w2相加

1:35:04.020,1:35:07.380
所以，w2包含了所有参数矩阵平方和，

1:35:07.500,1:35:11.020
然后，在乘以某个值，

1:35:11.120,1:35:16.580
这个值，我们设置为1e-5,  这里看到的就是权值衰减的计算过程

1:35:17.120,1:35:31.100
所以，当人们讨论权值衰减时，这不是什么神奇且复杂的事情，需要上千行cuda/c++代码；权值衰减，仅需要这两行python代码即可

1:35:31.680,1:35:37.140
这就是权值衰减，这不是什么简化版的代码，只为当前目的；这就是完整版的代码，就这样。

1:35:37.260,1:35:41.400
要注意的是，

1:35:41.400,1:35:47.320
有趣的是，有一个二元思路是关于权值衰减的，

1:35:47.780,1:35:51.500
一个是我们就是加上了参数平方和，

1:35:51.500,1:35:54.780
这个听起来很合理，的确也是，

1:35:54.780,1:35:57.420
让我们先跑起来

1:36:01.540,1:36:04.780
这里生成的一列数，对理解有帮助

1:36:04.820,1:36:13.420
遍历我的整个DataLoader, DataLoader逐一给我全部的小批量，

1:36:13.420,1:36:18.840
每次给我一个(x, y)， 再输入给update, 输出一个损失值，

1:36:18.880,1:36:26.460
pytorch 的张量tensors, 因为刚才所有计算发生在GPU，

1:36:26.460,1:36:31.520
所有这些数据都在GPU上一起就是梯度gradient, 这会占用很多内存，

1:36:31.520,1:36:38.680
如果你执行.item()，针对的是1维的张量，将返回给你一个普通的python数值。

1:36:38.680,1:36:48.720
所以，这里就是返回普通python数值。然后，我们可以作图，你快看，我的损失值在下降

1:36:48.880,1:36:55.080
尝试这些代码感觉很好，因为你可以看到你认为会发生的真的发生了，

1:36:55.080,1:36:59.540
当我们越来越接近（最优值/答案），损失值跳动/震动得更加激烈，

1:36:59.540,1:37:06.800
因为我们在靠近我们的真实值/我们应该在的区域，而参数值区间可能变得越来越平缓，（损失值来回）跳跃幅度很大，

1:37:06.800,1:37:12.080
这就是为什么我们希望在训练中不断缩小学习率，这就是learning rate annealing 学习率退火

1:37:12.420,1:37:17.400
这背后发生了什么呢？

1:37:17.580,1:37:23.400
权值衰减，只是在训练神经网络时，才有用；

1:37:23.400,1:37:36.400
因为，我们要对它取梯度，这一部分会帮助更新参数，

1:37:36.400,1:37:43.960
所以（wd*参数平方和）的唯一有趣/重要的点是，它的梯度计算

1:37:44.400,1:37:48.760
所以，我们要在这里做很多计算（手动），但我认为我们可以应对，

1:37:48.760,1:37:59.489
这整个的梯度，如果你还记得高中数学，等于它一分为二的每个部分的梯度，然后再相加，

1:37:59.489,1:38:03.780
那么让我们计算一下第二部分的梯度

1:38:03.880,1:38:06.780
因为我们已知这一部分的梯度计算，

1:38:06.780,1:38:11.780
所以第二部分的梯度怎么计算呢？

1:38:11.920,1:38:17.540
让我们先移除求和，假装只有一个参数，不会改变整体的逻辑，

1:38:17.540,1:38:35.680
（wd * w**2)的梯度(相对于w） = 2 wd * w

1:38:35.760,1:38:47.160
还记得吗？wd是一个常量，(在那个小循环里）1e-5,  这个是参数

1:38:47.260,1:38:54.740
我们可以用wd来取代2wd, 不会影响整体逻辑，所以让我们剔除2

1:38:54.940,1:39:05.540
换言之，权值衰减所做的，是在减去（某个常量*参数），在每次批量计算中。

1:39:05.540,1:39:09.720
这就是为什么叫它权值衰减的原因

1:39:09.900,1:39:22.140
当权值衰减，存在于这种表达式中（我们将参数平方和加给损失函数），我们称之为L2 regularization 正则化;

1:39:22.140,1:39:35.560
当处于这个表达式（在梯度中减去wd * w) 我们称之为weight decay 权值衰减

1:39:37.340,1:39:43.140
它们可以说在数学上是一样的，到目前为止，全部都是，

1:39:43.140,1:39:49.040
一会我们会看到，它们不一致的地方，也是有趣的地方

1:39:49.500,1:39:53.300
这是一个非常重要的工具，已经纳入你的工具包里了，

1:39:53.300,1:39:56.940
你可以构建巨大的神经网络，

1:39:56.940,1:40:03.000
依旧能避免过拟合，只要增加更多的权值衰减就可以了。

1:40:03.020,1:40:08.500
或者，你可以使用非常小的数据集，相对更大的模型，

1:40:08.500,1:40:10.280
依旧可以规避过拟合，通过使用权值衰减。

1:40:10.280,1:40:12.900
这没什么神奇的，

1:40:12.900,1:40:20.020
你可能发现你没有足够的数据，也就是在训练时，你依旧没有过拟合，因为加入了很多权值衰减，

1:40:20.020,1:40:22.040
只是训练效果不是很好，这是会出现的情况。

1:40:22.040,1:40:26.520
但至少，这是你可以实现对这个技巧的驾驭。

1:40:27.640,1:40:42.360
我们继续往下，现在我们有了这个update 函数，我们可以取代掉Mnist_logistic, 而采用Mnist_NN, 从头构建一个神经网络

1:40:42.500,1:40:45.840
现在我们需要2个线性层结构，

1:40:45.840,1:40:49.120
第一个层的参数矩阵的大小可以设置为50（列），

1:40:49.120,1:40:53.560
所以我们需要确认第二层的输入数据大小（也就是输入矩阵的列，对应参数矩阵的行数）是50，

1:40:53.560,1:40:59.520
而最后一层的输出数据的大小是10，因为这是我们要预测的类别的数量。

1:40:59.660,1:41:02.440
所以，我们的forward函数，要做的是，构建一个线性层，

1:41:02.440,1:41:04.760
计算ReLU,

1:41:04.760,1:41:07.020
构建第线性二层；

1:41:07.020,1:41:10.080
这样我们就有了一个从头构建的神经网络。

1:41:10.080,1:41:12.640
当然，我们没有手写nn.Linear,

1:41:12.640,1:41:13.860
但你完全可以自己写，

1:41:13.860,1:41:18.060
或者你可以直接做矩阵乘法，你已经知道学过了。

1:41:18.180,1:41:22.460
然后，同样的，Mnist_NN().cuda(),

1:41:22.460,1:41:25.860
然后用相同的方法计算损失值，

1:41:25.860,1:41:28.020
然后就有了（下图）

1:41:28.040,1:41:31.160
这就是为什么神经网络很简单，

1:41:31.160,1:41:38.700
一旦你有了一个可以在梯度下降的函数，你就可以尝试不同的模型了

1:41:38.780,1:41:45.780
然后，你可以加入更多pytorch代码，与其用所有这些手写的代码，

1:41:45.780,1:41:52.760
为什么不用opt = optim.Something, 这个Something(某个优化算法），

1:41:52.760,1:41:57.060
我们目前所用到的，是SGD梯度下降；

1:41:57.060,1:42:05.240
然后，你跟pytorch说，我要用这些参数，对这些参数更新，使用的方法是梯度下降。

1:42:05.600,1:42:14.480
然后，这里与其写 for p in model.parameters(): 
                         p.sub_(lr * p.grad)

1:42:14.480,1:42:15.640
你只需要写opt.step()

1:42:15.640,1:42:16.860
效果是一样的

1:42:17.060,1:42:21.640
代码少了好多，但是做一样的事情，

1:42:21.640,1:42:28.900
这里有趣的的原因是，现在你可以用Adam取代SGD，

1:42:28.900,1:42:33.360
你甚至可以加入权值衰减，

1:42:33.360,1:42:41.840
因为这里面有很多可以添加的内容，

1:42:41.840,1:42:46.640
这就是为什么我们倾向于使用optim.Something,

1:42:46.640,1:42:51.480
而fastai背后的源代码里，我们用的就是optim.Adam

1:42:51.480,1:43:02.260
如果我用optim.SGD, 我们得到这样一张图，

1:43:02.260,1:43:15.820
如果换成另一个优化算法，看看这里发生了什么，损失值diverged扩散了。

1:43:15.820,1:43:21.040
我们看过一张很棒的散度图，有一位学员画的，

1:43:21.040,1:43:28.540
这是训练中散度的样子，我们用了不同的优化算法，所以需要不同的学习率。

1:43:28.600,1:43:30.800
我们不能继续训练，

1:43:30.800,1:43:36.060
因为当散度出现时，权重/参数要么特别大或特别小，

1:43:36.060,1:43:39.660
它们不会纠正自己，所以我们要重新训练

1:43:40.200,1:43:42.940
调整学习率重新训练后，看看这里，

1:43:42.940,1:43:46.720
我们下降到了0.5，只用了200epochs,

1:43:46.720,1:43:52.520
而之前，我不确定我们有降到这么低过。

1:43:52.520,1:43:55.700
所以，到底发生了什么？什么是Adam

1:43:55.800,1:43:58.360
让我展示给你看看，

1:43:58.360,1:44:05.720
我们要去Excel里做梯度下降，为什么不呢？

1:44:05.720,1:44:16.180
这里是随机生成的数据，随机生成的x,y, 但实际上，X是随机生成的，

1:44:16.180,1:44:22.420
Y则是通过计算ax + b, a 是2， b是30，

1:44:22.420,1:44:25.080
这是我们要对应/映射/预测的数据

1:44:25.220,1:44:32.460
这里是梯度下降，我们要来做梯度下降，

1:44:32.460,1:44:34.920
在我们的第二课SGDnotebook，

1:44:34.920,1:44:37.880
我们对全部数据集一次性完成所有的更新，作为一个完整批量，

1:44:37.880,1:44:46.200
在刚才的notebook中我们用的是小批量，在这个Excel中我们要做的是online gradient descent 在线梯度下降,

1:44:46.200,1:44:51.320
意思是每一行数据都是一个小批量，也就是批量大小是bs = 1。

1:44:51.360,1:44:56.660
就像往常一样，我们要随意选择一个截距和一个斜率，

1:44:56.660,1:45:01.240
我们给它们的值都是1，

1:45:01.240,1:45:04.620
我们将数据（X, Y )复制粘贴到这里,

1:45:04.620,1:45:10.320
如我所获，我们的截距和斜率都是1， 我只是将它们的值链接到这个单元格里

1:45:10.600,1:45:18.320
我的预测值，针对这组截距和斜率，就是14*1+1 = 15，

1:45:18.320,1:45:26.800
这就是我的误差，误差平方和，不对，这是误差平方，

1:45:26.840,1:45:31.660
现在我需要就是梯度，来更新参数，

1:45:31.660,1:45:37.120
有两种计算梯度的方法，一种是analytic solution 解析解，

1:45:37.120,1:45:40.460
（这里是解析解的计算公式）你可以从Wolfram查找到，

1:45:40.460,1:45:45.440
这就是梯度，你可以手动推算出这两个式子，或者在网上查找；

1:45:45.440,1:45:49.420
或者你可以采用finite differencing 有限差分法，还记得，

1:45:49.420,1:46:00.260
梯度就是输出值变化幅度/输入值变化幅度（两者的变化都是非常的小）

1:46:00.280,1:46:05.780
首先，让我们做一些特别小的变化，

1:46:05.780,1:46:15.640
这里我们对截距增加0.01，再计算误差平方；

1:46:15.640,1:46:21.400
你可以看到，损失值变小了一点；

1:46:21.400,1:46:28.960
然后，因为截距有0.01的改变，估算的梯度就是梯度的改变/0.01的截距的改变，

1:46:28.960,1:46:34.040
这就是有限差分，你可以随时做梯度的有限差分但是比较慢

1:46:34.280,1:46:37.520
我们在实际查找中并不适用有限差分，但是它能帮助我们确认梯度计算的正确性，

1:46:37.520,1:46:42.340
我们可以将这种方法用到斜率上，对斜率增加0.01，

1:46:42.340,1:46:44.900
在用误差平方的改变/0.01，得到相对于斜率的梯度；

1:46:44.900,1:46:49.500
或者，我们可以直接用解析解方式计算梯度，

1:46:49.500,1:46:58.360
你们可以看到，两种方法的结果符合预期，非常接近；

1:46:58.400,1:47:09.240
所以，梯度下降，也就是用当前的参数，减去学习率乘上梯度，这样就实现参数更新了；

1:47:09.240,1:47:20.220
现在，我们可以复制新的参数，给到下一行数据来重复刚才一系列的计算；

1:47:20.220,1:47:25.200
重复许多次，直到完成所有数据，这就是一个完成的epoch迭代次数

1:47:25.220,1:47:29.140
在这个epoch结束时，我们可以说，好的，

1:47:29.140,1:47:36.320
这就是我们的斜率，然后复制它粘贴到这里，

1:47:36.320,1:47:44.400
复制新的截距到这里；

1:47:44.400,1:47:48.320
现在，第二个epoch的计算也完成了（宏）

1:47:48.420,1:47:53.040
这有些无聊，不停复制粘贴，

1:47:53.040,1:47:58.200
所以我构建了一个“很复杂“的宏，

1:47:58.200,1:48:04.880
来为我们做复制粘贴，这里就是将复制和粘贴录制下来

1:48:04.880,1:48:09.360
然后构建了一个”复杂“的for loop 循环，重复5次上述复制粘贴工作，

1:48:09.360,1:48:11.860
然后将它赋值给“run“这个按钮；

1:48:11.860,1:48:18.320
那么，按下”run“，就会运行5遍，并记录这5次运行的误差值。

1:48:18.320,1:48:22.400
这就是梯度下降。

1:48:22.440,1:48:27.340
如你所见，这个速度是超级的慢，

1:48:27.340,1:48:32.980
尤其是截距应该是30，

1:48:32.980,1:48:40.220
而我们依旧在1.57这个位置，实在太慢了；

1:48:40.300,1:48:45.260
所以，让我们来提速，首先我们可以用Momentum 动量法 来提速，

1:48:45.260,1:48:51.360
这里是和上一个worksheet一样的worksheet，

1:48:51.360,1:48:58.020
我移除了有限差分法的部分，因为它们没那么有用，只保留了解析解的方法，

1:48:58.100,1:49:10.740
下面内容是关键，我们用这里的导数（de/db），来做更新

1:49:10.740,1:49:18.300
具体操作是，用这个看更有趣，让这个导数乘以0.1，

1:49:18.300,1:49:26.460
然后用上一次的导数值(J8)乘以0.9, 然后将两者相加，

1:49:26.460,1:49:33.940
换言之，这次更新并不仅仅是依据导数，

1:49:33.940,1:49:37.140
只有10%是依赖了de/db这个导数，

1:49:37.140,1:49:41.640
而90%的部分沿用了上一次的导数更新值（J8），

1:49:41.640,1:49:49.140
这就是动量法。它的意思是，还记得

1:49:49.660,1:49:58.560
我们有考虑过会发生什么，如果尝试寻找它的最小值；

1:49:58.560,1:50:04.440
如果你在这里，但学习率又很小，不断重复相同的操作；

1:50:04.440,1:50:15.560
如果你反复做相同的操作，如果你还加入你上一次的操作，你的跨越的步伐将会越来越大，不是吗？

1:50:15.560,1:50:28.480
直到最后，步伐过大，但到那时，当然，你的导数将指向与动量相反的方向，所以你的步伐会变小，

1:50:28.560,1:50:34.120
然后，你将小步更新，大步更新，小步，。。。就像这样

1:50:34.120,1:50:39.920
这就是动量法的效果。

1:50:39.920,1:50:51.320
如果你走的过远，像这样，也很慢，

1:50:51.320,1:51:01.940
那么你之前几步的均值，应该是前几步的中间部位，

1:51:01.940,1:51:09.320
这是非常常见的想法，在这里可以理解为，

1:51:09.320,1:51:26.640
S_t（当前的步伐大小） = 某个值人们喜欢使用alpha, 如我所说，人们喜欢希腊符号

1:51:26.640,1:51:36.020
乘上（我们真正想要做的事情，也就是梯度）

1:51:36.020,1:51:49.180
加上 （1-alpha）乘上（S_{t-1}上一次的步伐大小）。

1:51:49.180,1:51:56.020
这个式子，叫做exponentially weighted moving average 指数加权移动平均值.

1:51:56.020,1:52:01.560
为什么呢？因为1-alpha会不断累积相乘，

1:52:01.560,1:52:07.680
这里面包含了（1-alpha)**2 * S_{t-2},

1:52:07.680,1:52:10.680
同理，也就包含了（1-alpha)**3 * S_{t-3}

1:52:10.700,1:52:16.860
换言之，这就变成了，我想要的梯度，

1:52:16.860,1:52:21.220
加上一个之前多个时刻的值的加权平均值，

1:52:21.220,1:52:24.980
其中时间越近的值拥有越高的权重。

1:52:24.980,1:52:30.940
这个概念以后会反复出现的，这就是动量，

1:52:30.940,1:52:40.180
也就是说，我想在当前梯度状态下前进，加上最近几个值的加权平均值。

1:52:40.180,1:52:45.180
这是非常有帮助的，这就是梯度下降+动量的方法。

1:52:45.300,1:52:51.320
我们可以（写成代码），将Adam改成SGD，

1:52:51.320,1:53:01.480
加上动量，设置成momentum=0.9比较常见，几乎总是0.9，对于基础类的问题，

1:53:01.480,1:53:03.660
这是如何使用梯度下降和动量。

1:53:03.660,1:53:11.840
同样的，我没有给你们一个简约版代码，我给大家的是唯一的版本，这就是SGD，

1:53:11.840,1:53:16.740
你可以手写自己的版本，试试吧，这会是很好的作业项目。

1:53:16.740,1:53:21.860
你可以在第二课SGD基础上加上动量。

1:53:21.860,1:53:30.360
或者用这个Mnist Notebook，丢掉optim.SGD, 手写一个update函数里面包含SGD+momentum

1:53:30.460,1:53:32.480
然后，有一个很酷的优化算法，叫RMSprop.

1:53:32.480,1:53:41.560
RMSprop的一个非常有趣的地方在于，Jeffeory Hinton，非常著名的神经网络之父，创造了它，

1:53:41.560,1:53:48.840
所有人都用RMSProp, 所以非常受欢迎和常见，RMSprop的正确的引用方式是

1:53:48.840,1:53:51.900
Coursera 在线免费课程

1:53:51.900,1:53:56.600
这是他首次介绍了RMSProp

1:53:56.660,1:54:02.380
我特别喜欢这种风格，非常酷的东西源于免费在线课程，而非论文

1:54:02.400,1:54:05.840
RMSprop 与动量很相似，

1:54:05.840,1:54:13.340
但这一次，我们的指数加权平均值，没有用在梯度更新上，

1:54:13.340,1:54:19.740
而是用在了梯度的平方上（F8**2）。

1:54:19.740,1:54:27.140
RMSprop = 0.9 * 上一次更新值 + 0.1 * 梯度的平方。

1:54:27.140,1:54:32.200
所以，这是梯度平方的指数加权平均值。

1:54:32.200,1:54:33.640
那么，这个值的意义应该如何解读？

1:54:33.720,1:54:40.520
如果我们的梯度特别小，并且持续很小，这将是一个很小的值；

1:54:40.520,1:54:46.320
如果我的梯度波动非常大，这将是一个很大的数；

1:54:46.320,1:54:49.820
或者这个值一直很大，也会是很大的数；

1:54:49.820,1:54:55.700
为什么这很有趣呢？因为当我们在这里更新参数时，我们说：

1:54:55.700,1:55:09.120
参数-学习率*梯度/上一次的RMSprop值的平方根。

1:55:09.200,1:55:14.840
换言之，如果梯度持续很小，且波动不大，

1:55:14.840,1:55:18.940
RMSprop能让我们做更大的跳跃，这也是我们所希望的；

1:55:18.940,1:55:27.000
我们发现截距值更新太慢，所以很明显更新步子应该再大一些；

1:55:27.000,1:55:34.260
所以，如果我们按下"run", 5个epochs之后，截距已经是3+；

1:55:34.260,1:55:42.860
而基础版，5个epochs之后，依旧只有1.27；

1:55:42.860,1:55:45.120
记住，我们要靠近的值是30；

1:55:45.120,1:55:51.820
所以，显然我们该做的事情是，“显然”其实也只是在几年前才被弄清楚的事情，

1:55:51.820,1:55:55.940
就是两个都做（动量+RMSprop),这就是所谓的Adam

1:55:56.080,1:56:03.420
所以，Adam，就是跟踪记录梯度平方的指数加权移动平均值（RMSprop)，

1:56:03.420,1:56:08.880
跟踪记录我的步伐的指数加权移动平均值(动量），

1:56:08.880,1:56:17.720
新的参数b = 当前b - 学习率*动量b/RMSprop(b)的平方根，

1:56:17.720,1:56:23.500
动量和RMSprop对b参数都沿用了上一个值的90%，

1:56:23.500,1:56:28.420
包含了动量和RMSprop, 这就是Adam

1:56:28.480,1:56:37.280
看这里，5个epochs，我们到了25，

1:56:37.280,1:56:43.920
这些优化算法，被称之为dynamic learning rate 动态学习率，

1:56:43.920,1:56:48.500
对它常见的误解是，你不必设置学习率；

1:56:48.500,1:56:57.380
当然，你需要设置学习率，就像需要找到需要更快更新的参数，

1:56:57.380,1:56:59.780
或者是持续朝一个方向更新；

1:56:59.780,1:57:04.740
这不是说你不需要学习率，你仍旧要用学习率，

1:57:04.740,1:57:29.220
事实上，如果再“run"一次，我们要靠近30，2，当我们再跑一次，已经比较靠近了；

1:57:29.220,1:57:36.000
最终，结果就在目标值附近波动，

1:57:36.000,1:57:38.480
你可以看出，这就是学习率太大造成的

1:57:38.540,1:57:41.320
所以，我们可以到这里把学习率改小，

1:57:41.320,1:57:48.120
再跑一次，已经非常接近了，

1:57:48.120,1:57:53.780
所以你可以看出，你已经需要学习率退火，即使用了Adam.

1:57:53.780,1:58:01.140
这是一个好玩的spreadsheet,

1:58:01.140,1:58:07.580
我倒是有一个谷歌版本的SGD spreadsheet

1:58:07.620,1:58:10.560
所有都能正常工作，包括宏，

1:58:10.560,1:58:16.700
谷歌版本spreadsheet特别差劲，让这个能用已经让我”抓狂“了，所以放弃了其他的worksheet的谷歌迁移。

1:58:16.700,1:58:27.580
我会分享一个谷歌版本，他们有一个宏语言，但太难用了，

1:58:27.580,1:58:33.460
如果有人仍旧要用谷歌版，最终也能用，但太折磨人了。

1:58:33.460,1:58:37.020
所以，也许有人能成功将剩余的worksheet都迁移到谷歌spreadsheet上

1:58:37.120,1:58:47.480
这些就是权值衰减和Adam，Adam是非常快

1:58:47.480,1:59:04.100
我们不倾向使用optim.Something 来自主构建优化算法，

1:59:04.100,1:59:06.520
我们更多使用Learner。

1:59:06.520,1:59:12.460
Learner, 会为我们做所有这类工作，没什么神奇的。

1:59:12.460,1:59:20.120
你构建一个Learner, 给出你的数据堆，这是pytorch nn.Module 的一个子类，

1:59:20.120,1:59:29.620
这是我的损失函数，这里是我的度量函数，还记得，度量函数就是要打印的内容。

1:59:29.720,1:59:32.260
然后，你能得到一系列很有用的工具，

1:59:32.260,1:59:36.140
learn.lr_finder 可以启用了，并且记录（损失值，然后可以用recorder.plot来下图），

1:59:36.140,1:59:39.400
然后你可以说fit_one_cycle来取代fit。

1:59:39.400,1:59:45.080
这些工具非常好用，比如，通过lr_finder，我们可以找到很好的学习率值，

1:59:45.080,1:59:51.640
看这里我的损失值是0.13, 而这里都没有低过0.5，

1:59:51.640,1:59:58.100
所以，这些工具帮助我们产生巨大的改进，而不是微小的进步。

1:59:58.100,2:00:02.500
这还仅仅是训练了一个迭代次数epoch.

2:00:02.520,2:00:06.500
fit_one_cycle的工作内容是什么呢？

2:00:06.500,2:00:12.520
这是它的核心工作内容，我们见过左边这张图，

2:00:12.520,2:00:18.520
提示一下，这里画出的是学习率/批量图；

2:00:18.520,2:00:22.200
还记得，Adam也有学习率，我们用Adam作为默认值，

2:00:22.200,2:00:27.020
并做了一些小的调整，以后我们可能会讲到，

2:00:27.160,2:00:32.560
学习率，一开始很小，然后一半的时间都在变大，

2:00:32.560,2:00:35.780
剩下一半时间都在变小；

2:00:35.780,2:00:39.460
因为一开始时，我们不知道我们（的损失值）在哪，

2:00:39.460,2:00:43.440
（可能）我们在某个极度崎岖不同函数空间里，

2:00:43.440,2:00:45.960
如果我们一开始就大幅度跳越，

2:00:45.960,2:00:50.980
因为这些崎岖不平的地方的梯度都很大，所以会将我们带入非常离谱（的损失值）位置，

2:00:50.980,2:00:58.980
因此我们才要从小学习率开始，这样才能逐步进入一些合理的参数空间；

2:00:58.980,2:01:08.280
一旦进入比较合理的参数空间，你可以调大学习率，因为梯度已经处于希望改进的方向；

2:01:08.280,2:01:16.680
如我们已经讨论过多次的，当你接近最终最优结果时，你需要做学习率退火（变小），从而精确触达目标

2:01:16.740,2:01:20.920
有趣的是，在右图中，画的是动量，

2:01:20.920,2:01:28.540
每次当学习率小时，动量就很大，为什么会这样？

2:01:28.540,2:01:33.940
因为当你的学习率小时，你会持续在一个方向上运动，

2:01:33.940,2:01:35.740
但与其这样，不如步伐（学习率）更大一些；

2:01:35.740,2:01:44.440
但如果步伐（学习率）过大，你会来回大幅跳跃（动量无法持续，变得越来越小），这会将你丢入很差的参数空间；

2:01:44.440,2:01:52.880
但当你进入到末期，你在微调（学习率小），你的方向越来越一致（动量变大），（再后来）会需要调大学习率步伐。

2:01:52.880,2:02:01.600
这个组合变化，就叫做one-cycle, 它很简单，但超级棒，

2:02:01.600,2:02:07.960
能帮助实现super-convergence 超级收敛，能让训练提升10倍，

2:02:07.960,2:02:13.600
这是去年的论文，你们可能已经看过上周我对Leslie Smith的采访，

2:02:13.600,2:02:18.340
特别棒特别谦虚的人，我还想说的是，

2:02:18.340,2:02:22.820
是一位在从事深度学习前沿突破性研究，并且是年过60，

2:02:22.820,2:02:25.380
所有这些都非常鼓舞人心！

2:02:25.480,2:02:33.980
还有一个有趣的地方是，当你用fastai对损失值作图时，你的图不会像这样，而是长这个样子，

2:02:33.980,2:02:40.500
为什么呢？因为fastai为你画的是损失值的指数加权移动平均值，

2:02:40.500,2:02:44.420
所以，这个指数加权的概念，非常好用。

2:02:44.420,2:02:49.920
我总是会用到，其中的一个使用目的，就是为了作图解读更简单。

2:02:49.920,2:02:58.380
这意味着，图中的损失值会有1-2个批量的滞后，

2:02:58.380,2:03:11.300
这是使用指数加权移动平均值的一点点弊端，因为你的值需要使用历史数据，但能让读图非常便捷。

2:03:11.380,2:03:22.920
现在，我们要进入表格数据和协同过滤课时的最后一部分，

2:03:22.920,2:03:27.300
我们要尝试理解表格数据的所有代码。

2:03:27.300,2:03:31.580
还记得，表格数据模型使用的是ADULT_SAMPLE数据集，

2:03:31.580,2:03:34.240
目的是为了预测谁的收入会更高，

2:03:34.240,2:03:38.580
这是一个分类问题，

2:03:38.580,2:03:42.720
我们有一组类别变量categorical variables, 和一组连续变量，

2:03:42.720,2:03:47.940
首先，我们意识到的是，我们还不知道应该如何预测类别变量，

2:03:47.940,2:03:54.800
因为目前我们只是在用手比划/解释说我们的损失函数是nn.crossentropy 交叉熵函数，

2:03:54.800,2:03:57.900
这是到底什么呢？我们来看看

2:03:57.980,2:04:05.000
当然，我们要通过Excel来做探索和解说，

2:04:05.000,2:04:09.480
交叉熵，也就是（众多损失函数中）又一个损失函数，

2:04:09.480,2:04:14.940
我们已经学过了一个损失函数，也就是均方误差MSE，（y_hat - y)**2.

2:04:14.940,2:04:18.200
但这不是一个合适我们这里的损失函数，

2:04:18.200,2:04:26.680
因为就Mnist数据而言，我们有10个数字，10个激活值，每个值都是对应要预测的数字的概率，

2:04:26.680,2:04:38.080
所以，我们需要一个函数，能给正确且自信预测，很低的损失值，

2:04:38.080,2:04:43.280
但给错误且自信的预测，很高的损失值。

2:04:43.280,2:04:44.780
这就是我们想要的。

2:04:44.800,2:04:47.040
这里是一个例子，

2:04:47.040,2:04:51.760
这里是猫VS狗，one-hot编码处理过了，

2:04:51.760,2:05:01.580
这里是我的对猫和狗的预测值/概率值，源于我训练的某个模型，

2:05:01.700,2:05:03.620
这一组预测对猫对狗都不确定，

2:05:03.620,2:05:06.840
这一组预测，对猫非常自信，并且是正确的；

2:05:06.840,2:05:09.300
这一组对猫的预测很自信但却错了。

2:05:09.300,2:05:14.160
我们希望对这组预测的损失值要中性（不大不小），

2:05:14.160,2:05:18.300
因为对任何预测都不自信的预测不是我们想要的，

2:05:18.300,2:05:20.840
所以这里损失值是0.3，

2:05:20.840,2:05:25.280
这里预测自信且正确，（损失值低）所以0.01，

2:05:25.280,2:05:29.160
这里预测自信但错误，所以损失值是1。

2:05:29.160,2:05:31.600
那么，我们怎么才能做到这样呢？

2:05:31.720,2:05:33.920
这就是交叉熵，

2:05:33.920,2:06:03.220
并且公式是=（是否是猫）*  log(是猫的概率，其实是激活值) - （是否是狗）* log(是狗的激活值)， 就这些。

2:06:03.220,2:06:15.240
换言之，这是所有one-hot编码乘上，所有的激活值，再求和。所有，有趣的是，

2:06:15.320,2:06:20.640
这些数值，和这里的数值，是一样的，但是它们生成方式不同，

2:06:20.640,2:06:32.120
我是用if函数写的，因为zeros不增加任意有益的东西，所以，函数内容其实是一样的：

2:06:32.120,2:06:38.520
如果真实值是猫，那么就对猫的预测值取对数；

2:06:38.520,2:06:46.900
如果真实值是狗，对（1-猫预测值=狗的预测值）取对数。

2:06:46.900,2:06:56.080
所以，猫狗的one-hot编码，乘以，猫狗预测值，在求和，与这里的if函数效果是一样的。

2:06:56.080,2:07:02.260
试想，因为这其实就是矩阵乘法，

2:07:02.260,2:07:08.640
从我们在嵌入矩阵查找的内容学习中，我们知道其实这也就是序号查找。

2:07:08.640,2:07:18.940
所以，当你做交叉熵时，你也可以直接对应真实值做lookup 查找。

2:07:18.940,2:07:25.000
这种方法工作的前提是每组预测值/概率值之和为1.

2:07:25.140,2:07:30.300
这是其中一个原因，为什么会出现奇怪的交叉熵值，

2:07:30.300,2:07:32.120
这就是为什么我说你按下了“错误的按钮”（关于之前的提问），

2:07:32.120,2:07:34.060
如果它们相加所得不是1，你就有麻烦了。

2:07:34.060,2:07:36.540
那么如何确保它们之和为1 呢？

2:07:36.540,2:07:42.660
确保之和为1 的方法是对最后一层使用正确的激活函数，

2:07:42.660,2:07:46.580
这里要用的正确的激活函数是Softmax函数。

2:07:46.580,2:07:53.700
Softmax 函数，是一个激活函数，它的激活值之和为1，

2:07:53.700,2:07:59.800
每个激活值都大于0但小于1，

2:07:59.800,2:08:05.880
这些是我们想要的，需要的。那怎么做到呢？

2:08:05.920,2:08:09.740
比如，我们要预测5个物品，猫，狗，飞机，鱼，建筑，

2:08:09.740,2:08:16.340
这些值来源于我们模型的一组预测值，

2:08:16.340,2:08:20.400
如果我们把这组值做为e的指数来计算会怎样，

2:08:20.400,2:08:25.500
这个方向是对的，因为e**{任意值}都是大于0的，

2:08:25.500,2:08:28.520
这样我们就有了一组值，都是大于0的。

2:08:28.520,2:08:32.720
这里是这些数值之和。

2:08:32.720,2:08:41.240
这里是e**{猫}/(e**{猫} + ....之和）所生成的值，

2:08:41.240,2:08:45.760
这些数值都是小于1的，

2:08:45.760,2:08:48.760
因为所有的值都是大于0的，

2:08:48.760,2:08:53.680
而也不可能出现任意一个值大于它们的和，

2:08:53.680,2:08:58.740
同时所有的值之和都会是1，

2:08:58.740,2:09:04.420
因为这里的每个值都是对它们各自在所有值之和中的百分比。

2:09:04.420,2:09:05.940
这就是softmax。

2:09:06.020,2:09:14.560
所以softmax函数=e**(激活值)/e**(各个激活值)之和，

2:09:14.560,2:09:17.580
这就是Softmax。

2:09:17.580,2:09:23.600
所以，当我们面对单一标注但多类别分类时，

2:09:23.600,2:09:27.620
我们通常需要Softmax作为我们的激活函数，

2:09:27.620,2:09:31.680
通常需要交叉熵，作为我们的损失函数。

2:09:31.680,2:09:41.480
因为这些东西通常一起用，而且是以非常好用的形式，pytorch能让我们轻松使用它们。

2:09:41.480,2:09:49.460
你可能已经发现，在这个Mnist案例中，我没有在这里添加softmax 函数，

2:09:49.460,2:09:56.920
因为如果你调用交叉熵，softmax计算已经包含在交叉熵函数中了。

2:09:56.920,2:10:03.180
所以，nn.CrossEntropyLoss()不仅仅是交叉熵，而是softmax + 交叉熵。

2:10:03.180,2:10:06.160
你可能已经发现，有时候，

2:10:06.160,2:10:13.240
你的模型预测值看起来更像这些值，

2:10:13.240,2:10:19.500
特别大的值，携带负号，而不是这些值，在0到1之间，之和为1

2:10:19.500,2:10:25.780
原因应该是，pytorch模型里面没有softmax函数，

2:10:25.780,2:10:27.940
因为我们直接使用了交叉熵函数，

2:10:27.940,2:10:33.560
（因此你要要直接输出softmax的结果的话）你需要自己写softmax代码。

2:10:33.600,2:10:38.740
fastai变得越来越擅长了解什么情况下会发生这类事件，

2:10:38.740,2:10:41.580
如果我们能识别你用的损失函数，

2:10:41.580,2:10:46.960
当你做预测时，我们会自动为你添加softmax;

2:10:46.960,2:10:51.280
但尤其是当你使用定制的损失函数时，

2:10:51.280,2:10:54.900
你可能在背后调用了交叉熵，

2:10:54.900,2:11:00.540
那么你有可能会生成并看到这样的值。

2:11:00.600,2:11:05.080
我们只剩3分钟了，但我要指出一些内容，

2:11:05.100,2:11:12.620
在下周的前十分钟里，我们会结束表格数据的全部内容。

2:11:12.620,2:11:17.560
这是表格数据模型的forward 函数，

2:11:17.560,2:11:21.920
这里是遍历所有的嵌入矩阵，

2:11:21.920,2:11:24.900
每个嵌入都被赋值给e,

2:11:24.900,2:11:30.240
你可以将它当作一个函数来使用，这里是将每一个类别变量给到对应的嵌入，

2:11:30.240,2:11:36.000
然后在concat合并乘一个矩阵，

2:11:36.060,2:11:39.660
然后执行一系列层计算，

2:11:39.660,2:11:42.760
基本上都是线性层，

2:11:42.760,2:11:46.380
然后再做S函数的技巧运算，

2:11:46.440,2:11:51.120
然后，只剩下2个新的知识点要学习，

2:11:51.120,2:11:54.500
一个是dropout 随机失活，

2:11:54.500,2:12:00.460
另一个是batch norm批量归一，

2:12:00.460,2:12:04.420
它们是另外的正则化方法，

2:12:04.420,2:12:10.840
批量归一所做的不仅仅是正则化处理，其他很多技巧都能做正则化处理。

2:12:10.840,2:12:19.020
正则化的基本方法是，权值衰减，批量归一，随机失活。

2:12:19.020,2:12:24.400
然后，你还可以规避过拟合，采用数据增强。

2:12:24.400,2:12:27.880
批量归一和随机失活，我们下周来讲，

2:12:27.880,2:12:34.020
我们还会将数据增强，以及什么是卷积，

2:12:34.020,2:12:41.116
还会学习新的机器视觉模型结构设计和应用，

2:12:41.120,2:12:49.520
但基本上，我们已经了解了整个collab.py的内容，

2:12:49.520,2:12:54.360
关于fastai.collab的内容，你已经知道为什么需要它，以及它的功能，

2:12:54.360,2:13:01.740
你也已经知道了表格数据模型的主要功能。

2:13:01.740,2:13:06.020
这个表格模型，如果你用这个模型跑Rossmann数据，

2:13:06.020,2:13:11.080
你将得到与我展示给你看到那篇论文里的相同的结果，第二名的成绩，

2:13:11.080,2:13:13.760
事实上，会更好一点。

2:13:13.760,2:13:16.460
下周我会演示一下，如果我记得的话，

2:13:16.460,2:13:22.680
我是如何跑更多的实验并找到了进一步提升效果的小技巧。

2:13:22.680,2:13:32.580
好了，下周见，谢谢大家！尽情享受一下外面的烟火吧
